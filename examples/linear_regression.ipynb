{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import distrax\n",
    "import haiku as hk\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from lfiax.flows.nsf import make_nsf\n",
    "\n",
    "from typing import (\n",
    "    Any,\n",
    "    Iterator,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "OptState = Any\n",
    "\n",
    "\n",
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helper functions to simulate data\n",
    "# ----------------------------------------\n",
    "def load_dataset(split: tfds.Split, batch_size: int) -> Iterator[Batch]:\n",
    "    ds = split\n",
    "    ds = ds.shuffle(buffer_size=10 * batch_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=1000)\n",
    "    ds = ds.repeat()\n",
    "    return iter(tfds.as_numpy(ds))\n",
    "\n",
    "\n",
    "def sim_data(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack((y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d)))))\n",
    "\n",
    "\n",
    "def prepare_data(batch: Batch, prng_key: Optional[PRNGKey] = None) -> Array:\n",
    "    # Batch is [y, thetas, d]\n",
    "    data = batch.astype(np.float32)\n",
    "    # Handling the scalar case\n",
    "    if data.shape[1] <= 3:\n",
    "        x = jnp.expand_dims(data[:, :-2], -1)\n",
    "    # Use known length of x to split up the cond_data\n",
    "    data_shape = data.shape\n",
    "    start = [0, 0]\n",
    "    stop = [data_shape[0], len_x]\n",
    "    x = lax.dynamic_slice(data, start, stop)\n",
    "    cond_data = data[:, len_x:]\n",
    "    # breakpoint()\n",
    "    return x, cond_data\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Haiku transform functions for training and evaluation\n",
    "# ----------------------------\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob(data: Array, cond_data: Array) -> Array:\n",
    "    # Get batch\n",
    "    shift = data.mean(axis=0)\n",
    "    scale = data.std(axis=0) + 1e-14\n",
    "    \n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "        standardize_x=True,\n",
    "        standardize_z=True,\n",
    "        use_resnet=True,\n",
    "        event_dim=EVENT_DIM,\n",
    "        shift=shift,\n",
    "        scale=scale,\n",
    "    )\n",
    "    return model.log_prob(data, cond_data)\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def model_sample(key: PRNGKey, num_samples: int, cond_data: Array) -> Array:\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "    )\n",
    "    z = jnp.repeat(cond_data, num_samples, axis=0)\n",
    "    z = jnp.expand_dims(z, -1)\n",
    "    return model._sample_n(key=key, n=[num_samples], z=z)\n",
    "\n",
    "\n",
    "def loss_fn(params: hk.Params, prng_key: PRNGKey, batch: Batch) -> Array:\n",
    "    x, cond_data = prepare_data(batch, prng_key)\n",
    "    # Loss is average negative log likelihood.\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, cond_data))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_fn(params: hk.Params, batch: Batch) -> Array:\n",
    "    x, cond_data = prepare_data(batch)\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, cond_data))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    grads = jax.grad(loss_fn)(params, prng_key, batch)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Shapes must be 1D sequences of concrete values of integer type, got Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=0/1)>.\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m opt_state \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39minit(params)\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(training_steps):\n\u001b[0;32m---> 54\u001b[0m     params, opt_state \u001b[39m=\u001b[39m update(params, \u001b[39mnext\u001b[39;49m(prng_seq), opt_state, \u001b[39mnext\u001b[39;49m(train_ds), len_x)\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m eval_frequency \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     57\u001b[0m         val_loss \u001b[39m=\u001b[39m eval_fn(params, \u001b[39mnext\u001b[39m(valid_ds))\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [45], line 170\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(params, prng_key, opt_state, batch, len_x)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39m@jax\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m#(static_argnums = (4,))\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\n\u001b[1;32m    167\u001b[0m     params: hk\u001b[39m.\u001b[39mParams, prng_key: PRNGKey, opt_state: OptState, batch: Batch, len_x: \u001b[39mint\u001b[39m\n\u001b[1;32m    168\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[hk\u001b[39m.\u001b[39mParams, OptState]:\n\u001b[1;32m    169\u001b[0m     \u001b[39m\"\"\"Single SGD update step.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     grads \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mgrad(loss_fn)(params, prng_key, batch, len_x)\n\u001b[1;32m    171\u001b[0m     updates, new_opt_state \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mupdate(grads, opt_state)\n\u001b[1;32m    172\u001b[0m     new_params \u001b[39m=\u001b[39m optax\u001b[39m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [45], line 152\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, prng_key, batch, len_x)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_fn\u001b[39m(params: hk\u001b[39m.\u001b[39mParams, prng_key: PRNGKey, batch: Batch, len_x: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m    151\u001b[0m     \u001b[39m# TODO: pass the length of x here\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     x, cond_data \u001b[39m=\u001b[39m prepare_data(batch, len_x, prng_key)\n\u001b[1;32m    153\u001b[0m     \u001b[39m# Loss is average negative log likelihood.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mjnp\u001b[39m.\u001b[39mmean(log_prob\u001b[39m.\u001b[39mapply(params, x, cond_data))\n",
      "Cell \u001b[0;32mIn [45], line 103\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(batch, len_x, prng_key)\u001b[0m\n\u001b[1;32m    101\u001b[0m start \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m])\n\u001b[1;32m    102\u001b[0m stop \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray([data_shape[\u001b[39m0\u001b[39m], len_x])\n\u001b[0;32m--> 103\u001b[0m x \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39;49mdynamic_slice(data, start, stop)\n\u001b[1;32m    104\u001b[0m cond_data \u001b[39m=\u001b[39m data[:, len_x:]\n\u001b[1;32m    105\u001b[0m \u001b[39mreturn\u001b[39;00m x, cond_data\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/core.py:1827\u001b[0m, in \u001b[0;36mcanonicalize_shape\u001b[0;34m(shape, context)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1826\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[0;32m-> 1827\u001b[0m \u001b[39mraise\u001b[39;00m _invalid_shape_error(shape, context)\n",
      "\u001b[0;31mTypeError\u001b[0m: Shapes must be 1D sequences of concrete values of integer type, got Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=0/1)>.\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions."
     ]
    }
   ],
   "source": [
    "# TODO: Put this in hydra config file\n",
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "d = jnp.array([-10.0, 0.0, 5.0, 10.0])\n",
    "# d = jnp.array([1., 2.])\n",
    "# d = jnp.array([1.])\n",
    "len_x = len(d)\n",
    "num_samples = 100\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "EVENT_SHAPE = (len(d),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = (theta_shape[0] + len(d),)\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 10\n",
    "mlp_num_layers = 4\n",
    "hidden_size = 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "training_steps = 10  # 00\n",
    "eval_frequency = 100\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 10000\n",
    "X = sim_data(d, num_samples, key)\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *cond_info_shape)),\n",
    ")\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "for step in range(training_steps):\n",
    "    params, opt_state = update(params, next(prng_seq), opt_state, next(train_ds))\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11523.323\n",
      "{'b': DeviceArray(21.859552, dtype=float32, weak_type=True), 'w': DeviceArray([-179.33102 , -201.21684 ,  -60.641083], dtype=float32)}\n",
      "[[ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# create our dataset\n",
    "X, y = make_regression(n_features=3)\n",
    "X, X_test, y, y_test = train_test_split(X, y)\n",
    "\n",
    "\n",
    "# model weights\n",
    "params = {\n",
    "    'w': jnp.zeros(X.shape[1:]),\n",
    "    'b': 0.\n",
    "}\n",
    "\n",
    "\n",
    "def forward(params, X):\n",
    "    return jnp.dot(X, params['w']) + params['b']\n",
    "\n",
    "\n",
    "def loss_fn(params, X, y):\n",
    "    err = forward(params, X) - y\n",
    "    return jnp.mean(jnp.square(err))  # mse\n",
    "\n",
    "\n",
    "def jvp_fn(params, xs, y):\n",
    "    # return jax.jacfwd(lambda params: loss_fn(params, xs, y))(params)\n",
    "    return jax.vjp(loss_fn)(params, xs, y)\n",
    "\n",
    "\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "grad_xs = jax.grad(loss_fn, argnums=1)\n",
    "\n",
    "# loss, grad_fn = jax.value_and_grad(loss_fn, argnums=(1, 2))\n",
    "\n",
    "# def loss_and_grads_fn(params, xs, y):\n",
    "#     loss, grads = jax.value_and_grad(loss_fn)(params, xs, y)\n",
    "#     vjp_fn = jax.vjp(loss_fn, (params, xs, y))\n",
    "#     inputs_grads = vjp_fn(params)\n",
    "#     return loss, grads, inputs_grads\n",
    "\n",
    "# def loss_and_grads_fn(params, xs, y):\n",
    "#     loss, grads = jax.value_and_grad(loss_fn)(params, xs, y)\n",
    "#     vjp_fn = jax.vjp(lambda params: loss_fn(params, xs, y))\n",
    "#     inputs_grads = vjp_fn(params)\n",
    "#     return loss, grads, inputs_grads\n",
    "\n",
    "# def loss_and_grads_fn(params, xs, y):\n",
    "#     loss, grads = jax.value_and_grad(loss_fn)(params, xs, y)\n",
    "#     vjp_fn = jax.vjp(lambda params, xs, y: loss_fn(params, xs, y))\n",
    "#     inputs_grads = vjp_fn(params)\n",
    "#     return loss, grads, inputs_grads\n",
    "\n",
    "\n",
    "# def loss_and_grads_fn(params, xs, y):\n",
    "#     loss, grads = jax.value_and_grad(lambda params, xs, y: loss_fn(params, xs, y))(params, xs, y)\n",
    "#     vjp_fn = jax.vjp(lambda params, xs, y: loss_fn(params, xs, y))\n",
    "#     inputs_grads = vjp_fn(params, xs, y)\n",
    "#     return loss, grads, inputs_grads\n",
    "\n",
    "# def loss_and_grads_fn(params, xs, y):\n",
    "#     loss, grads = jax.value_and_grad(loss_fn)(params, xs, y)\n",
    "#     vjp_fn = jax.vjp(lambda params, xs, y: loss_fn(params, xs, y))\n",
    "#     inputs_grads = vjp_fn(params, xs, y)\n",
    "#     return loss, grads, inputs_grads\n",
    "\n",
    "# def loss_and_grads_fn(params, xs, y):\n",
    "#     loss, grads = jax.value_and_grad(loss_fn)(params, xs, y)\n",
    "#     vjp_fn = jax.vjp(lambda p, x, y: loss_fn(p, x, y))\n",
    "#     inputs_grads = vjp_fn(params, xs, y)\n",
    "#     return loss, grads, inputs_grads\n",
    "\n",
    "\n",
    "# def loss_and_grads_vjp(params, xs, y):\n",
    "#     def loss_and_grads(params):\n",
    "#         loss, grads = jax.value_and_grad(loss_fn)(params, xs, y)\n",
    "#         return loss, grads\n",
    "#     return jax.vjp(loss_and_grads)(params)\n",
    "\n",
    "\n",
    "# loss, grads, grads_x = loss_and_grads_fn(params, X, y)\n",
    "\n",
    "\n",
    "def update(params, grads):\n",
    "    return jax.tree_map(lambda p, g: p - 0.05 * g, params, grads)\n",
    "\n",
    "\n",
    "# the main training loop\n",
    "# for _ in range(50):\n",
    "loss = loss_fn(params, X_test, y_test)\n",
    "print(loss)\n",
    "\n",
    "grads = grad_fn(params, X, y)\n",
    "print(grads)\n",
    "grads_x = grad_xs(params, X, y)\n",
    "print(grads_x)\n",
    "params = update(params, grads)\n",
    "# jvp = jvp_fn(params, X, y)\n",
    "# vjp_fn, grad_inputs = jvp_fn(params, X, y)\n",
    "# print(grad_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "print(jnp.min(grads_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Using a non-tuple sequence for multidimensional indexing is not allowed; use `arr[tuple(seq)]` instead of `arr[seq]`. See https://github.com/google/jax/issues/4564 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [58], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m forward_grads_X \u001b[39m=\u001b[39m forward_grad(params, X)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Compute the Jacobian of the loss function with respect to X using the chain rule\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m loss_grads_X \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mdot(grad_fn(params[\u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m], X, y), forward_grads_X)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [57], line 24\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, X, y)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_fn\u001b[39m(params, X, y):\n\u001b[0;32m---> 24\u001b[0m     err \u001b[39m=\u001b[39m forward(params, X) \u001b[39m-\u001b[39m y\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mmean(jnp\u001b[39m.\u001b[39msquare(err))\n",
      "Cell \u001b[0;32mIn [57], line 20\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(params, X)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(params, X):\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mdot(X, params[\u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m+\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:3649\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3643\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(aval, core\u001b[39m.\u001b[39mDShapedArray) \u001b[39mand\u001b[39;00m aval\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m () \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3644\u001b[0m         dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3645\u001b[0m         \u001b[39mnot\u001b[39;00m dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, dtypes\u001b[39m.\u001b[39mbool_) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3646\u001b[0m         \u001b[39misinstance\u001b[39m(arr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mint\u001b[39m)):\n\u001b[1;32m   3647\u001b[0m       \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39mdynamic_index_in_dim(arr, idx, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 3649\u001b[0m treedef, static_idx, dynamic_idx \u001b[39m=\u001b[39m _split_index_for_jit(idx, arr\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m   3650\u001b[0m \u001b[39mreturn\u001b[39;00m _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m   3651\u001b[0m                unique_indices, mode, fill_value)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:3724\u001b[0m, in \u001b[0;36m_split_index_for_jit\u001b[0;34m(idx, shape)\u001b[0m\n\u001b[1;32m   3719\u001b[0m \u001b[39m\"\"\"Splits indices into necessarily-static and dynamic parts.\u001b[39;00m\n\u001b[1;32m   3720\u001b[0m \n\u001b[1;32m   3721\u001b[0m \u001b[39mUsed to pass indices into `jit`-ted function.\u001b[39;00m\n\u001b[1;32m   3722\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3723\u001b[0m \u001b[39m# Convert list indices to tuples in cases (deprecated by NumPy.)\u001b[39;00m\n\u001b[0;32m-> 3724\u001b[0m idx \u001b[39m=\u001b[39m _eliminate_deprecated_list_indexing(idx)\n\u001b[1;32m   3726\u001b[0m \u001b[39m# Expand any (concrete) boolean indices. We can then use advanced integer\u001b[39;00m\n\u001b[1;32m   3727\u001b[0m \u001b[39m# indexing logic to handle them.\u001b[39;00m\n\u001b[1;32m   3728\u001b[0m idx \u001b[39m=\u001b[39m _expand_bool_indices(idx, shape)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:4004\u001b[0m, in \u001b[0;36m_eliminate_deprecated_list_indexing\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m   4000\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4001\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mUsing a non-tuple sequence for multidimensional indexing is not allowed; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4002\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39muse `arr[array(seq)]` instead of `arr[seq]`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4003\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/google/jax/issues/4564 for more information.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 4004\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m   4005\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4006\u001b[0m   idx \u001b[39m=\u001b[39m (idx,)\n",
      "\u001b[0;31mTypeError\u001b[0m: Using a non-tuple sequence for multidimensional indexing is not allowed; use `arr[tuple(seq)]` instead of `arr[seq]`. See https://github.com/google/jax/issues/4564 for more information."
     ]
    }
   ],
   "source": [
    "forward_grad = jax.jacfwd(forward)\n",
    "\n",
    "# Compute the gradients of the forward pass with respect to X\n",
    "forward_grads_X = forward_grad(params, X)\n",
    "\n",
    "# Compute the Jacobian of the loss function with respect to X using the chain rule\n",
    "loss_grads_X = jnp.dot(grad_fn(params['w'], X, y), forward_grads_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdm3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bac022e209f3f8f811ac02bf6d6b971751e1ab224096f1893a92a620959b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
