{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import distrax\n",
    "import haiku as hk\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from lfiax.flows.nsf import make_nsf\n",
    "\n",
    "from typing import (\n",
    "    Any,\n",
    "    Iterator,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "OptState = Any\n",
    "\n",
    "\n",
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helper functions to simulate data\n",
    "# ----------------------------------------\n",
    "def load_dataset(split: tfds.Split, batch_size: int) -> Iterator[Batch]:\n",
    "    ds = split\n",
    "    ds = ds.shuffle(buffer_size=10 * batch_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=1000)\n",
    "    ds = ds.repeat()\n",
    "    return iter(tfds.as_numpy(ds))\n",
    "\n",
    "\n",
    "def sim_data(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack((y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d)))))\n",
    "\n",
    "\n",
    "def prepare_data(batch: Batch, prng_key: Optional[PRNGKey] = None) -> Array:\n",
    "    # Batch is [y, thetas, d]\n",
    "    data = batch.astype(np.float32)\n",
    "    # Handling the scalar case\n",
    "    if data.shape[1] <= 3:\n",
    "        y = jnp.expand_dims(data[:, :-2], -1)\n",
    "    # Use known length of x to split up the cond_data\n",
    "    # data_shape = data.shape\n",
    "    # start = [0, 0]\n",
    "    # stop = [data_shape[0], len_x]\n",
    "    # y = lax.dynamic_slice(data, start, stop)\n",
    "    y = data[:, :len_x]\n",
    "    cond_data = data[:, len_x:]\n",
    "    theta = cond_data[:, :-len_x]\n",
    "    x = cond_data[:, -len_x:-len_xi]\n",
    "    xi = cond_data[:, -len_xi:]\n",
    "    # return x, cond_data\n",
    "    # breakpoint()\n",
    "    return y, theta, x, xi\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Haiku transform functions for training and evaluation\n",
    "# ----------------------------\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob(data: Array, cond_data: Array) -> Array:\n",
    "    # Get batch\n",
    "    shift = data.mean(axis=0)\n",
    "    scale = data.std(axis=0) + 1e-14\n",
    "    \n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "        standardize_x=True,\n",
    "        standardize_z=True,\n",
    "        use_resnet=True,\n",
    "        event_dim=EVENT_DIM,\n",
    "        shift=shift,\n",
    "        scale=scale,\n",
    "    )\n",
    "    return model.log_prob(data, cond_data)\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def model_sample(key: PRNGKey, num_samples: int, cond_data: Array) -> Array:\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "    )\n",
    "    z = jnp.repeat(cond_data, num_samples, axis=0)\n",
    "    z = jnp.expand_dims(z, -1)\n",
    "    return model._sample_n(key=key, n=[num_samples], z=z)\n",
    "\n",
    "\n",
    "def loss_fn(params: hk.Params, prng_key: PRNGKey, y: Array, theta: Array, x: Array, xi: Array) -> Array:\n",
    "    # x, cond_data = prepare_data(batch, prng_key)\n",
    "    # I wonder if this will work...\n",
    "    cond_data = jnp.concatenate((theta, x, xi), axis=1)\n",
    "    # Loss is average negative log likelihood.\n",
    "    loss = -jnp.mean(log_prob.apply(params, y, cond_data))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_fn(params: hk.Params, batch: Batch) -> Array:\n",
    "    y, theta, x, xi = prepare_data(batch)\n",
    "    cond_data = jnp.concatenate((theta, x, xi), axis=1)\n",
    "    # loss = -jnp.mean(log_prob.apply(params, x, cond_data))\n",
    "    loss = -jnp.mean(log_prob.apply(params, y, cond_data))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    # x, cond_data = prepare_data(batch, prng_key)\n",
    "    y, theta, x, xi = prepare_data(batch)\n",
    "    grads = jax.grad(loss_fn)(params, prng_key, y, theta, x, xi)\n",
    "    grads_d = jax.grad(loss_fn, argnums=5)(params, prng_key, y, theta, x, xi)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, grads_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "key = jrandom.PRNGKey(seed)\n",
    "d_prop = jrandom.uniform(key, shape=(1,), minval=-10., maxval=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_obs = jnp.array([1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.concatenate((d_obs, d_prop), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:     0; Validation loss: 7.679\n"
     ]
    }
   ],
   "source": [
    "# TODO: Put this in hydra config file\n",
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "# d = jnp.array([-10.0, 0.0, 5.0, 10.0])\n",
    "# d = jnp.array([1., 2.])\n",
    "# d = jnp.array([1.])\n",
    "d_obs = jnp.array([1.])\n",
    "d_prop = jrandom.uniform(key, shape=(1,), minval=-10., maxval=10.)\n",
    "d = jnp.concatenate((d_obs, d_prop), axis=0)\n",
    "len_x = len(d_obs) + len(d_prop)\n",
    "len_xi = len(d_prop)\n",
    "num_samples = 100\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "EVENT_SHAPE = (len(d),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = (theta_shape[0] + len(d),)\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 10\n",
    "mlp_num_layers = 4\n",
    "hidden_size = 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "training_steps = 10  # 00\n",
    "eval_frequency = 100\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 10000\n",
    "# TODO: put this function in training since d will be changing.\n",
    "X = sim_data(d, num_samples, key)\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *cond_info_shape)),\n",
    ")\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "for step in range(training_steps):\n",
    "    params, opt_state, grads_d = update(params, next(prng_seq), opt_state, next(train_ds))\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using a more simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2181.5369\n",
      "1858.5502\n",
      "1585.1548\n",
      "1353.422\n",
      "1156.7448\n",
      "989.6083\n",
      "847.4021\n",
      "726.26587\n",
      "622.9615\n",
      "534.76965\n",
      "459.40234\n",
      "394.93176\n",
      "339.73135\n",
      "292.42667\n",
      "251.85446\n",
      "217.0293\n",
      "187.11482\n",
      "161.40063\n",
      "139.28238\n",
      "120.24537\n",
      "103.85087\n",
      "89.72437\n",
      "77.54581\n",
      "67.04163\n",
      "57.97752\n",
      "50.15276\n",
      "43.395264\n",
      "37.557304\n",
      "32.51204\n",
      "28.15043\n",
      "24.378696\n",
      "21.116192\n",
      "18.293394\n",
      "15.850478\n",
      "13.735813\n",
      "11.904947\n",
      "10.31944\n",
      "8.946174\n",
      "7.7565503\n",
      "6.725816\n",
      "5.8326335\n",
      "5.058549\n",
      "4.3875713\n",
      "3.8059084\n",
      "3.3016088\n",
      "2.8643444\n",
      "2.4851553\n",
      "2.156312\n",
      "1.8710963\n",
      "1.6236962\n",
      "1.4090894\n",
      "1.2229112\n",
      "1.0613791\n",
      "0.92123014\n",
      "0.7996186\n",
      "0.6940952\n",
      "0.6025214\n",
      "0.5230455\n",
      "0.45407078\n",
      "0.39420485\n",
      "0.34224513\n",
      "0.29714483\n",
      "0.25799254\n",
      "0.22400692\n",
      "0.19450277\n",
      "0.16889036\n",
      "0.1466547\n",
      "0.12734972\n",
      "0.110588394\n",
      "0.09603701\n",
      "0.08339975\n",
      "0.07242788\n",
      "0.06290057\n",
      "0.054628\n",
      "0.047444887\n",
      "0.041206427\n",
      "0.03578893\n",
      "0.03108498\n",
      "0.026999\n",
      "0.023450699\n",
      "0.020369269\n",
      "0.017693063\n",
      "0.015368364\n",
      "0.01334941\n",
      "0.011596036\n",
      "0.01007299\n",
      "0.008750262\n",
      "0.007601229\n",
      "0.0066029932\n",
      "0.0057360493\n",
      "0.0049831304\n",
      "0.004328905\n",
      "0.0037607104\n",
      "0.0032672042\n",
      "0.0028383692\n",
      "0.0024660188\n",
      "0.002142265\n",
      "0.0018611191\n",
      "0.0016168596\n",
      "0.0014047199\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# create our dataset\n",
    "X, y = make_regression(n_features=3)\n",
    "X, X_test, y, y_test = train_test_split(X, y)\n",
    "\n",
    "\n",
    "# model weights\n",
    "params = {\n",
    "    'w': jnp.zeros(X.shape[1:]),\n",
    "    'b': 0.\n",
    "}\n",
    "\n",
    "\n",
    "def forward(params, X):\n",
    "    return jnp.dot(X, params['w']) + params['b']\n",
    "\n",
    "\n",
    "def loss_fn(params, X, y):\n",
    "    err = forward(params, X) - y\n",
    "    return jnp.mean(jnp.square(err))  # mse\n",
    "\n",
    "\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "grad_xs = jax.grad(loss_fn, argnums=1)\n",
    "\n",
    "\n",
    "def update(params, grads):\n",
    "    return jax.tree_map(lambda p, g: p - 0.05 * g, params, grads)\n",
    "\n",
    "\n",
    "# the main training loop\n",
    "for _ in range(100):\n",
    "    loss = loss_fn(params, X_test, y_test)\n",
    "    print(loss)\n",
    "\n",
    "    grads = grad_fn(params, X, y)\n",
    "    # print(grads)\n",
    "    grads_x = grad_xs(params, X, y)\n",
    "    # print(grads_x)\n",
    "    params = update(params, grads)\n",
    "# jvp = jvp_fn(params, X, y)\n",
    "# vjp_fn, grad_inputs = jvp_fn(params, X, y)\n",
    "# print(grad_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 3.52432905e-03,  2.47096978e-02,  5.91810495e-02],\n",
       "             [ 4.47242710e-05,  3.13569850e-04,  7.51016545e-04],\n",
       "             [-1.65014213e-03, -1.15694404e-02, -2.77094282e-02],\n",
       "             [ 2.14535417e-03,  1.50414603e-02,  3.60250995e-02],\n",
       "             [-3.33245192e-03, -2.33644154e-02, -5.59590235e-02],\n",
       "             [-2.63125426e-03, -1.84481926e-02, -4.41844091e-02],\n",
       "             [-4.31130687e-03, -3.02273389e-02, -7.23960921e-02],\n",
       "             [-1.45995826e-03, -1.02360277e-02, -2.45158337e-02],\n",
       "             [-6.79470249e-04, -4.76388726e-03, -1.14097642e-02],\n",
       "             [-1.30786747e-03, -9.16969217e-03, -2.19619032e-02],\n",
       "             [-2.38773972e-03, -1.67408697e-02, -4.00952771e-02],\n",
       "             [ 5.24106342e-03,  3.67460325e-02,  8.80087093e-02],\n",
       "             [-1.81098015e-03, -1.26971044e-02, -3.04102451e-02],\n",
       "             [-7.59501301e-04, -5.32499887e-03, -1.27536571e-02],\n",
       "             [ 2.56988191e-04,  1.80178997e-03,  4.31538327e-03],\n",
       "             [ 1.57931703e-03,  1.10728731e-02,  2.65201237e-02],\n",
       "             [-1.80999271e-03, -1.26901809e-02, -3.03936619e-02],\n",
       "             [ 2.51245988e-03,  1.76153034e-02,  4.21895944e-02],\n",
       "             [ 2.02719448e-03,  1.42130218e-02,  3.40409465e-02],\n",
       "             [ 9.46122920e-04,  6.63343631e-03,  1.58874355e-02],\n",
       "             [-1.07267709e-03, -7.52073014e-03, -1.80125497e-02],\n",
       "             [ 3.48426029e-03,  2.44287699e-02,  5.85082099e-02],\n",
       "             [ 7.25746504e-04,  5.08833816e-03,  1.21868420e-02],\n",
       "             [ 2.06126671e-04,  1.44519086e-03,  3.46130924e-03],\n",
       "             [ 2.18965521e-04,  1.53520633e-03,  3.67690111e-03],\n",
       "             [-4.19787364e-03, -2.94320397e-02, -7.04913065e-02],\n",
       "             [-2.66031804e-03, -1.86519641e-02, -4.46724519e-02],\n",
       "             [ 2.23141653e-03,  1.56448595e-02,  3.74702737e-02],\n",
       "             [ 2.80422578e-03,  1.96609274e-02,  4.70889695e-02],\n",
       "             [-3.16794566e-03, -2.22110320e-02, -5.31966090e-02],\n",
       "             [ 3.18854419e-03,  2.23554522e-02,  5.35425022e-02],\n",
       "             [ 1.41096604e-03,  9.89253446e-03,  2.36931499e-02],\n",
       "             [ 7.20808515e-04,  5.05371718e-03,  1.21039227e-02],\n",
       "             [ 2.03728225e-04,  1.42837490e-03,  3.42103420e-03],\n",
       "             [ 8.99907536e-05,  6.30941242e-04,  1.51113793e-03],\n",
       "             [-1.74523419e-04, -1.22361479e-03, -2.93062278e-03],\n",
       "             [ 5.09038335e-04,  3.56895872e-03,  8.54784716e-03],\n",
       "             [ 1.14561850e-03,  8.03213567e-03,  1.92373954e-02],\n",
       "             [ 4.67276899e-04,  3.27616162e-03,  7.84658268e-03],\n",
       "             [-1.44345104e-03, -1.01202922e-02, -2.42386423e-02],\n",
       "             [ 7.13613117e-04,  5.00326883e-03,  1.19830966e-02],\n",
       "             [-9.46687185e-04, -6.63739257e-03, -1.58969108e-02],\n",
       "             [ 1.18371169e-03,  8.29921383e-03,  1.98770612e-02],\n",
       "             [-2.06169020e-03, -1.44548770e-02, -3.46202031e-02],\n",
       "             [ 1.16367755e-03,  8.15875083e-03,  1.95406433e-02],\n",
       "             [-2.83413590e-03, -1.98706333e-02, -4.75912280e-02],\n",
       "             [ 2.29067286e-03,  1.60603151e-02,  3.84653136e-02],\n",
       "             [-1.37615297e-03, -9.64845438e-03, -2.31085625e-02],\n",
       "             [ 4.23991727e-03,  2.97268145e-02,  7.11973086e-02],\n",
       "             [ 1.37304922e-03,  9.62669309e-03,  2.30564438e-02],\n",
       "             [-1.34426763e-03, -9.42489970e-03, -2.25731395e-02],\n",
       "             [ 7.69059872e-04,  5.39201591e-03,  1.29141668e-02],\n",
       "             [-3.39114363e-03, -2.37759128e-02, -5.69445826e-02],\n",
       "             [-1.63165983e-04, -1.14398578e-03, -2.73990724e-03],\n",
       "             [ 1.40521675e-03,  9.85222589e-03,  2.35966071e-02],\n",
       "             [ 2.41807336e-03,  1.69535428e-02,  4.06046435e-02],\n",
       "             [-6.39754580e-04, -4.48543346e-03, -1.07428534e-02],\n",
       "             [-1.07115156e-04, -7.51003448e-04, -1.79869344e-03],\n",
       "             [ 4.32287547e-04,  3.03084520e-03,  7.25903595e-03],\n",
       "             [ 4.29367088e-03,  3.01036928e-02,  7.20999539e-02],\n",
       "             [ 1.97802612e-04,  1.38682942e-03,  3.32153053e-03],\n",
       "             [ 2.66539725e-03,  1.86875742e-02,  4.47577424e-02],\n",
       "             [ 7.00209930e-04,  4.90929652e-03,  1.17580276e-02],\n",
       "             [ 3.15835187e-03,  2.21437681e-02,  5.30355051e-02],\n",
       "             [-1.02047517e-03, -7.15473387e-03, -1.71359703e-02],\n",
       "             [-2.39281901e-04, -1.67764805e-03, -4.01805667e-03],\n",
       "             [-9.49367823e-04, -6.65618712e-03, -1.59419235e-02],\n",
       "             [-1.43089448e-03, -1.00322561e-02, -2.40277890e-02],\n",
       "             [-5.30031975e-03, -3.71614881e-02, -8.90037492e-02],\n",
       "             [ 1.88603799e-03,  1.32233482e-02,  3.16706263e-02],\n",
       "             [-2.65573291e-03, -1.86198168e-02, -4.45954539e-02],\n",
       "             [ 2.11629026e-06,  1.48376885e-05,  3.55370576e-05],\n",
       "             [-1.43089448e-03, -1.00322561e-02, -2.40277890e-02],\n",
       "             [ 3.64679168e-03,  2.55683064e-02,  6.12374619e-02],\n",
       "             [-2.53221183e-03, -1.77537892e-02, -4.25212756e-02]],            dtype=float32)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "print(jnp.min(grads_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Using a non-tuple sequence for multidimensional indexing is not allowed; use `arr[tuple(seq)]` instead of `arr[seq]`. See https://github.com/google/jax/issues/4564 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [58], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m forward_grads_X \u001b[39m=\u001b[39m forward_grad(params, X)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Compute the Jacobian of the loss function with respect to X using the chain rule\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m loss_grads_X \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mdot(grad_fn(params[\u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m], X, y), forward_grads_X)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [57], line 24\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, X, y)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_fn\u001b[39m(params, X, y):\n\u001b[0;32m---> 24\u001b[0m     err \u001b[39m=\u001b[39m forward(params, X) \u001b[39m-\u001b[39m y\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mmean(jnp\u001b[39m.\u001b[39msquare(err))\n",
      "Cell \u001b[0;32mIn [57], line 20\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(params, X)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(params, X):\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mdot(X, params[\u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m+\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:3649\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3643\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(aval, core\u001b[39m.\u001b[39mDShapedArray) \u001b[39mand\u001b[39;00m aval\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m () \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3644\u001b[0m         dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3645\u001b[0m         \u001b[39mnot\u001b[39;00m dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, dtypes\u001b[39m.\u001b[39mbool_) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3646\u001b[0m         \u001b[39misinstance\u001b[39m(arr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mint\u001b[39m)):\n\u001b[1;32m   3647\u001b[0m       \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39mdynamic_index_in_dim(arr, idx, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 3649\u001b[0m treedef, static_idx, dynamic_idx \u001b[39m=\u001b[39m _split_index_for_jit(idx, arr\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m   3650\u001b[0m \u001b[39mreturn\u001b[39;00m _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m   3651\u001b[0m                unique_indices, mode, fill_value)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:3724\u001b[0m, in \u001b[0;36m_split_index_for_jit\u001b[0;34m(idx, shape)\u001b[0m\n\u001b[1;32m   3719\u001b[0m \u001b[39m\"\"\"Splits indices into necessarily-static and dynamic parts.\u001b[39;00m\n\u001b[1;32m   3720\u001b[0m \n\u001b[1;32m   3721\u001b[0m \u001b[39mUsed to pass indices into `jit`-ted function.\u001b[39;00m\n\u001b[1;32m   3722\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3723\u001b[0m \u001b[39m# Convert list indices to tuples in cases (deprecated by NumPy.)\u001b[39;00m\n\u001b[0;32m-> 3724\u001b[0m idx \u001b[39m=\u001b[39m _eliminate_deprecated_list_indexing(idx)\n\u001b[1;32m   3726\u001b[0m \u001b[39m# Expand any (concrete) boolean indices. We can then use advanced integer\u001b[39;00m\n\u001b[1;32m   3727\u001b[0m \u001b[39m# indexing logic to handle them.\u001b[39;00m\n\u001b[1;32m   3728\u001b[0m idx \u001b[39m=\u001b[39m _expand_bool_indices(idx, shape)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:4004\u001b[0m, in \u001b[0;36m_eliminate_deprecated_list_indexing\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m   4000\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4001\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mUsing a non-tuple sequence for multidimensional indexing is not allowed; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4002\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39muse `arr[array(seq)]` instead of `arr[seq]`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4003\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/google/jax/issues/4564 for more information.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 4004\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m   4005\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4006\u001b[0m   idx \u001b[39m=\u001b[39m (idx,)\n",
      "\u001b[0;31mTypeError\u001b[0m: Using a non-tuple sequence for multidimensional indexing is not allowed; use `arr[tuple(seq)]` instead of `arr[seq]`. See https://github.com/google/jax/issues/4564 for more information."
     ]
    }
   ],
   "source": [
    "forward_grad = jax.jacfwd(forward)\n",
    "\n",
    "# Compute the gradients of the forward pass with respect to X\n",
    "forward_grads_X = forward_grad(params, X)\n",
    "\n",
    "# Compute the Jacobian of the loss function with respect to X using the chain rule\n",
    "loss_grads_X = jnp.dot(grad_fn(params['w'], X, y), forward_grads_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdm3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bac022e209f3f8f811ac02bf6d6b971751e1ab224096f1893a92a620959b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
