{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from omegaconf import DictConfig, OmegaConf\n",
    "# import hydra\n",
    "from collections import deque\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "import jax.random as jrandom\n",
    "# from jax.test_util import check_grads\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import optax\n",
    "import distrax\n",
    "import haiku as hk\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from lfiax.flows.nsf import make_nsf\n",
    "\n",
    "from typing import (\n",
    "    Any,\n",
    "    Iterator,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Callable,\n",
    ")\n",
    "\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "OptState = Any\n",
    "\n",
    "\n",
    "def jax_lexpand(A, *dimensions):\n",
    "    \"\"\"Expand tensor, adding new dimensions on left.\"\"\"\n",
    "    if jnp.isscalar(A):\n",
    "        A = A * jnp.ones(dimensions)\n",
    "        return A\n",
    "    shape = tuple(dimensions) + A.shape\n",
    "    A = A[jnp.newaxis, ...]\n",
    "    A = jnp.broadcast_to(A, shape)\n",
    "    return A\n",
    "\n",
    "# ----------------------------------------\n",
    "# Prior simulators\n",
    "# ----------------------------------------\n",
    "def sim_linear_prior(num_samples: int, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Simulate prior samples and return their log_prob.\n",
    "    \"\"\"\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    samples, log_prob = base_distribution.sample_and_log_prob(seed=key, sample_shape=[num_samples])\n",
    "\n",
    "    return samples, log_prob\n",
    "\n",
    "# ----------------------------------------\n",
    "# Likelihood simulators\n",
    "# ----------------------------------------\n",
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Simulate linear model with normal and gamma noise, from Kleinegesse et al. 2020.\n",
    "    \"\"\"\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "def sim_linear_jax_laplace(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Sim linear laplace prior regression model.\n",
    "\n",
    "    Returns: \n",
    "        y: scalar value, or, array of scalars.\n",
    "    \"\"\"\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    concentration = jnp.ones(noise_shape)\n",
    "    rate = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Gamma(concentration, rate).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = distrax.MultivariateNormalDiag(y, jnp.squeeze(n_n)).sample(seed=keys[1], sample_shape=())\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def sim_data_laplace(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    Uses `sim_linear_jax_laplace` function.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "    theta_shape = (1,)\n",
    "\n",
    "    loc = jnp.zeros(theta_shape)\n",
    "    scale = jnp.ones(theta_shape)\n",
    "\n",
    "    # Leaving in case this fixes future dimensionality issues\n",
    "    # base_distribution = distrax.Independent(\n",
    "    #     distrax.Laplace(loc, scale)\n",
    "    # )\n",
    "    base_distribution = distrax.Laplace(loc, scale)\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    y = sim_linear_jax_laplace(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack(\n",
    "        (y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d))))\n",
    "    )\n",
    "\n",
    "\n",
    "def sim_data(d: Array, num_samples: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack(\n",
    "        (y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d))))\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helper functions to simulate data\n",
    "# ----------------------------------------\n",
    "def load_dataset(split: tfds.Split, batch_size: int) -> Iterator[Batch]:\n",
    "    ds = split\n",
    "    ds = ds.shuffle(buffer_size=10 * batch_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=1000)\n",
    "    ds = ds.repeat()\n",
    "    return iter(tfds.as_numpy(ds))\n",
    "\n",
    "\n",
    "def prepare_data(batch: Batch, prng_key: Optional[PRNGKey] = None) -> Array:\n",
    "    # Batch is [y, thetas, d]\n",
    "    data = batch.astype(np.float32)\n",
    "    x = data[:, :len_x]\n",
    "    cond_data = data[:, len_x:]\n",
    "    theta = cond_data[:, :-len_x]\n",
    "    d = cond_data[:, -len_x:-len_xi]\n",
    "    xi = cond_data[:, -len_xi:]\n",
    "    return x, theta, d, xi\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Haiku transform functions for training and evaluation\n",
    "# ----------------------------\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob(data: Array, theta: Array, xi: Array) -> Array:\n",
    "    # Get batch\n",
    "    shift = data.mean(axis=0)\n",
    "    scale = data.std(axis=0) + 1e-14\n",
    "\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "        standardize_x=True,\n",
    "        standardize_theta=False,\n",
    "        use_resnet=True,\n",
    "        event_dim=EVENT_DIM,\n",
    "        shift=shift,\n",
    "        scale=scale,\n",
    "    )\n",
    "    return model.log_prob(data, theta, xi)\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def model_sample(key: PRNGKey, num_samples: int, theta: Array, xi: Array) -> Array:\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "        standardize_x=False,\n",
    "        standardize_theta=False,\n",
    "        use_resnet=True,\n",
    "        event_dim=EVENT_DIM,\n",
    "    )\n",
    "    return model._sample_n(key=key, n=[num_samples], theta=theta, xi=xi)\n",
    "\n",
    "\n",
    "def loss_fn(\n",
    "    params: hk.Params, prng_key: PRNGKey, x: Array, theta: Array, d: Array, xi: Array\n",
    ") -> Array:\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, d, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_fn(params: hk.Params, batch: Batch) -> Array:\n",
    "    x, theta, d, xi = prepare_data(batch)\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def unified_loss_fn(\n",
    "    params: hk.Params, prng_key: PRNGKey, x: Array, theta: Array\n",
    ") -> Array:\n",
    "    xi = jnp.asarray(params['xi'])\n",
    "    xi = jnp.broadcast_to(xi, (len(x), len(xi)))\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "    \n",
    "    # Loss is average negative log likelihood.\n",
    "    loss = -jnp.mean(log_prob.apply(flow_params, x, theta, xi))\n",
    "    return loss\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    x, theta, d, xi = prepare_data(batch)\n",
    "    # Note that `xi` is passed as a parameter to be updated during optimization\n",
    "    grads = jax.grad(unified_loss_fn)(params, prng_key, x, theta)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n",
    "\n",
    "def lfi_pce_eig(params: hk.Params, prng_key: PRNGKey, N: int=100, M: int=10, **kwargs):\n",
    "    keys = jrandom.split(prng_key, 3 + M)\n",
    "    xi = params['xi']\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "\n",
    "    # simulate the outcomes before finding their log_probs\n",
    "    X = sim_data(d_sim, num_samples, keys[0])  # Do I need to split up the prng_key?\n",
    "\n",
    "    # I'm implicitly returning the prior here, that's a little annoying...\n",
    "    x, theta_0, d, xi = prepare_data(X)  # TODO: Maybe refactor this?\n",
    "\n",
    "    conditional_lp = log_prob.apply(flow_params, x, theta_0, d, xi)\n",
    "\n",
    "    contrastive_lps = []\n",
    "    thetas = []\n",
    "    for i in range(M):\n",
    "        theta, _ = sim_linear_prior(num_samples, keys[i + 1])\n",
    "        thetas.append(theta)\n",
    "        contrastive_lp = log_prob.apply(flow_params, x, theta, d, xi)\n",
    "        contrastive_lps.append(contrastive_lp)\n",
    "\n",
    "    marginal_log_prbs = jnp.concatenate((jax_lexpand(conditional_lp, 1), jnp.array(contrastive_lps)))\n",
    "\n",
    "    marginal_lp = jax.nn.logsumexp(marginal_log_prbs, 0) - math.log(M + 1)\n",
    "\n",
    "    return - sum(conditional_lp - marginal_lp) - jnp.mean(conditional_lp)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_pce(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, N: int, M: int\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    grads = jax.grad(lfi_pce_eig)(params, prng_key, N=num_samples, M=inner_samples)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:     0; Xi: [0.]\n",
      "STEP:     0; Validation loss: 3.609\n",
      "STEP:     1; Xi: [7.441119e-05]\n"
     ]
    }
   ],
   "source": [
    "seed = 1231\n",
    "M = 3\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "# d = jnp.array([1.])\n",
    "d = jnp.array([])\n",
    "xi = jnp.array([0.])\n",
    "d_sim = jnp.concatenate((d, xi), axis=0)\n",
    "\n",
    "# Params and hyperparams\n",
    "len_x = len(d_sim)\n",
    "len_d = len(d)\n",
    "len_xi = len(xi)\n",
    "\n",
    "theta_shape = (2,)\n",
    "d_shape = (len(d),)\n",
    "xi_shape = (len_xi,)\n",
    "EVENT_SHAPE = (len(d_sim),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "\n",
    "num_samples = 2\n",
    "inner_samples = 10 # AKA M or L in BOED parlance\n",
    "batch_size = 128\n",
    "flow_num_layers = 1 # 5 #3 # 10\n",
    "mlp_num_layers = 1 # 3 # 4\n",
    "hidden_size = 128 # 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 10\n",
    "early_stopping_memory = 10\n",
    "early_stopping_threshold = 5e-2\n",
    "\n",
    "training_steps = 2\n",
    "eval_frequency = 5\n",
    "\n",
    "# Initialize the params\n",
    "prng_seq = hk.PRNGSequence(42)  # TODO: Put one of \"keys\" here?\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "params['xi'] = xi\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# TODO: put this function in training since d will be changing.\n",
    "X_samples = 512*20\n",
    "X = sim_data(d_sim, X_samples, key)\n",
    "\n",
    "shift = X.mean(axis=0)\n",
    "scale = X.std(axis=0) + 1e-14\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "loss_deque = deque(maxlen=early_stopping_memory)\n",
    "for step in range(training_steps):\n",
    "    # params, opt_state = update_pce(\n",
    "    #     params, next(prng_seq), opt_state, N=num_samples, M=M\n",
    "    # )\n",
    "    params, opt_state = update(\n",
    "        params, next(prng_seq), opt_state, next(train_ds)\n",
    "    )\n",
    "\n",
    "    print(f\"STEP: {step:5d}; Xi: {params['xi']}\")\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "    \n",
    "        loss_deque.append(val_loss)\n",
    "        avg_abs_diff = jnp.mean(abs(jnp.array(loss_deque) - sum(loss_deque)/len(loss_deque)))\n",
    "        if step > warmup_steps and avg_abs_diff < early_stopping_threshold:\n",
    "            break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng_seq = hk.PRNGSequence(42)  # TODO: Put one of \"keys\" here?\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "params['xi'] = xi\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.4411095e-05]\n",
      "[0.00015835]\n"
     ]
    }
   ],
   "source": [
    "print(params['xi'])\n",
    "params, opt_state = update(\n",
    "        params, next(prng_seq), opt_state, next(train_ds)\n",
    "    )\n",
    "print(params['xi'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out new `sim_data` function\n",
    "Want to `vmap` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=1)\n",
    "def sim_linear_data_vmap(d: Array, num_samples: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape (y, thetas, d). The `y` variable can vary in size.\n",
    "    Has a fixed prior.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # Simulating the priors\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # Simulating noise and response\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[1], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 0.5).sample(\n",
    "        seed=keys[2], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jax.vmap(partial(jnp.dot, priors[:, 0]))(d)\n",
    "    y = jax.vmap(partial(jnp.add, priors[:, 1]))(y) + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y.T, jnp.squeeze(priors)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=0)\n",
    "def sim_linear_prior(num_samples: int, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Simulate prior samples and return their log_prob.\n",
    "    \"\"\"\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    samples, log_prob = base_distribution.sample_and_log_prob(seed=key, sample_shape=[num_samples])\n",
    "\n",
    "    return samples, log_prob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making PCE loss with new simulator function\n",
    "Now that I have a simulator that's a little more efficient, can plug that in with the PCE loss to see how training it works.\n",
    "1. Want to do a test to make sure the log probs are working.\n",
    "2. Run 10 training steps and profile its performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works. Before moving on to creating a new `update` function and incorporating the new loss function... I should check to see that the encapsulated loss function is able to get a single loss and gradients w.r.t. all of the appropriate parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.lax as lax\n",
    "\n",
    "@partial(jax.jit, static_argnums=[2,4,5])\n",
    "def lfi_pce_eig_scan(\n",
    "    params: hk.Params, prng_key: PRNGKey, log_prob_fun: Callable, designs: Array, N: int=100, M: int=10,):\n",
    "    def compute_marginal_lp3(keys, log_prob_fun, M, num_samples, x, conditional_lp):\n",
    "        def scan_fun(contrastive_lps, i):\n",
    "            theta, _ = sim_linear_prior(N, keys[i + 1])\n",
    "            contrastive_lp = log_prob_fun(flow_params, x, theta, xi)\n",
    "            contrastive_lps += jnp.exp(contrastive_lp)\n",
    "            return contrastive_lps, i + 1\n",
    "        result = jax.lax.scan(scan_fun, conditional_lp, jnp.array(range(M)))\n",
    "        return jnp.log(result[0])\n",
    "    keys = jrandom.split(prng_key, 3 + M)\n",
    "    xi = jnp.asarray(params['xi'])\n",
    "    xi = jnp.broadcast_to(xi, (N, len(xi)))\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "    # xi = params['xi']\n",
    "    # flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "\n",
    "    # simulate the outcomes before finding their log_probs\n",
    "    x, theta_0 = sim_linear_data_vmap(d_sim, N, keys[0])\n",
    "    # xi_broadcast = jnp.broadcast_to(xi, (N, len(xi)))\n",
    "    \n",
    "    conditional_lp = log_prob_fun(flow_params, x, theta_0, xi)\n",
    "    conditional_lp_exp = jnp.exp(conditional_lp)\n",
    "    marginal_lp = compute_marginal_lp3(keys[1:M+1], log_prob_fun, M, N, x, conditional_lp_exp)\n",
    "    return - sum(conditional_lp - marginal_lp) - jnp.mean(conditional_lp)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=[3,4])\n",
    "def update_pce(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, N: int, M: int, designs: Array,\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    # xi_broadcast = jnp.broadcast_to(params[\"xi\"], (N, len(xi)))\n",
    "    log_prob_fun = lambda params, x, theta, xi: log_prob.apply(\n",
    "        params, x, theta, xi)\n",
    "    # Bingo. This is where to change data generation process\n",
    "    # params_values = tuple(params[key] for key in params)\n",
    "    # flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "    loss, grads = jax.value_and_grad(lfi_pce_eig_scan)(\n",
    "        params, prng_key, log_prob_fun, designs, N=N, M=M)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, loss, grads['xi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "params['xi'] = xi\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.44129e-05]\n",
      "[0.00015999]\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "M = 10\n",
    "print(params['xi'])\n",
    "params, opt_state, loss, xi_grads = update_pce(\n",
    "        params, next(prng_seq), opt_state, N, M, designs = d_sim\n",
    "    )\n",
    "print(params['xi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.04012257], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(247.3694, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=100\n",
    "M=10\n",
    "xi = params['xi']\n",
    "xi_broadcast = jnp.broadcast_to(xi, (N, len(xi)))\n",
    "log_prob_fun_test = lambda x, theta: log_prob.apply(params, x, theta, xi_broadcast)\n",
    "lfi_pce_eig_scan(params, key, log_prob_fun_test, d_sim, N=N, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(247.3694, dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfi_pce_eig_scan(params, key, log_prob_fun_test, d_sim, N=N, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "print(params['xi'])\n",
    "grads = jax.grad(lfi_pce_eig_scan)(params, key, log_prob_fun_test, d_sim, N=N, M=M)\n",
    "updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "new_params = optax.apply_updates(params, updates)\n",
    "print(new_params['xi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:     0; Xi: [10.004369]; Loss: 243.20777893066406\n",
      "STEP:     1; Xi: [10.004406]; Loss: 243.22930908203125\n",
      "STEP:     2; Xi: [10.00444]; Loss: 243.2487335205078\n",
      "STEP:     3; Xi: [10.004471]; Loss: 243.26617431640625\n",
      "STEP:     4; Xi: [10.0044985]; Loss: 243.2819366455078\n",
      "STEP:     5; Xi: [10.004524]; Loss: 243.29620361328125\n",
      "STEP:     6; Xi: [10.004547]; Loss: 243.30908203125\n",
      "STEP:     7; Xi: [10.004568]; Loss: 243.32064819335938\n",
      "STEP:     8; Xi: [10.004587]; Loss: 243.33119201660156\n",
      "STEP:     9; Xi: [10.004604]; Loss: 243.3407440185547\n",
      "STEP:    10; Xi: [10.00462]; Loss: 243.34930419921875\n",
      "STEP:    11; Xi: [10.004634]; Loss: 243.3569793701172\n",
      "STEP:    12; Xi: [10.004646]; Loss: 243.36404418945312\n",
      "STEP:    13; Xi: [10.004658]; Loss: 243.37033081054688\n",
      "STEP:    14; Xi: [10.004668]; Loss: 243.37612915039062\n",
      "STEP:    15; Xi: [10.004678]; Loss: 243.3813018798828\n",
      "STEP:    16; Xi: [10.004686]; Loss: 243.38597106933594\n",
      "STEP:    17; Xi: [10.004694]; Loss: 243.39028930664062\n",
      "STEP:    18; Xi: [10.004701]; Loss: 243.39410400390625\n",
      "STEP:    19; Xi: [10.004707]; Loss: 243.39756774902344\n",
      "STEP:    20; Xi: [10.004713]; Loss: 243.40072631835938\n",
      "STEP:    21; Xi: [10.004719]; Loss: 243.403564453125\n",
      "STEP:    22; Xi: [10.004724]; Loss: 243.40621948242188\n",
      "STEP:    23; Xi: [10.004728]; Loss: 243.4084930419922\n",
      "STEP:    24; Xi: [10.004732]; Loss: 243.41067504882812\n",
      "STEP:    25; Xi: [10.004736]; Loss: 243.41250610351562\n",
      "STEP:    26; Xi: [10.004739]; Loss: 243.41424560546875\n",
      "STEP:    27; Xi: [10.004742]; Loss: 243.41580200195312\n",
      "STEP:    28; Xi: [10.004745]; Loss: 243.41725158691406\n",
      "STEP:    29; Xi: [10.004747]; Loss: 243.41851806640625\n",
      "STEP:    30; Xi: [10.004749]; Loss: 243.4196319580078\n",
      "STEP:    31; Xi: [10.004751]; Loss: 243.42071533203125\n",
      "STEP:    32; Xi: [10.004753]; Loss: 243.4216766357422\n",
      "STEP:    33; Xi: [10.004755]; Loss: 243.4225616455078\n",
      "STEP:    34; Xi: [10.004757]; Loss: 243.42335510253906\n",
      "STEP:    35; Xi: [10.004758]; Loss: 243.4240264892578\n",
      "STEP:    36; Xi: [10.004759]; Loss: 243.42469787597656\n",
      "STEP:    37; Xi: [10.00476]; Loss: 243.42529296875\n",
      "STEP:    38; Xi: [10.004761]; Loss: 243.42578125\n",
      "STEP:    39; Xi: [10.004762]; Loss: 243.4263153076172\n",
      "STEP:    40; Xi: [10.004763]; Loss: 243.42672729492188\n",
      "STEP:    41; Xi: [10.004764]; Loss: 243.42709350585938\n",
      "STEP:    42; Xi: [10.004765]; Loss: 243.42745971679688\n",
      "STEP:    43; Xi: [10.0047655]; Loss: 243.42779541015625\n",
      "STEP:    44; Xi: [10.004766]; Loss: 243.42808532714844\n",
      "STEP:    45; Xi: [10.004767]; Loss: 243.42831420898438\n",
      "STEP:    46; Xi: [10.004767]; Loss: 243.42855834960938\n",
      "STEP:    47; Xi: [10.004767]; Loss: 243.42877197265625\n",
      "STEP:    48; Xi: [10.004767]; Loss: 243.4289093017578\n",
      "STEP:    49; Xi: [10.004767]; Loss: 243.42910766601562\n",
      "STEP:    50; Xi: [10.004767]; Loss: 243.4292755126953\n",
      "STEP:    51; Xi: [10.004767]; Loss: 243.4293670654297\n",
      "STEP:    52; Xi: [10.004767]; Loss: 243.42955017089844\n",
      "STEP:    53; Xi: [10.004767]; Loss: 243.42965698242188\n",
      "STEP:    54; Xi: [10.004767]; Loss: 243.42974853515625\n",
      "STEP:    55; Xi: [10.004767]; Loss: 243.42984008789062\n",
      "STEP:    56; Xi: [10.004767]; Loss: 243.42994689941406\n",
      "STEP:    57; Xi: [10.004767]; Loss: 243.43002319335938\n",
      "STEP:    58; Xi: [10.004767]; Loss: 243.4300994873047\n",
      "STEP:    59; Xi: [10.004767]; Loss: 243.43016052246094\n",
      "STEP:    60; Xi: [10.004767]; Loss: 243.43019104003906\n",
      "STEP:    61; Xi: [10.004767]; Loss: 243.43026733398438\n",
      "STEP:    62; Xi: [10.004767]; Loss: 243.4302978515625\n",
      "STEP:    63; Xi: [10.004767]; Loss: 243.43031311035156\n",
      "STEP:    64; Xi: [10.004767]; Loss: 243.43043518066406\n",
      "STEP:    65; Xi: [10.004767]; Loss: 243.43045043945312\n",
      "STEP:    66; Xi: [10.004767]; Loss: 243.43048095703125\n",
      "STEP:    67; Xi: [10.004767]; Loss: 243.43051147460938\n",
      "STEP:    68; Xi: [10.004767]; Loss: 243.43051147460938\n",
      "STEP:    69; Xi: [10.004767]; Loss: 243.4305419921875\n",
      "STEP:    70; Xi: [10.004767]; Loss: 243.43057250976562\n",
      "STEP:    71; Xi: [10.004767]; Loss: 243.43060302734375\n",
      "STEP:    72; Xi: [10.004767]; Loss: 243.4306182861328\n",
      "STEP:    73; Xi: [10.004767]; Loss: 243.43060302734375\n",
      "STEP:    74; Xi: [10.004767]; Loss: 243.43064880371094\n",
      "STEP:    75; Xi: [10.004767]; Loss: 243.43064880371094\n",
      "STEP:    76; Xi: [10.004767]; Loss: 243.43063354492188\n",
      "STEP:    77; Xi: [10.004767]; Loss: 243.4306640625\n",
      "STEP:    78; Xi: [10.004767]; Loss: 243.4306640625\n",
      "STEP:    79; Xi: [10.004767]; Loss: 243.4306640625\n",
      "STEP:    80; Xi: [10.004767]; Loss: 243.4306640625\n",
      "STEP:    81; Xi: [10.004767]; Loss: 243.4306640625\n",
      "STEP:    82; Xi: [10.004767]; Loss: 243.43069458007812\n",
      "STEP:    83; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    84; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    85; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    86; Xi: [10.004767]; Loss: 243.43069458007812\n",
      "STEP:    87; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    88; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    89; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    90; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    91; Xi: [10.004767]; Loss: 243.4307403564453\n",
      "STEP:    92; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    93; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    94; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    95; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    96; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    97; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    98; Xi: [10.004767]; Loss: 243.43072509765625\n",
      "STEP:    99; Xi: [10.004767]; Loss: 243.4307403564453\n"
     ]
    }
   ],
   "source": [
    "N=100\n",
    "M=10\n",
    "\n",
    "@partial(jax.jit, static_argnums=[3,4])\n",
    "def update_pce(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, N: int, M: int, designs: Array,\n",
    ") -> Tuple[hk.Params, OptState, Array]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    xi_broadcast = jnp.broadcast_to(params[\"xi\"], (N, len(xi)))\n",
    "    log_prob_fun = lambda x, theta: log_prob.apply(\n",
    "        params, x, theta, xi_broadcast)\n",
    "    loss, grads = jax.value_and_grad(lfi_pce_eig_scan)(\n",
    "        params, key, log_prob_fun, designs, N=N, M=M)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, loss\n",
    "\n",
    "training_steps = 100\n",
    "for step in range(training_steps):\n",
    "    params, opt_state, loss = update_pce(\n",
    "        params, next(prng_seq), opt_state, N, M, designs = d_sim\n",
    "    )\n",
    "    d_sim = jnp.concatenate((d, params['xi']), axis=0)\n",
    "\n",
    "    print(f\"STEP: {step:5d}; Xi: {params['xi']}; Loss: {loss}\")\n",
    "    # if step % eval_frequency == 0:\n",
    "    #     val_loss = eval_fn(params, next(valid_ds))\n",
    "    #     print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "    \n",
    "    #     loss_deque.append(val_loss)\n",
    "    #     avg_abs_diff = jnp.mean(abs(jnp.array(loss_deque) - sum(loss_deque)/len(loss_deque)))\n",
    "    #     if step > warmup_steps and avg_abs_diff < early_stopping_threshold:\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument '<function make_jax_compatible.<locals>.wrapped at 0x7fa2325995e0>' of type <class 'function'> is not a valid JAX type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [291], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m params \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray([\u001b[39m1.\u001b[39m, \u001b[39m2.\u001b[39m, \u001b[39m3.\u001b[39m])\n\u001b[1;32m     20\u001b[0m log_prob_fun \u001b[39m=\u001b[39m make_jax_compatible(log_prob)\n\u001b[0;32m---> 22\u001b[0m \u001b[39mprint\u001b[39m(loss_fn(params, log_prob_fun))\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/api.py:2993\u001b[0m, in \u001b[0;36m_check_arg\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_arg\u001b[39m(arg):\n\u001b[1;32m   2992\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(arg, core\u001b[39m.\u001b[39mTracer) \u001b[39mor\u001b[39;00m _valid_jaxtype(arg)):\n\u001b[0;32m-> 2993\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mArgument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00marg\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(arg)\u001b[39m}\u001b[39;00m\u001b[39m is not a valid JAX type.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument '<function make_jax_compatible.<locals>.wrapped at 0x7fa2325995e0>' of type <class 'function'> is not a valid JAX type."
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from functools import partial\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, log_prob_fun):\n",
    "  x = jnp.array([1., 2., 3.])\n",
    "  return -jnp.sum(log_prob_fun(params, x))\n",
    "\n",
    "def make_jax_compatible(haiku_func):\n",
    "  def wrapped(params, x):\n",
    "    return haiku_func(params, x)\n",
    "  return wrapped\n",
    "\n",
    "@hk.transform\n",
    "def log_prob(params, x):\n",
    "  return jnp.sum(params * x)\n",
    "\n",
    "params = jnp.array([1., 2., 3.])\n",
    "log_prob_fun = make_jax_compatible(log_prob)\n",
    "\n",
    "print(loss_fn(params, log_prob_fun))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `vmap`ing `distrax`\n",
    "Want to generate an array of priors from a distribution. Really, I need to pass an array of random keys that's used to generate the samples, then concatenate however i like using the `out_axes` of the `vmap` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = jax.vmap(partial(jnp.dot, priors[:, 0]))(d)\n",
    "@partial(jax.jit, static_argnums=[0])\n",
    "def sim_linear_prior(num_samples: int, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Simulate prior samples and return their log_prob.\n",
    "    \"\"\"\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    samples, log_prob = base_distribution.sample_and_log_prob(seed=key, sample_shape=[num_samples])\n",
    "\n",
    "    return samples, log_prob\n",
    "\n",
    "\n",
    "# y = jax.vmap(partial(jnp.dot, priors[:, 0]))(d)\n",
    "@partial(jax.jit, static_argnums=[0,1])\n",
    "def sim_linear_prior_M_samples(num_samples: int, M: int, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Simulate prior samples and return their log_prob.\n",
    "    \"\"\"\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    samples, log_prob = base_distribution.sample_and_log_prob(seed=key, sample_shape=[M, num_samples])\n",
    "\n",
    "    return samples, log_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy3 = sim_linear_prior_M_samples(num_samples=num_samples, M=M, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000, 2)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy3[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy3[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_keys = jrandom.split(key, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "testy = jax.vmap(partial(sim_linear_prior, num_samples))(test_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5, 2)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to get the `log_prob` of the 10 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_broadcast = jnp.broadcast_to(xi, (num_samples, len(xi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.broadcast_to(x[:,0], (num_samples, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy2 = jax.vmap(lambda theta: log_prob.apply(params, x, theta, xi_broadcast))(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy2.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing it out\n",
    "Got vmapped version set up, time to test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @partial(jax.jit, static_argnums=[2,3])\n",
    "def lfi_pce_eig(params: hk.Params, prng_key: PRNGKey, N: int=100, M: int=10, **kwargs):\n",
    "    keys = jrandom.split(prng_key, 3 + M)\n",
    "    xi = params['xi']\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "\n",
    "    # simulate the outcomes before finding their log_probs\n",
    "    x, theta_0 = sim_linear_data_vmap(d_sim, num_samples, keys[0])\n",
    "    xi_broadcast = jnp.broadcast_to(xi, (num_samples, len(xi)))\n",
    "\n",
    "    conditional_lp = log_prob.apply(flow_params, x, theta_0, xi_broadcast)\n",
    "\n",
    "    contrastive_lps = []\n",
    "    thetas = []\n",
    "    # TODO: Make this a vmapped expression\n",
    "    for i in range(M):\n",
    "        theta, _ = sim_linear_prior(num_samples, keys[i + 1])\n",
    "        thetas.append(theta)\n",
    "        contrastive_lp = log_prob.apply(flow_params, x, theta, xi_broadcast)\n",
    "        contrastive_lps.append(contrastive_lp)\n",
    "\n",
    "    marginal_log_prbs = jnp.concatenate((jax_lexpand(conditional_lp, 1), jnp.array(contrastive_lps)))\n",
    "\n",
    "    marginal_lp = jax.nn.logsumexp(marginal_log_prbs, 0) - math.log(M + 1)\n",
    "\n",
    "    return - sum(conditional_lp - marginal_lp) - jnp.mean(conditional_lp)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=[2,3])\n",
    "def lfi_pce_eig_vmap(params: hk.Params, prng_key: PRNGKey, N: int=100, M: int=10, **kwargs):\n",
    "    keys = jrandom.split(prng_key, 2)\n",
    "    xi = params['xi']\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "\n",
    "    # simulate the outcomes before finding their log_probs\n",
    "    x, theta_0 = sim_linear_data_vmap(d_sim, num_samples, keys[0])\n",
    "\n",
    "    xi_broadcast = jnp.broadcast_to(xi, (num_samples, len(xi)))\n",
    "\n",
    "    conditional_lp = log_prob.apply(flow_params, x, theta_0, xi_broadcast)\n",
    "\n",
    "    # TODO: Make function that returns M x num_samples priors\n",
    "    thetas, log_probs = sim_linear_prior_M_samples(num_samples=num_samples, M=M, key=keys[1])\n",
    "    \n",
    "    # conditional_lp could be the initial starting state that is added upon... \n",
    "    contrastive_lps = jax.vmap(lambda theta: log_prob.apply(params, x, theta, xi_broadcast))(thetas)\n",
    "    marginal_log_prbs = jnp.concatenate((jax_lexpand(conditional_lp, 1), jnp.array(contrastive_lps)))\n",
    "    marginal_lp = jax.nn.logsumexp(marginal_log_prbs, 0) - math.log(M + 1)\n",
    "    # marginal_lp = compute_marginal_lp3(M, num_samples, key, flow_params, x, xi_broadcast, conditional_lp)\n",
    "\n",
    "    return - sum(conditional_lp - marginal_lp) - jnp.mean(conditional_lp)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=[2,3])\n",
    "def lfi_pce_eig_vmap2(params: hk.Params, prng_key: PRNGKey, N: int=100, M: int=10, **kwargs):\n",
    "    keys = jrandom.split(prng_key, M + 1)\n",
    "    xi = params['xi']\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "\n",
    "    # simulate the outcomes before finding their log_probs\n",
    "    x, theta_0 = sim_linear_data_vmap(d_sim, num_samples, keys[0])\n",
    "\n",
    "    xi_broadcast = jnp.broadcast_to(xi, (num_samples, len(xi)))\n",
    "\n",
    "    conditional_lp = log_prob.apply(flow_params, x, theta_0, xi_broadcast)\n",
    "\n",
    "    thetas, log_probs = jax.vmap(partial(sim_linear_prior, num_samples))(keys[1:M+1])\n",
    "    \n",
    "    # conditional_lp could be the initial starting state that is added upon... \n",
    "    contrastive_lps = jax.vmap(lambda theta: log_prob.apply(params, x, theta, xi_broadcast))(thetas)\n",
    "    marginal_log_prbs = jnp.concatenate((jax_lexpand(conditional_lp, 1), jnp.array(contrastive_lps)))\n",
    "    marginal_lp = jax.nn.logsumexp(marginal_log_prbs, 0) - math.log(M + 1)\n",
    "    # marginal_lp = compute_marginal_lp3(M, num_samples, key, flow_params, x, xi_broadcast, conditional_lp)\n",
    "\n",
    "    return - sum(conditional_lp - marginal_lp) - jnp.mean(conditional_lp)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_pce(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, N: int, M: int\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    grads = jax.grad(lfi_pce_eig)(params, prng_key, N=num_samples, M=inner_samples)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(7.6992054, dtype=float32)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfi_pce_eig_vmap(params, key, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(7.64139, dtype=float32)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfi_pce_eig_vmap2(params, key, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 22:06:54.540302: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2m7.268412s\n",
      "\n",
      "********************************\n",
      "[Compiling module jit_backward_pass] Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "grads = jax.grad(lfi_pce_eig_vmap)(params, key, N=N, M=M)\n",
    "updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "new_params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = jax.grad(lfi_pce_eig_vmap)(params, key, N=N, M=M)\n",
    "updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "new_params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 22:11:15.249467: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2m0.762934s\n",
      "\n",
      "********************************\n",
      "[Compiling module jit_backward_pass] Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "grads = jax.grad(lfi_pce_eig_vmap2)(params, key, N=N, M=M)\n",
    "updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "new_params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = jax.grad(lfi_pce_eig_vmap2)(params, key, N=N, M=M)\n",
    "updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "new_params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.grad(unified_loss_fn)(params, key, N=3, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 13:11:17.239048: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] \n",
      "********************************\n",
      "[Compiling module jit_update_pce] Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "seed = 1231\n",
    "M = 3\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "d = jnp.array([1.])\n",
    "xi = jnp.array([0.])\n",
    "d_sim = jnp.concatenate((d, xi), axis=0)\n",
    "\n",
    "# helper variables for `prepare_tf_dataset`\n",
    "len_x = len(d_sim)\n",
    "len_d = len(d)\n",
    "# Actually necessary value for `sim_data` output\n",
    "len_xi = len(xi)\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "d_shape = (len(d),)\n",
    "xi_shape = (len_xi,)\n",
    "EVENT_SHAPE = (len(d_sim),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "\n",
    "num_samples = 2\n",
    "inner_samples = 10 # AKA M or L in BOED parlance\n",
    "batch_size = 128\n",
    "flow_num_layers = 5 #3 # 10\n",
    "mlp_num_layers = 1 # 3 # 4\n",
    "hidden_size = 128 # 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 10\n",
    "early_stopping_memory = 10\n",
    "early_stopping_threshold = 5e-2\n",
    "\n",
    "training_steps = 100\n",
    "eval_frequency = 5\n",
    "\n",
    "prng_seq = hk.PRNGSequence(42)  # TODO: Put one of \"keys\" here?\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "params['xi'] = xi\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "loss_deque = deque(maxlen=early_stopping_memory)\n",
    "for step in range(training_steps):\n",
    "    params, opt_state = update_pce(\n",
    "        params, next(prng_seq), opt_state, N=10, M=M\n",
    "    )\n",
    "    # params, opt_state = update(\n",
    "    #     params, next(prng_seq), opt_state, next(train_ds)\n",
    "    # )\n",
    "\n",
    "    print(f\"STEP: {step:5d}; Xi: {params['xi']}\")\n",
    "    # if step % eval_frequency == 0:\n",
    "    #     val_loss = eval_fn(params, next(valid_ds))\n",
    "    #     print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "    \n",
    "    #     loss_deque.append(val_loss)\n",
    "    #     avg_abs_diff = jnp.mean(abs(jnp.array(loss_deque) - sum(loss_deque)/len(loss_deque)))\n",
    "    #     if step > warmup_steps and avg_abs_diff < early_stopping_threshold:\n",
    "    #         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Gaussian input samples\n",
    "Want to plot the gaussian samples that are used to sample new values. To compare, also want to plot some of the original samples... It's like we want to see how the _path_ changes from the input to output samples based on the conditioning variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_test = X[:10, len_x:-len_x]\n",
    "xi_test = X[:10, -len_xi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = model_sample.apply(params, \n",
    "                    next(prng_seq),\n",
    "                    num_samples=len(theta_test),\n",
    "                    theta=theta_test,\n",
    "                    # d=d_test,\n",
    "                    # d=d_obs,\n",
    "                    xi=xi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "# plt.hist(samples)[-1]\n",
    "\n",
    "density_1 = gaussian_kde(samples[:, 0])\n",
    "# density_2 = gaussian_kde(samples[:, 1])\n",
    "\n",
    "\n",
    "# Plot the density\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(jnp.min(samples), jnp.max(samples), 100)\n",
    "ax.plot(x, density_1(x), label='x1')\n",
    "# ax.plot(x, density_2(x), label='x2')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just messing it up by changing the $\\xi$ input value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2 = model_sample.apply(params, \n",
    "                    next(prng_seq),\n",
    "                    num_samples=len(theta_test),\n",
    "                    theta=theta_test,\n",
    "                    # d=d_test,\n",
    "                    # d=d_obs,\n",
    "                    xi=xi_test+3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "density_1 = gaussian_kde(samples2[:, 0])\n",
    "# density_2 = gaussian_kde(samples[:, 1])\n",
    "\n",
    "\n",
    "# Plot the density\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(jnp.min(samples2), jnp.max(samples2), 100)\n",
    "ax.plot(x, density_1(x), label='x1')\n",
    "# ax.plot(x, density_2(x), label='x2')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(False, dtype=bool)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(samples == samples2).any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so changing the $\\xi$ value changes the sampled distribution. What about the $\\theta$ samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_test2 = jnp.flip(theta_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples3 = model_sample.apply(params, \n",
    "                    next(prng_seq),\n",
    "                    num_samples=len(theta_test),\n",
    "                    theta=theta_test2,\n",
    "                    # d=d_test,\n",
    "                    # d=d_obs,\n",
    "                    xi=xi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "density_1 = gaussian_kde(samples3[:, 0])\n",
    "# density_2 = gaussian_kde(samples[:, 1])\n",
    "\n",
    "\n",
    "# Plot the density\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(jnp.min(samples3), jnp.max(samples3), 100)\n",
    "ax.plot(x, density_1(x), label='x1')\n",
    "# ax.plot(x, density_2(x), label='x2')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(False, dtype=bool)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(samples == samples2).any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alriiiight, so changing the conditioning information yields... different samples. But what about log_probs?\n",
    "\n",
    "## Testing Log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2.7751296, dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# samples's log_prob should be different under different conditioning variables\n",
    "-jnp.mean(log_prob.apply(params, samples, theta_test, xi_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2.768444, dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-jnp.mean(log_prob.apply(params, samples, theta_test, xi_test + 3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2.7901638, dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-jnp.mean(log_prob.apply(params, samples, jnp.flip(theta_test, 1), xi_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log_probs for simulated data\n",
    "The above verified that log_probs for _generated_ data acted like I thought whenever applying the log_prob to it using flow params. However, this doesn't look at _simulated_ data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.],\n",
       "             [0.],\n",
       "             [0.],\n",
       "             [0.],\n",
       "             [0.],\n",
       "             [0.],\n",
       "             [0.],\n",
       "             [0.],\n",
       "             [0.],\n",
       "             [0.]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim_data(d_sim, num_samples=len(theta_test), key=key)  # Do I need to split up the prng_key?\n",
    "\n",
    "# I'm implicitly returning the prior here, that's a little annoying...\n",
    "x, theta_0, d, xi = prepare_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(7.13578, dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-jnp.mean(log_prob.apply(params, x, theta_0, xi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(7.148746, dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-jnp.mean(log_prob.apply(params, x, jnp.flip(theta_0, axis=1), xi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(7.1353607, dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-jnp.mean(log_prob.apply(params, x, theta_0, xi + 3.))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log_probs work\n",
    "Why not when I've tried the PCE loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "keys = jrandom.split(key, 3 + M)\n",
    "num_samples = 10\n",
    "\n",
    "# xi = params['xi']\n",
    "# flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "\n",
    "X = sim_data(d_sim, num_samples, keys[0])  # Do I need to split up the prng_key?\n",
    "\n",
    "# I'm implicitly returning the prior here, that's a little annoying...\n",
    "x, theta_0, d, xi = prepare_data(X)\n",
    "\n",
    "contrastive_lps = []\n",
    "thetas = []\n",
    "for i in range(M):\n",
    "    theta, _ = sim_linear_prior(num_samples, keys[i + 1])\n",
    "    thetas.append(theta)\n",
    "    contrastive_lp = log_prob.apply(params, x, theta, xi)\n",
    "    contrastive_lps.append(contrastive_lp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_lps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(7.830993, dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_lp = log_prob.apply(params, x, theta_0, xi)\n",
    "\n",
    "marginal_log_prbs = jnp.concatenate((jax_lexpand(conditional_lp, 1), jnp.array(contrastive_lps)))\n",
    "\n",
    "marginal_lp = jax.nn.logsumexp(marginal_log_prbs, 0) - math.log(M + 1)\n",
    "\n",
    "sum(conditional_lp - marginal_lp) - jnp.mean(contrastive_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-6.6525245, -7.334112 , -6.8119946, -6.5381484, -6.567997 ,\n",
       "              -6.8708863, -6.199002 , -6.5186396, -7.452368 , -7.3830347],\n",
       "             [-6.569809 , -7.559225 , -6.725845 , -6.526496 , -6.673326 ,\n",
       "              -6.896696 , -6.3781633, -6.7583585, -7.5782037, -8.794767 ],\n",
       "             [-6.6859007, -7.4570117, -6.834794 , -6.958585 , -6.5040655,\n",
       "              -7.058741 , -6.224936 , -6.542375 , -7.626199 , -7.5463586],\n",
       "             [-6.6079445, -7.33452  , -6.718301 , -6.4771147, -6.3280044,\n",
       "              -6.9939547, -6.158974 , -6.495627 , -7.480525 , -9.910831 ],\n",
       "             [-6.6840515, -7.3341107, -6.8081636, -6.633423 , -6.475223 ,\n",
       "              -7.1661158, -6.1138706, -6.7829494, -7.5835342, -8.585045 ],\n",
       "             [-6.7376723, -7.3368926, -6.7818146, -6.4458237, -6.741234 ,\n",
       "              -6.768721 , -6.0479784, -6.5972233, -7.5532255, -8.240985 ],\n",
       "             [-6.651277 , -7.3343854, -6.7848053, -6.3908324, -6.4510765,\n",
       "              -7.098256 , -6.3482113, -6.5958643, -7.825919 , -7.3351045],\n",
       "             [-6.6577835, -7.3503366, -6.7124453, -6.578536 , -6.537038 ,\n",
       "              -6.89886  , -6.20527  , -6.7031784, -7.6644335, -8.38169  ],\n",
       "             [-6.7440867, -7.342089 , -6.719078 , -6.4508276, -6.4263077,\n",
       "              -6.7796764, -6.136995 , -6.9280157, -7.6112967, -9.13043  ],\n",
       "             [-6.713032 , -7.3366423, -6.8238745, -6.526683 , -6.5593596,\n",
       "              -6.9868894, -6.3152485, -6.513795 , -7.6857843, -7.1407127],\n",
       "             [-6.6462307, -7.334083 , -6.862208 , -6.4584756, -6.4299183,\n",
       "              -7.015811 , -6.222979 , -6.550604 , -7.380868 , -8.6483755]],            dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginal_log_prbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-6.666951 , -7.366186 , -6.7790036, -6.534328 , -6.511456 ,\n",
       "             -6.950374 , -6.2091656, -6.626684 , -7.5789866, -7.98161  ],            dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginal_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(6.078916, dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-sum(conditional_lp - marginal_lp) - jnp.mean(contrastive_lp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, this function works as expected. Now time to scale it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lfi_pce_eig(params: hk.Params, prng_key: PRNGKey, N: int=100, M: int=10, **kwargs):\n",
    "    keys = jrandom.split(prng_key, 1 + M)\n",
    "    xi = jnp.squeeze(params['xi'])\n",
    "    # flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "\n",
    "    # simulate the outcomes before finding their log_probs\n",
    "    # TODO: should `d_sim` be here? Or how should designs be influenced?\n",
    "    X = sim_data(d_sim, num_samples, keys[0])  # Do I need to split up the prng_key?\n",
    "\n",
    "    # I'm implicitly returning the prior here, that's a little annoying...\n",
    "    x, theta_0, _, xi = prepare_data(X)  # TODO: Maybe refactor this?\n",
    "\n",
    "    conditional_lp = log_prob.apply(params, x, theta_0, xi)\n",
    "\n",
    "    contrastive_lps = []\n",
    "    thetas = []\n",
    "    # TODO: can this be parallelized to speed up the computation?\n",
    "    for i in range(M):\n",
    "        theta, _ = sim_linear_prior(num_samples, keys[i + 1])\n",
    "        thetas.append(theta)\n",
    "        contrastive_lp = log_prob.apply(params, x, theta, xi)\n",
    "        contrastive_lps.append(contrastive_lp)\n",
    "\n",
    "    marginal_log_prbs = jnp.concatenate((jax_lexpand(conditional_lp, 1), jnp.array(contrastive_lps)))\n",
    "\n",
    "    marginal_lp = jax.nn.logsumexp(marginal_log_prbs, 0) - math.log(M + 1)\n",
    "\n",
    "    return - sum(conditional_lp - marginal_lp) - jnp.mean(contrastive_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(-0.00022329, dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.squeeze(params['xi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.00022329], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['xi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['scalar_conditioner_module/linear', 'scalar_conditioner_module/linear_1', 'scalar_conditioner_module/mlp/~/linear_0', 'xi'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'conditioner_module/mlp_2/~/linear_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m params[\u001b[39m'\u001b[39;49m\u001b[39mconditioner_module/mlp_2/~/linear_0\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'conditioner_module/mlp_2/~/linear_0'"
     ]
    }
   ],
   "source": [
    "params['conditioner_module/mlp_2/~/linear_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and checking outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.expand_dims(X[:, X.shape[1] // 2], -1) == jnp.expand_dims(X[:, 2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[5.],\n",
       "             [5.],\n",
       "             [5.],\n",
       "             ...,\n",
       "             [5.],\n",
       "             [5.],\n",
       "             [5.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, -len_xi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABW3UlEQVR4nO3deZxVdeH/8de5d+be2fd9GBh2VJAdxA1M3LVQM1JTI7MyKf3y65uSJi0mLWaUqZh9TSNN00rLTFMSRcUNGEV2gYFh9oXZl7ud3x/nzsgo4DDcO2fm3vfz8Zhwzty5930xhzef7RimaZqIiIiIRAiH3QFEREREQknlRkRERCKKyo2IiIhEFJUbERERiSgqNyIiIhJRVG5EREQkoqjciIiISESJsTvAQAsEAlRUVJCcnIxhGHbHERERkT4wTZOWlhYKCgpwOI48NhN15aaiooKioiK7Y4iIiEg/lJWVMWzYsCM+JurKTXJyMmD95qSkpNicRkRERPqiubmZoqKinj/HjyTqyk33VFRKSorKjYiIyBDTlyUlWlAsIiIiEUXlRkRERCKKyo2IiIhElKhbcyMiIjJY+f1+vF6v3TFs43K5PnWbd1+o3IiIiNjMNE2qqqpobGy0O4qtHA4HI0eOxOVyHdPzqNyIiIjYrLvY5OTkkJCQEJWHzHYfsltZWcnw4cOP6fdA5UZERMRGfr+/p9hkZmbaHcdW2dnZVFRU4PP5iI2N7ffzaEGxiIiIjbrX2CQkJNicxH7d01F+v/+YnkflRkREZBCIxqmojwvV74HKjYiIiEQUlRsRERGJKCo3IiIiElFUbkREjoJpmnR4jm2xo0g0qKys5IorrmDcuHE4HA5uuummAXttlRsRkaNw53NbmfzD//DfbdV2RxEZ1Lq6usjOzua2225j8uTJA/raKjciIn3U4fHz2Fv78PgDfPep96lv7bI7kkQo0zRp9/hs+TBNs08Za2trycvL48477+y59sYbb+ByuVi9ejXFxcX8+te/5uqrryY1NTVcv1WHpEP8RET66MWt1bQFp6TqWj187++bWPml6drCKyHX4fVz/O0v2PLaW350DgmuT68H2dnZPPTQQyxYsICzzz6b8ePHc9VVV7F48WLOPPPMAUh6eBq5ERHpo2c2lgNw1vG5xDgMXthczd+D10Si0fnnn891113HlVdeyTe+8Q0SExNZvny53bE0ciMi0hcNbR5e2VELwM3njmfysFTu+s8Olv1jMyeNyqQgLd7mhBJJ4mOdbPnROba99tG46667mDhxIk8++STr16/H7XaHKVnfqdyIiPTBvzZV4guYnFCQwpicZL4xN5GXttZQUtbI/z71Hqu+MhuHQ9NTEhqGYfRpamgw2LVrFxUVFQQCAUpLS5k0aZLdkTQtJSLSF91TUgumFAIQ43Rw9xcmExfr4PUP6/njulIb04nYw+Px8KUvfYmFCxfy4x//mK9+9avU1NTYHUvlRkTk05Q1tPPu3gMYBnx2SkHP9VHZSSw97zgAfvvyLrviidjm1ltvpampid/85jfcfPPNjBs3jq985Ss9Xy8pKaGkpITW1lZqa2spKSlhy5YtYc+lciMi8imeKbFGbU4ZlU7uhhXw+/lQtxOAS6cPA6CutYvmTq9dEUUG3Jo1a1ixYgWrVq0iJSUFh8PBqlWrWLt2Lffffz8AU6dOZerUqaxfv57HHnuMqVOncv7554c929CY0BMRsYlpmjxdUkEsPn7Cb2HNv6wvvPA9uPJJktwxpCfEcqDdy/6GDo4viLU3sMgAmTdvHl5v70JfXFxMU1NTz+d9PTMn1DRyIyJyBJsrmqmsqeVh1y8YUf4vcMSA4YSd/4HS1wEoykgAoOxAu51RRSRI5UZE5Aheeud9nnD9mFMcmyA2ES5/AqZfY31x9Q/BNClKD5abBpUbkcFA01IiIofhbzvAZSVfodBRQ5c7E/fVT0HhNMg9AUr+DGVvwY7nGZZRDMD+Ax32BhYRQCM3IiKHVbb2jxRSQwVZcO1/rGIDkJIPs79u/fPqHzE8zTq0TCM3IoODyo2IyGE4t1uLh9/MugR3zpjeXzz1JohLhZotTGt+CdCaG5HBQuVGRORQOg6Qf+BdAHxjD7F1NT4dTrkJgDEf/IZYfJQ1dNi2O0REPqJyIyJyCOaO/xCDn+2BYYyeMPnQD5r9DUjKI7aljCucq+nw+qlv8wxsUBH5BJUbEZFD6Nj0DwBWmzM4oSD10A9yJcDc7wLwjVhrCkvrbkTsp3IjIvJx3k5ce1YDsCtjHnFHukvy5C8CBvnUkUUTZdoxJWI7lRsRkY/bvYYYfwcVZgbJo2Ye+bGuRMgcDcBxjr3s16JiEQD+9re/cdZZZ5GdnU1KSgpz5szhhRdeGJDXVrkREfm4bc8C8B//DKaOSP/0x+dNAuB4Yy9lDRq5EQF49dVXOeuss3juuedYv349Z5xxBhdddBEbN24M+2ur3IiIHCzgx9z+bwD+E5jBtOF9KDe5EwGN3Eh0qa2tJS8vjzvvvLPn2htvvIHL5WL16tWsWLGC7373u8ycOZOxY8dy5513MnbsWP75z3+GPZtOKBYROVjZ2xjtdTSaieyKP5Fh6fGf/j15JwLWyM0KLSiWUDBN8Nr0/6XYBDCMT31YdnY2Dz30EAsWLODss89m/PjxXHXVVSxevJgzzzzzE48PBAK0tLSQkZERjtS9qNyIiBwsOCW1OjCVE0dkY/Thhzx51sjNKKOSusYm/AETp6MP3ydyON52uLPAntf+XoW1lqwPzj//fK677jquvPJKZsyYQWJiIsuXLz/kY++66y5aW1v5whe+EMq0h6RpKRGRbqbZe73N8LS+fV9yPmZCJjFGgJGBMqqbO8OXUWSQueuuu/D5fDz55JM8+uijuN3uTzzmscce44c//CF/+ctfyMnJCXsmjdyIiHSr2QIHSunCxauBE/lyUR/W2wAYBkbuRNjzCsc59lLW0E5BWh+ms0QOJzbBGkGx67WPwq5du6ioqCAQCFBaWsqkSZN6ff3xxx/nq1/9Kk8++STz588PZdLDUrkREem2zTqI71X/RLqMOE4cdpjD+w4lbxLsecXaMXWgg9lhiihRwjD6PDVkJ4/Hw5e+9CUWLlzI+PHj+epXv8qmTZt6Rmf+/Oc/85WvfIXHH3+cCy64YMByqdyIiHTb8Txg7ZKakJdCovsofkQGt4Mf59jHG1pULFHi1ltvpampid/85jckJSXx3HPP8ZWvfIVnn32Wxx57jGuuuYZf//rXzJ49m6qqKgDi4+NJTT2Kvzj0g9bciIgABPxQvRmAdwLj+77eplt3uTH2UtbQFuJwIoPPmjVrWLFiBatWrSIlJQWHw8GqVatYu3Yt999/P7/73e/w+XzccMMN5Ofn93zceOONYc+mkRsREYADpeDrxIOLfWYui/tyvs3Bssbhd8SSEuigq7YUmBqGkCKDx7x58/B6vb2uFRcX09TUBMD1119vRyxAIzciIpbabQB8aBYQwHH0IzfOWLrSxgGQ3LglxOFE5Gio3IiIANRsBWB7oJDU+FhGZR39Yk4j35qayuv8EI8vENJ4ItJ3KjciItAzcrMzMIypw9P6dnjfx8QNmwxYJxVXNOoeUyJ2UbkREQGoscrNDnMYU/t6vs3HGD2LivdRpntMidhG5UZEJOCHuh2AVW6mHO16m27B2zAUOWqpqq4OUTiJFqZp2h3BdqH6PVC5ERFp2AP+LjpMF2VmNhPykvv3PPHpNMbmAuCp2BTCgBLJYmNjAWhv12ifx+MBwOl0HtPzaCu4iEittZj4Q7OApDgXOcmfvDdOXzWmTiCtrhpX7QehSicRzul0kpaWRk1NDQAJCQn9WvM11AUCAWpra0lISCAm5tjqicqNiMhB623G5iQd0x8svuwToO4V0lp2hCqdRIG8vDyAnoITrRwOB8OHDz/mcqdyIyISHLnZGRjG2Jx+TkkFuQonw1YY1vlhKJJJlDAMg/z8fHJycj5xMF40cblcOBzHvmJG5UZE5KCRm5Nzk47pqdJHTQNglFlGW3sHiQm6O7j0ndPpPOb1JqIFxSIS7fw+qN8JWOVmdM6xlZvkvDG0Eo/b8FJTqnU3InZQuRGR6NawG/we2k035WYWY4+x3OBwsNc5EoC2vSXHnk9EjprKjYhEt+71NmYh8a5YClKPfRqpNnEMAIFq3WNKxA4qNyIS3YLrbXaawxiTk4TDcexbcL0pwwFwNpcd83OJyNFTuRGR6FZjja7sCBQy5linpIKM9BEAJLTvD8nzicjRUbkRkehWe/AZN8e2DbybO2sUAOmeypA8n4gcHZUbEYlePg/UW+fR7AwMC9nITXK+teYmLdAInraQPKeI9J3KjYhEr4ZdEPDRZsZRTgh2SgXl5ubSZCYA4G8oDclzikjfDYpyc++991JcXExcXByzZ8/m7bff7tP3Pf744xiGwYIFC8IbUEQiU81HO6VcMU6KMhJC8rQ5yXGUmTkANFfqpGKRgWZ7uXniiSdYsmQJy5YtY8OGDUyePJlzzjnnU++vUVpayne+8x1OO+20AUoqIhGne71NYBijs5NwhmCnFIDTYVAbY90rqK1qV0ieU0T6zvZyc/fdd3PdddexaNEijj/+eFauXElCQgIPPfTQYb/H7/dz5ZVX8sMf/pBRo0YNYFoRiSjBkZvtwRtmhlKzuwAAb31pSJ9XRD6dreXG4/Gwfv165s+f33PN4XAwf/581q1bd9jv+9GPfkROTg7XXnvtp75GV1cXzc3NvT5ERICekZudYSg3HclFADga94b0eUXk09laburq6vD7/eTm5va6npubS1VV1SG/57XXXuP//u//ePDBB/v0GsuXLyc1NbXno6io6Jhzi0gE8HVBvTVltCOEO6W6manWWTdxbTrIT2Sg2T4tdTRaWlq46qqrePDBB8nKyurT9yxdupSmpqaej7Iy/aAREawt4KafFjOeKjIYe4x3A/+42Czr/lKpnRVgmiF9bhE5shg7XzwrKwun00l1dXWv69XV1eTl5X3i8bt27aK0tJSLLrqo51ogEAAgJiaG7du3M3r06F7f43a7cbvdYUgvIkPaQYf3xTgcjMhMDOnTJ+da6wHjzA5ob4DEzJA+v4gcnq0jNy6Xi+nTp7N69eqea4FAgNWrVzNnzpxPPH7ChAls2rSJkpKSno/PfvaznHHGGZSUlGjKSUT6rmE3AHvMfEZmJRLrDO2Pw5yMVKrMdOuTA6UhfW4ROTJbR24AlixZwjXXXMOMGTOYNWsWK1asoK2tjUWLFgFw9dVXU1hYyPLly4mLi2PixIm9vj8tLQ3gE9dFRI4oeLje3kBOyKekAArS4tlnZpNnHMDfsAfnsOkhfw0ROTTby83ChQupra3l9ttvp6qqiilTpvD888/3LDLet28fDseQWhokIkNBcDRln5nDmBDdU+pgWUlu1pk5zGQHbdW7SQn5K4jI4dhebgAWL17M4sWLD/m1NWvWHPF7H3744dAHEpHIFyw3ZWYOZ4Z4pxRYB/kdcOWDH7pqd4f8+UXk8DQkIiLRx9eF2VwOwD4zN+Rn3HRrSwyuA9RZNyIDSuVGRKJP4z4MTNpMNw1GCiOzQrtTqpsvxSo3rhYdQSEykFRuRCT6HLTeZkRmEnGxzrC8TEymddZNUmclBPxheQ0R+SSVGxGJPgettxmdHZ4pKYCk7CI8phOn6YPmirC9joj0pnIjItHnoJGbUdnhmZICyEtLpMLM6vWaIhJ+KjciEn0a9gCw18wN23obgPzUePaZOdYnWlQsMmBUbkQk+hw0LRXWcpMWx34zG4BA8NBAEQk/lRsRiS6miXnQtFQ4y01Woptywxq56azdE7bXEZHeVG5EJLq01WF42wiYBvWxeeQkh+/Gug6HQUtcIQCBBpUbkYGiciMi0eWAVTIqyWBYVhqGYYT15TzBs25imveF9XVE5CMqNyISXQZovU03I60YgLjOWvB2hP31RETlRkSiTfd6m8DAlJuUzFxazTjrk0adVCwyEFRuRCS6BMtNuLeBd8tPjacsuGNKZ92IDAyVGxGJLsGFvQM1LZWfFs9+nXUjMqBUbkQkqgSCC4rDvQ28W8HBB/lp5EZkQKjciEj08HbiaKkEoDmugLQEV9hfMi81rmdaKnBAIzciA0HlRkSiR6O1HbvFjCctK39AXjIz0UWlkQuAr15n3YgMBJUbEYkeB28Dz04ekJd0OAw6k4ZZ/6w1NyIDQuVGRKJHr/U2CQP2soHUEQDEeFugo3HAXlckWqnciEj06HVPqaQBe9nM9DQazODrNemsG5FwU7kRkegxQDfM/Li81HjKzSzrEx3kJxJ2KjciEjX89R9NSxUP4LRUQVoc5d0H+WnkRiTsVG5EJDqYJjSWAtCRWESCK2bAXjq/18iNbqApEm4qNyISHVprcPo68JsG7qziAX3p/NS4j8qNRm5Ewk7lRkSiQ3C9TSWZDM9JG9CXPrjcBLTmRiTsVG5EJDocfDfwzIFbTAyQkeii0WUd5Bc4oHIjEm4qNyISHQb4nlIHMwyDhOxiAGI6asHbOaCvLxJtVG5EJCqYB5eb7IEtNwB5eYW0m27rk6b9A/76ItFE5UZEooK3zio3+408itIHbht4t/F5yQctKtaOKZFwUrkRkejQUApAV9IwXDED/6NvXG6yDvITGSAqNyIS+bwduDqqAYjNGmVLhLEHlRvfAY3ciISTyo2IRL7gGpdWM47snHxbImQluWiItXZMtVTvsSWDSLRQuRGRyHdgLwBlZjYjswfuhpkHMwyDQGoRAL4GjdyIhJPKjYhEvuBtF/ab2QO+DfxgCcGTkWNbtVtKJJxUbkQk4gWCa1z2m9kUD/ABfgfLKLDW+yR11UDAb1sOkUinciMiEa+zdjcAFUYOBWnxtuUoHD4Kr+kkBj+0VNqWQyTSqdyISMTz15cC0Jk4DKfDsC3HuPw0qswMK0tdqW05RCKdyo2IRLzYFutcGSN9hK05MhJd1DiyAagu+9DWLCKRTOVGRCJbVwtx3kYA4nNG25sFaIsvAKC5arfNSUQil8qNiES2Rmsx8QEzifycbJvDgBncDu6t13ZwkXBRuRGRyBY842a/mcUIG3dKdXNnWVNjMS3aDi4SLio3IhLRzMbuA/xyGJE58DfM/Lj0AmtqLLlLu6VEwkXlRkQiWkeNtbZlv5nNMBvuBv5xBcPHApAbqKWlw2NzGpHIpHIjIhGtq9a6j1NrfIEtdwP/uOTckQAkGF3sLtPUlEg42P9fuohIGBlN1sJdf4q928B7xMbR6EgHoHrfDpvDiEQmlRsRiVymSXybNToSE7yv02DQFmfdmbyxUtvBRcJB5UZEIlfHAdz+NgBS8kbaHOYj/pRhAHjq99qcRCQyqdyISOQK7pSqNVMZlpNlc5iPuIOjSM5mrbkRCQeVGxGJXMED/MrM7EGxDbxbar51d/B0bzVNHV6b04hEHpUbEYlY3dvAy8wchmcMnnITFxy5KTRq2VndYm8YkQikciMiEautehcAB2LzSHDF2JzmIMFbMBQY9eyobrU5jEjkUbkRkYjla7DW3HQlFdmc5GNSrQXFmUYLu8urbQ4jEnlUbkQkYrmaywAw0gfJGTfd4tPwxiQBUFO+y+YwIpFH5UZEIpNpktRZAUB8zuDZBt4tEJyaaq/Zg88fsDmNSGRRuRGRyNRWi8vsImAapBeMsjvNJ7gyhgOQE6hlV22bzWlEIovKjYhEpgPWeptKMhiRnW5zmE8y0qxyU2jU8v7+RnvDiEQYlRsRiUidtR/dDXz4IDrjpkd6MQAjjBo2lTfZm0UkwqjciEhEaqq0FurWOHJJjY+1Oc0hZI0DYLRRzvv7VW5EQknlRkQikqfOGrlpSyi0OclhZI0FYJRRxfbKRrxaVCwSMio3IhKRjOCtF3ypw21OchhpwzGdbtyGlyx/NTt1mJ9IyAyKcnPvvfdSXFxMXFwcs2fP5u233z7sY//2t78xY8YM0tLSSExMZMqUKaxatWoA04rIUBDfZt2U0pVZbG+Qw3E4MYKjN2OMCjaVN9qbRySC2F5unnjiCZYsWcKyZcvYsGEDkydP5pxzzqGmpuaQj8/IyODWW29l3bp1vP/++yxatIhFixbxwgsvDHByERm0An5SPdbJv0m5o20OcwTBcjPaqNC6G5EQsr3c3H333Vx33XUsWrSI448/npUrV5KQkMBDDz10yMfPmzePiy++mOOOO47Ro0dz4403cuKJJ/Laa68NcHIRGbRaKonBh8d0kjts8B3g1yNrPGCVmw+0Y0okZGwtNx6Ph/Xr1zN//vyeaw6Hg/nz57Nu3bpP/X7TNFm9ejXbt2/n9NNPP+Rjurq6aG5u7vUhIpHNU7cHgAozixHZKTanOYLuaSlHOVsrW/D4tKhYJBRsLTd1dXX4/X5yc3N7Xc/NzaWqquqw39fU1ERSUhIul4sLLriAe+65h7POOuuQj12+fDmpqak9H0VFg+wGeiIScgfKPwSg0sgmM9Flc5ojCG4HH+OoxOMPsKO6xeZAIpHB9mmp/khOTqakpIR33nmHn/zkJyxZsoQ1a9Yc8rFLly6lqamp56OsrGxgw4rIgGurts64aXYXYBiGzWmOIHMMYJBOCxk06zA/kRCJsfPFs7KycDqdVFdX97peXV1NXl7eYb/P4XAwZswYAKZMmcLWrVtZvnw58+bN+8Rj3W43brc7pLlFZHDzNVjbwDuThtmc5FO4EiCtCBr39SwqvnyW3aFEhj5bR25cLhfTp09n9erVPdcCgQCrV69mzpw5fX6eQCBAV1dXOCKKyBAU02KN0DrSB+kZNwfrPqnYoe3gIqFi68gNwJIlS7jmmmuYMWMGs2bNYsWKFbS1tbFo0SIArr76agoLC1m+fDlgraGZMWMGo0ePpquri+eee45Vq1Zx//332/k2RGQQSeqoACAxd/DdDfwTssbDhy8xxijnr1UtdPn8uGOcdqcSGdJsLzcLFy6ktraW22+/naqqKqZMmcLzzz/fs8h43759OBwfDTC1tbXxzW9+k/379xMfH8+ECRP405/+xMKFC+16CyIymAT8ZPhrAcgsHGNzmD4I7piaEFOFt9Nke1ULJw5LszeTyBBnmKZp2h1iIDU3N5OamkpTUxMpKYN4i6iI9Etn/V7i7jkRr+mk6f/tJytlEN4R/GClr8PD51PjzGNW293csWAiXzpphN2pRAado/nze0julhIROZyafdY28Coji8zkeJvT9EG2dZBftr8aNx426aRikWOmciMiEeVAhVVuGmLzBvc28G4JmRCfjoHJKKOS97UdXOSYqdyISETprC21fk0osDdIXxlGr9sw7KxuodPrtzmUyNCmciMikaXJOuMmkDoEtoF3Cy4qnuSuwhewFhWLSP+p3IhIRIlvKwcgLqvY3iBHI3jWzYlxNQBsq9I98ESOhcqNiESUNE8lAKn5o21OchSCi4pHYRWzrZUauRE5Fio3IhIxmtq7yDXrAMgZPtbmNEchOC2V1VWGgwBbKjVyI3IsVG5EJGLs31eK2/Dhw0FS1hBac5M2ApxunIEuCow6tlU2E2VHkImElMqNiESM+u5t4M4scNp+AHvfOZzBO4TDeEcFzZ0+Kpo6bQ4lMnSp3IhIxGit2m39Gpdvc5J+CE5NzU6uB2CbpqZE+k3lRkQihr9hLwDe5CKbk/RDcMfURHc1AFtVbkT6TeVGRCJGTMt+AJwZQ/DeTMEdUyOx3sNWnXUj0m8qNyISEUzTJLmzAoDknJE2p+mH4LRUZqc1+qSRG5H+U7kRkYhQ29pFnlkLQHrhGJvT9EOmVW5cXQfIoJnSujY6PLoNg0h/qNyISEQorW1jmGGVG1dmsb1h+sOV0LPuZn7CTgIm7KjW1JRIf6jciEhEqKjYS5zhJYABKYV2x+mfMfMBOD9uM6CpKZH+6le5efnll0OdQ0TkmDRWWtvAW2KzIcZlc5p+GnMmANO86wGTbVpULNIv/So35557LqNHj+aOO+6grKws1JlERI5aV20pAJ2JBfYGORYjToWYeFK8tUwwynQbBpF+6le5KS8vZ/HixTz11FOMGjWKc845h7/85S94PJ5Q5xMR6RNH0z7rH9KG0G0XPi42DkaeBsA8R4luwyDST/0qN1lZWfzP//wPJSUlvPXWW4wbN45vfvObFBQU8O1vf5v33nsv1DlFRA7LHzBJaLfuBh6XPQS3gR9szFkAnOF8T7dhEOmnY15QPG3aNJYuXcrixYtpbW3loYceYvr06Zx22mls3rw5FBlFRI6oorGDfGoASM4dZXOaYzTWWlQ83bGDJNp1GwaRfuh3ufF6vTz11FOcf/75jBgxghdeeIHf/va3VFdX8+GHHzJixAguu+yyUGYVETmkPXVtFBp1ADjSh/C0FEDGKMgYTQx+TnF8oB1TIv3Qr9vmfutb3+LPf/4zpmly1VVX8fOf/5yJEyf2fD0xMZG77rqLgoIhvLBPRIaMPbWtTA+ecUPqEC83AGPPgrd2Mc/xHq9Vfs7uNCJDTr/KzZYtW7jnnnu45JJLcLvdh3xMVlaWtoyLyICorq4g0eiyPkkdZm+YUBhzFry1knnO93iwssnuNCJDTr+mpZYtW8Zll132iWLj8/l49dVXAYiJiWHu3LnHnlBE5FO01uwBoMOdbe04GuqKT8GMiSPfaMDVsF23YRA5Sv0qN2eccQYNDQ2fuN7U1MQZZ5xxzKFERI5GoMG62aQvOQJGbQBi4zGKTwXgdKNEt2EQOUr9KjemaWIYxieu19fXk5iYeMyhRET6qsvnJ769HIDYzBE2pwmh4JbweY73tKhY5Cgd1ZqbSy65BADDMPjyl7/ca1rK7/fz/vvvc/LJJ4c2oYjIEeytb6cAa6eUO6vY3jChNPYseP5mZji2s6a8CoiAhdIiA+Soyk1qaipgjdwkJycTHx/f8zWXy8VJJ53EddddF9qEIiJHsLu2tedu4MZQPp344zJH05pYRFJbGebuV4BZdicSGTKOqtz84Q9/AKC4uJjvfOc7moISEdvtqm3jM8EzbkiLoGkpwDH2LCh5iJGNb9DQ5iEjcYjeEFRkgPV7t5SKjYgMBruqWyjsPuMmrcjeMCGWcML5AJzp2MDrO2tsTiMydPR55GbatGmsXr2a9PR0pk6desgFxd02bNgQknAiIp+mqraaFKPD+iQ1ssoNI0+ny5FAbqCR0vdfgylftDuRyJDQ53Lzuc99rmcB8YIFC8KVR0Skz0zTxFe3BwzwxWcT40qwO1JoxbhpLjyd7LLnSdn7H0xz4RH/Yikilj6Xm2XLlh3yn0VE7FLb2kWmtxJc4MiIrPU23VKnLoCy5znJ+xa7atsYk5NkdySRQa9fa27KysrYv39/z+dvv/02N910E7/73e9CFkxE5NPsqmmjyLDWojjSi+0NEyauCefgx8F4x37ee09T/iJ90a9yc8UVV/TcN6qqqor58+fz9ttvc+utt/KjH/0opAFFRA5nd10rRd2LidMjc+SGhAyq0qYB4NvyL5vDiAwN/So3H3zwAbNmWWcu/OUvf2HSpEm88cYbPProozz88MOhzCcicljWyE13uSm2NUs4OY+7EIBRDa/g8QVsTiMy+PWr3Hi93p7FxS+99BKf/exnAZgwYQKVlZWhSycicgS7alt7pqUi7Yybg+XMvBiAaWzj/R27bE4jMvj1q9yccMIJrFy5krVr1/Liiy9y7rnnAlBRUUFmZmZIA4qIHM6e2uae04kjdloKcGQUU+4ejdMwqV3/D7vjiAx6/So3P/vZz3jggQeYN28el19+OZMnTwbgH//4R890lYhIOHV6/XgaK3EbPkzDCSkRckfww2gacTYAaWUv2pxEZPA7qtsvdJs3bx51dXU0NzeTnp7ec/1rX/saCQkRds6EiAxKpfVtDCM4JZVaCM5+/TgbMnJnXgI77mdy1wYONDaRnpZqdySRQatfIzcATqezV7EB655TOTk5xxxKROTTHLyY2IjgxcTdMsfMpMbIIsHo4sO3tGtK5Ej6VW6qq6u56qqrKCgoICYmBqfT2etDRCTcrMXE3feUitz1Nj0Mg71ZcwEwt6nciBxJv8Zxv/zlL7Nv3z6+//3vk5+fr+PARWTA7a5t5ZTunVIRvJj4YO6JF8HLf2X0gdcwA34Mh/4yKXIo/So3r732GmvXrmXKlCkhjiMi0je7atu43NG9DbzY1iwDZeysc2n5bzyZRiNlm9+gaNJpdkcSGZT6NS1VVFSEaZqhziIi0iemabK7tjUqtoEfLD4+np0J1u7U0pKXbU4jMnj1q9ysWLGCW265hdLS0hDHERH5dNXNXXg8XeTTYF2IggXF3WKKZgDgL3vX5iQig1e/pqUWLlxIe3s7o0ePJiEhgdjY2F5fb2hoCEk4EZFD2V3bSoFRh8MwITYBErPtjjRgiifPhR2/ZWTnVkrr2ijOSrQ7ksig069ys2LFihDHEBHpu947pYZDFG1qSBllHZQ6wlHD/63fwrXnzLQ5kcjg069yc80114Q6h4hIn+2qbWN4FNxT6pDi02hOLCalrZR9m9aCyo3IJ/T7EL9du3Zx2223cfnll1NTY/2Q+fe//83mzZtDFk5E5FB63TAzShYTH8xdbI3eZDRuorSuzeY0IoNPv8rNK6+8wqRJk3jrrbf429/+RmtrKwDvvfcey5YtC2lAEZGP213bFl0H+H2Me4RVbqYYH/KvTZU2pxEZfPpVbm655RbuuOMOXnzxRVwuV8/1z3zmM7z55pshCyci8nEdHj/ljR0M6xm5KbY1jy0KpwMw2bGL596vsDmMyODTr3KzadMmLr744k9cz8nJoa6u7phDiYgczu46a6R4hCO6zrjpJXciptNNmtFGW9UOTU2JfEy/yk1aWhqVlZ8cCt24cSOFhYXHHEpE5HB217aRSAfptFgXonBaihgXRr51mN8UY5empkQ+pl/l5otf/CI333wzVVVVGIZBIBDg9ddf5zvf+Q5XX311qDOKiPTotQ08Ph3iUuwNZJeDp6ZUbkR66Ve5ufPOO5kwYQJFRUW0trZy/PHHc9ppp3HyySdz2223hTqjiEiPXbVtH+2UisZRm27DrJOKpzo+ZHNFs6amRA7Sr3Ljcrl48MEH2b17N88++yx/+tOf2L59O6tWrcLp1F1qRSR8dh88chONi4m7BUduTnDsw4VXU1MiB+nzIX5Lliw54tcP3iV199139z+RiMhhBAImu2vb+HwUn3HTI70YEjKJba/nOGMvL2zO4oYzxtidSmRQ6HO52bhxY6/PN2zYgM/nY/z48QDs2LEDp9PJ9OnTQ5tQRCSosrmTDq+f4a7oPeOmh2FYozc7/8MUxy5WlY+hqd1LakLsp3+vSITrc7l5+eWXe/757rvvJjk5mUceeYT09HQADhw4wKJFizjttNNCn1JEBNhVY20DHxVTBwGie+QGoHAG7PwPp8bv5ZFWeGtPPWefkGd3KhHb9WvNzS9/+UuWL1/eU2wA0tPTueOOO/jlL3951M937733UlxcTFxcHLNnz+btt98+7GMffPBBTjvtNNLT00lPT2f+/PlHfLyIRI5dta2ASYHZvaC42M449guuu5nq/BCAN3bV25lGZNDoV7lpbm6mtrb2E9dra2tpaWk5qud64oknWLJkCcuWLWPDhg1MnjyZc845p+d+VR+3Zs0aLr/8cl5++WXWrVtHUVERZ599NuXl5f15KyIyhOyqbSWTZtxmJ2BAWpHdkexVOA2ArK79pNLKG7t0iKoI9LPcXHzxxSxatIi//e1v7N+/n/379/PXv/6Va6+9lksuueSonuvuu+/muuuuY9GiRRx//PGsXLmShIQEHnrooUM+/tFHH+Wb3/wmU6ZMYcKECfz+978nEAiwevXqQz6+q6uL5ubmXh8iMjTtqjnonlIpBRDjtjeQ3RIyIGM0YJ13s6O6ldqWLptDidivX+Vm5cqVnHfeeVxxxRWMGDGCESNGcMUVV3Duuedy33339fl5PB4P69evZ/78+R8FcjiYP38+69at69NztLe34/V6ycjIOOTXly9fTmpqas9HUVGU/01PZAjrdTfwaF5MfLDg1NRZKfsBWLdbU1Mi/So3CQkJ3HfffdTX17Nx40Y2btxIQ0MD9913H4mJiX1+nrq6Ovx+P7m5ub2u5+bmUlVV1afnuPnmmykoKOhVkA62dOlSmpqaej7Kysr6nE9EBo+WTi81LV0flZtoX0zcLXiY30nuPQCs09SUSN93Sx1KYmIiJ554YqiyHLWf/vSnPP7446xZs4a4uLhDPsbtduN2R/nQtUgE2F1rncA7zlUPJhq56RYcuRnRuQ0wtahYhH6O3IRKVlYWTqeT6urqXterq6vJyzvydsa77rqLn/70p/znP/+xtWCJyMCwdkrBhNjgqG7WWBvTDCK5E8ERg6urgWGOBvbWt7P/QLvdqURsZWu5cblcTJ8+vddi4O7FwXPmzDns9/385z/nxz/+Mc8//zwzZswYiKgiYrPuclPkD+6MVLmxxMZBzvEAXJRl3YJhnUZvJMrZWm7Auq3Dgw8+yCOPPMLWrVu5/vrraWtrY9GiRQBcffXVLF26tOfxP/vZz/j+97/PQw89RHFxMVVVVVRVVdHa2mrXWxCRAbCrpo00Wkj0N1oXMnWrgR4FUwGYmxRcVKxyI1HumNbchMLChQupra3l9ttvp6qqiilTpvD888/3LDLet28fDsdHHez+++/H4/Hw+c9/vtfzLFu2jB/84AcDGV1EBtCu2lZGGcGbQ6YMA1ffNy9EvMJpsOERJgR2AdZhfqZpYhiGzcFE7GF7uQFYvHgxixcvPuTX1qxZ0+vz0tLS8AcSkUHF5w9QWt/GAkeFdUFTUr0FR25SGz/A5TSoau5kT10bo7KTbA4mYg/bp6VERD5N2YEOvH6T8c7gyE3WOHsDDTY5x4PTjdHZxHmFHQC8rqkpiWIqNyIy6HXfMPMEd/CMG43c9OaMhbxJAJyX3r2oWOfdSPRSuRGRQa97p9QoNC11WMGpqakxuwFrUXEgYNqZSMQ2KjciMujtrm0jFh/Z3u5yo2mpTwiWm+yWrSS4nBxo97Kt6uhuZCwSKVRuRGTQ21XbynCjGgd+cCVBcr7dkQaf4B3CHVXvM3NEKgDvlDbYmUjENio3IjLo7aptZbQRHLXJHAPa4vxJWeMgNgE8rZyR2QjApvImezOJ2ETlRkQGtYY2DwfavYw2tFPqiBxOyJ8MwAzXXgA27Ve5keikciMig1r3YuJJ7uA96FRuDq/Ampoa5dkBwM6aFto9PjsTidhC5UZEBrXubeDjYrpvmKnbLhxWcFFxQt375CS7CZiwtbLZ5lAiA0/lRkQGNWvkxqTQb903SSM3RxBcVEzVJqYWWreneF9TUxKFVG5EZFDbVdtGFs3E+1sAAzJG2x1p8EofCe5U8HVyepp1iJ/W3Ug0UrkRkUGt106ptOEQG2dvoMHM4YACa1HxtNhSAN7XjimJQio3IjJodfn8lDW0M8qhw/v6LLiouLjLWlS8q7aV1i4tKpboonIjIoPWnro2AiYc17OYWOXmUwUXFcfXvkd+ahymCZs1eiNRRuVGRAatLRXWTp+JumFm3wXLDdVbmJofD+gwP4k+KjciMmhtDpabYsqtCyo3ny5tOCRkQsDLvHSrFKrcSLRRuRGRQWtLRTNuPKR7dDpxnxlGz7qbaWwHtGNKoo/KjYgMSqZpsqWymWKjCgMT4lIhMdvuWEND8akADG/eAMDuujaaO712JhIZUCo3IjIoVTR10tThZZwjOGqTOVY3zOyrkacB4Nq/jqJUFwAfaGpKoojKjYgMSt07fGYk11sXNCXVd3mTrcP8upo4P7sW0NSURBeVGxEZlLZUdu+U6r5hphYT95kzBopPAWCuayugRcUSXVRuRGRQ6t4GPjzQvVNKIzdHpdiamprQsRFQuZHoonIjIoOStQ3cJKNjr3VBIzdHZ+TpAKTXrScWH3vr22lq16JiiQ4qNyIy6DS1eylv7CCPBpy+NjCc1k0hpe9yjoeETAxvO2elWqNfGr2RaKFyIyKDTvd6m1NTguttMsdAjMvGREOQw9EzNXVeknWfqffLG20MJDJwVG5EZNDpLjenJAZvmJl/oo1phrDglvBp/k2AdkxJ9FC5EZFBZ3OF9YfwCc591oW8STamGcJGzgUgv/l93Hh4X+VGooTKjYgMOt07pYZ17rQuqNz0T+YYSM7HEfAwzbGT8sYOGts9dqcSCTuVGxEZVLp8fj6saSWRDhJagzul8jQt1S+G8dG6mwRr3U33zUhFIpnKjYgMKjurW/EFTGbEBdfbJBdAYpa9oYay4JbwU2I2Ax9N+YlEMpUbERlUuqekzkgN3lNKU1LHJlhuiru2k0gHH5Rr5EYin8qNiAwq3TulJseWWRe0U+rYpI+AtOE4TT8zHds1ciNRQeVGRAaV7pGbYu9u64JGbo5dcPRmjmMzu+vaaOvy2RxIJLxUbkRk0AgETLZUNhODj9QW7ZQKmeCW8LkxWzBN2FalqSmJbCo3IjJolB1op7XLx4SYKhwBD7hTIK3Y7lhDX7DcTGAPORzQuhuJeCo3IjJodE9JfSYteNuF3InWbQTk2CTnQuEMAOY7N2jdjUQ8/dQQkUGj+wyWme7gYmJNSYXOhPMBONvxrs66kYinciMig0b3iMKYwB7rgnZKhc74CwBrUXF5dQ0eX8DmQCLho3IjIoOCaZqUlDUCJtlt1mm6GrkJoezxmBmjcRs+TjZL2FHdYncikbBRuRGRQWFvfTsH2r2McB4gxtMEjhjInmB3rMhhGBjdU1POd7XuRiKayo2IDArWqA2cmxVcTJx9HMS47QsUiSZcCMBnHCVsLW+wOYxI+KjciMigsHHfAQDmJATvKaUpqdAbNpMuVwYpRjuUvmZ3GpGwUbkRkUGhe+RmPKXWBZWb0HM46Rx9DgBjGl7FHzBtDiQSHio3ImK7Tq+/555S2a3brYvaKRUWSSd+FoAzjHfZU9tqcxqR8FC5ERHbba5oxus3GZnoIaZlv3Uxd6K9oSKUc8wZdOKm0Khn/9Y37Y4jEhYqNyJiu+71Nudn11sX0oZDfJp9gSJZbDy7U2cD4Nzxb5vDiISHyo2I2K57vc2cxHLrQp6mpMKppdhad1NU+7LNSUTCQ+VGRGy3cV8jAOPM4MnEKjdhlXLihfhNg2LvbsyGPXbHEQk5lRsRsVVNSyfljR0YBmQ2brIuFkyxNVOkGz1iOO+a1gGJjRufsTmNSOip3IiIrUqCozYzsgI4Gz60Lg6baV+gKOCKcbAx6TTrky0qNxJ5VG5ExFbd623OTw/uksoaBwkZ9gWKEg3DrXU36fUboLnC5jQioaVyIyK26l5vMztmp3WhaLZ9YaLIhHETeDcwzvpk6z/tDSMSYio3ImIbf8Dk/f2NABR3fGBdVLkZEHNGZ/Kc3/q99m36m81pREJL5UZEbLOzpoU2j59Ul0l87XvWRZWbAZGfGs+mlNMBcO5/C1qqbE4kEjoqNyJim+7FxBfl1mP4OiE+HTLH2BsqioweM4ENgTEYmJqakoiiciMituleb3NGQvCslWGzwKEfSwNlzuhM/hWcmmLz07ZmEQkl/RQREdtsLLNuu3BCYJt1oWiWjWmiz5xRmTzvt37Pzb2vQ0u1zYlEQkPlRkRs0dLpZWdNK2CSfaDEuqj1NgMqJyUOd3YxJYHR1tTUNk1NSWRQuRERW7y/vwnThGmpbThbK8FwQuF0u2NFnTmjNDUlkUflRkRs8W6pNSX12Ywy60L+ieBKsDFRdJozOpN/B4LTgXtfh9ZaewOJhIDt5ebee++luLiYuLg4Zs+ezdtvv33Yx27evJlLL72U4uJiDMNgxYoVAxdURELq3b0NAMyODd5yQVNStjhpVCb7zRzeC4wCM6CpKYkItpabJ554giVLlrBs2TI2bNjA5MmTOeecc6ipqTnk49vb2xk1ahQ//elPycvLG+C0IhIq/oDZsw28uL378D4tJrZDVpKbcblJPQf6aWpKIoGt5ebuu+/muuuuY9GiRRx//PGsXLmShIQEHnrooUM+fubMmfziF7/gi1/8Im63e4DTikio7KhuoaXLR7bLS1z9FuuiRm5sM2dUJs91T02VroW2OnsDiRwj28qNx+Nh/fr1zJ8//6MwDgfz589n3bp1IXudrq4umpube32IiL3e3Wutt1mQU41h+iGlEFKH2Zwqes0ZnUmZmcsOx+jg1NSzdkcSOSa2lZu6ujr8fj+5ubm9rufm5lJVFbpjwJcvX05qamrPR1FRUcieW0T6Z0Ow3MzrPrxPoza2mj0yE8OAv3fNtC5oakqGONsXFIfb0qVLaWpq6vkoKyuzO5JI1OteTHycb6t1QeXGVumJLibkpXw0NbXnVWhvsDeUyDGwrdxkZWXhdDqpru59ImZ1dXVIFwu73W5SUlJ6fYiIfWqaOylr6MBpBEhvKLEuajGx7eaMymSvmUdF3Fgw/ZqakiHNtnLjcrmYPn06q1ev7rkWCARYvXo1c+bMsSuWiIRZ93qb+dnNGJ2NEBMPeZPsDSXMGZ0J8NGZN5qakiHM1mmpJUuW8OCDD/LII4+wdetWrr/+etra2li0aBEAV199NUuXLu15vMfjoaSkhJKSEjweD+Xl5ZSUlPDhhx/a9RZE5CitD5abi5J3WheGzQBnrI2JBGDWyAycDoM/tUyzLux5RVNTMmTF2PniCxcupLa2lttvv52qqiqmTJnC888/37PIeN++fTgOukNwRUUFU6dO7fn8rrvu4q677mLu3LmsWbNmoOOLSD90j9xM95dYF0Z/xr4w0iM1PpaTR2eydqdJXeJYstp2wvbnYOqX7I4mctRsLTcAixcvZvHixYf82scLS3FxMaZpDkAqEQmHDo+fzeVNxOAjt/4d6+LoM+wNJT0umJTP2p11/Ns/i6vYCVueUbmRISnid0uJyODx3v5GfAGTzyTtw+FthfgMyJtsdywJOvuEPJwOg4ebplgXdr0MHY12RhLpF5UbERkw3ettFqRsty6MPgMc+jE0WGQkujh5dCa7zELqE0ZDwGtNTYkMMfqpIiIDprvczPCVWBdGaUpqsLnwxHwAngsEzx7a8oyNaUT6R+VGRAZEIGCyfu8BUmglu2WzdVHrbQads4+3pqb+2DM19V/obLI1k8jRUrkRkQGxq7aVpg4v82K3YpgByBqn+0kNQumJLk4Zk8VOcxgNCSPB74Ht/7Y7lshRUbkRkQHx0fk23etttAV8sLpwUnBqyh+cmtKBfjLEqNyIyICwzrcxmenfaF1QuRm0zj4hlxiHwSPNwQP9dq3WrikZUlRuRCTsTNPkzd31jDCqSeuqBEcsjDjF7lhyGGkJH01N1SeM0tSUDDkqNyISdjuqW9l/oIN5MR9YF4pmgzvJ3lByRBf07Jo6ybqw+e82phE5Oio3IhJ2L22tBuCzSQedbyOD2tnHB6emDt411XHA1kwifaVyIyJh99LWapz4meR9z7qgcjPopSW4OHVsFh+aw6jrPtBv27/sjiXSJyo3IhJWda1dlJQ1MtnYhcvXCvHpkD/F7ljSBxedWADAM95Z1gVNTckQoXIjImH13201mCZ8Pi04JTVyLjic9oaSPjlvUh6JLiePtk63LuxeA+0NtmYS6QuVGxEJq5e2WOttznBusi5oC/iQkeCK4fxJ+ew2C6iIGwMBH2x71u5YIp9K5UZEwqbT62ftzjqGGTXkt34AGDD2bLtjyVG4dLp1ivSTHTOsC5qakiFA5UZEwmbd7no6vH6uSHjHujDyNEjJtzeUHJVZxRkUZcTzd89M68LuV6Ct3t5QIp9C5UZEwmZ1cAv4xbFvWhcmXWZjGukPh8Pg0mnDKDXzKY0dA6Yftv7D7lgiR6RyIyJhYZomq7fWMM4oI79zl3Uq8XEX2R1L+uHSadbU1BOampIhQuVGRMJic0UzlU2dXBK7zrow9mxrG7gMOUUZCcwemcG/um+kWboWmivtDSVyBCo3IhIWq7fWACaXurqnpC61NY8cm89PH8Y+M5dNjuPADMA7D9odSeSwVG5EJCxWb6tmmrGTbF8VxCbCuPPsjiTH4PxJ+SS4nPy28xzrwrsPgafN3lAih6FyIyIhV93cyfv7m7jIGZySOu5CcCXYG0qOSaI7hvMm5vNiYAb1rgLrPlPv/dnuWCKHpHIjIiH37PuVOPFzcexb1oWJn7c3kITEpdMLCeDgd13B0Zt190EgYG8okUNQuRGRkPIHTB55o5Q5ji2kmY0Qn6EbZUaIk0ZmUpQRz6qu0/DEJEPDLtjxvN2xRD5B5UZEQmr11mr2NbRzmSs4JXXCxeCMtTeUhITDYXDNnGLaiePvjrOsi+vutTeUyCGo3IhISD30+h7ceDjXGTyVeJKmpCLJF2YWkehy8qvmMwgYMbD3NajYaHcskV5UbkQkZDZXNPHm7gbOi3kXt78NUoZB0Ul2x5IQSomL5Qszi6gik7cS5loXNXojg4zKjYiEzB9eL8VBgFsSgsfzT78GHPoxE2m+fHIxhgF3NATv8L7579C0395QIgfRTx0RCYnali7+UVLB5xyvk+fZZ51GPPsbdseSMBiRmcj843LZbI5kd9JUCPjgjd/aHUukh8qNiITEo2/tJeD3cHNc8L5Dp9wEcSm2ZpLw+copIwG4s+lc68LbD8C+t2xMJPIRlRsROWZdPj9/enMvn3e+Sl6gChKzYdZ1dseSMDppVAbH5afwkncS2/MutG7J8PevQ1er3dFEVG5E5Nj9871KmlvbuCn2aevCaf8PXIm2ZpLwMgyDr5xSDMANDQsxUwrhwB548fv2BhNB5UZEjlEgYPJ/r+3hi87/kkcdJBfA9EV2x5IB8NkpBWQlufiw2cmbJ95hXXz3Idj5or3BJOqp3IjIMfnn+xXsqazlWzHPWBdO/w7ExtkbSgaEO8bJl04aAcCtJRn4Zn7d+sIzi6G9wcZkEu1UbkSk37p8fn7xwnaucr5IttEIacNh6lV2x5IBtOiUkeQku9ld18Y9xhWQNQ5aq+C579gdTaKYyo2I9Nuf3tyH0VjKt7vX2sy9BWJctmaSgZUaH8uPF0wE4LevVbDr1F+C4YQP/golumu42EPlRkT6panDywOrN3N/7K9Jph2GzYQTF9odS2xwzgl5XDApH3/A5NuvGvhPv9n6wr+WQO0Oe8NJVFK5EZF+WfnKLm7y/p6JjlLMhEy47GFwxtgdS2zyg8+eQGp8LJsrmnmQi2Hk6eBthye/DN4Ou+NJlFG5EZGjVtHYQcPrD3NFzMuYGBiXPAipw+yOJTbKTnbz/QuPB+BXq3exd+4K67yjms3w/FJ7w0nUUbkRkaP253/+mx84/s/6ZO7NMOZMewPJoHDptEJOG5tFly/A/75QQ2DB7wAD1v/BWoMjMkBUbkTkqGwt3c8lO5cSb3hoLjwdY+7NdkeSQcIwDO68eBIJLidv72lg6XtZmKf+P+uL/7gR6nfZG1CihsqNiPRZzYFG2v54OSMdVRyIySbliod112/ppSgjgZ9deiIOA554t4xbGy/ALJoDnhb4yzW6PYMMCP1UEpE+ae3oZNd9C5kReJ924jC++CgkZtodSwahiyYX8KuFU3AY8Ni7lfw08TuYidlQvQme/gYEAnZHlAinciMin8rr87Hxt19ijvdNuoilZcEfSRsz2+5YMoh9bkohd3/BKjgPlHSxMu9HmE4XbP0nrFludzyJcCo3InJEZiDAm/d9ndPaXsRnOqiYfx+5U86xO5YMAQumFvLLL0zGYcDPNqfydGHw1OJXf64FxhJWKjciclimabLuof/ltIanANg+ezkjT/2CzalkKLl46jB++YXJGAb8z46JbBpxtfWFp78J5RvsDScRS+VGRA7J5/Oz9v4bOHn/7wF497hbOOH8b9icSoaii6cO43vnHQfAgh1nU5c/F3yd8PgVOsFYwkLlRkQ+obm9k1d+dRWn1zwKwFtj/4cZC3UQm/TfV08byeWzivCbDs4r/zKd6eOgpRJ+Nw/ee9zueBJhVG5EpJey2kY2/OrznNn2LwKmwebpdzD7yh/YHUuGOMMw+NHnJnLKmExqPW4ubbsZT9Ep4G2Dv38dnr4BPG12x5QIoXIjIj3e31PJ3vsuZp53LV5i2D//Xk646Ft2x5IIEet0cN+V0xmdncjm5ngua7sZz2k3AwaU/Ake/AxUb7Y7pkQAlRsRAeCDjeuIe/gsTjU30Imblov/yPDTrrQ7lkSY1PhYHvryTDISXbxX0crX9s3Hd9UzkJQLtdtg5anwzGJo2m93VBnCVG5Eop1psudfv2Ls0xcxziij0ZFO4Kq/kzH5AruTSYQakZnIg1fPIC7WwZrttfzvu6kEvrYWjrsIzABsXAW/mQYv3Apt9XbHlSFI5UYkmrXVUf/7ixn5zg9wG15K4mbh/tabJIw+xe5kEuGmj0jn/iun43QY/H1jOXe+Wo/5hVVw7Usw4lTwd8G638KvJ8Pau8HbaXdkGUJUbkSiUSAAGx/F85tZZJa/TJcZy5/Sb2DCkn8Tn55ndzqJEmdMyOEXnz8RgN+/tocHXt0NRTPhy8/Cl/4KeSda96Ra/UP47UzY9BSYps2pZShQuRGJNvvXY/5+PjzzTVxd9WwPDOOnw+7lCzfcQZwrxu50EmUumTaM2y6wzsD56b+3sWpdKSbAmPnwtVfg4gcguQCa9sFfr4X/Owv2vWlrZhn8DNOMrhrc3NxMamoqTU1NpKSk2B1HZOA07YeX74QS6+yaVjOOX/suoXHSV7jzsunEOvV3HbHPT/+9jZWv7ALg9HHZ/GTBRIoyEqwvetqtKarXfgXeduva8JPhlBth7Nm6M32UOJo/v1VuRCKZacLe1+GtB2Dbv8D0A/CU/3R+ZV7ODRedyuWzijAMw+agEu1M0+S+Nbv49Us78fgDxMU6+J/547j21JHEdBfv5kpYcyeU/BkCXuta9gQ4+Vtw/AJwJ9mWX8JP5eYIVG4k4nnaoWYrlK+H9Q9DzUfnhrwZOI6feb9IU+YUfnvFNI4v0H8DMrjsrm3le3/fxJu7GwA4Lj+FG84YzdnH5+GK6S45FfDm/fDuH6w1OQBOFwyfA2PPgjFnQfZ4UGmPKCo3R6ByIxGjswnqdwU/PrTOCKn+wPqcj/6z9hhx/NV3Cg/7zmK7OZwFUwq44+JJJLm1vkYGJ9M0eWr9fn7y3FYa260RmoxEF5+fPowvzixiVHZwhKazySrw7z4EB0p7P0lKIYw42So8I06GrPGavhriVG6OQOVGhqym/bB7jfVR+pp1X57D6HRlsttZzN9aJvAX31yaSWLWyAy+OW80c8dlaxpKhoT61i4efqOUv7xbRnVzV8/1yUVpnDYmi1PGZDFtRBpup8Mq+DtfhA9fhNLXra3kB4vPgJzjIWMkZIyyPlIKwHCCARgO68Ppsj5i4qwPVwLExg/sG5dDGnLl5t577+UXv/gFVVVVTJ48mXvuuYdZs2Yd9vFPPvkk3//+9yktLWXs2LH87Gc/4/zzz+/Ta6ncSMiYJnQ1Q8cB6GgETytggMNp/cB0OCA2EeLTrY8YV9+f29dlHUNfWQIVG2HvG9YP749LysWXNoq6uCJ2+PJY3ZDNc7VZ1JqpPQ+Zf1wO188bzfQRGcf6jkVs4fMHWLO9lj+/vY+Xt9cQOOhPrbhYBzOLMxiZlUh+ajz5qXEUJJoUd2wmq349jrJ1UPYO+Dr6H8CVDEk5kJxn/ZpSCGnDe3+4k4/9jcoRDaly88QTT3D11VezcuVKZs+ezYoVK3jyySfZvn07OTk5n3j8G2+8wemnn87y5cu58MILeeyxx/jZz37Ghg0bmDhx4qe+nsqN9FlnszXUfaAUGvdZIydNZdavzeXQXm+dptpHXUY8bY4kOhwJdBgJdDgS6DQSwOEkweEl3vARZ3iID7SR2roHh+nt9f2m4aAxfRL70mazNW4ar7UVsLHaT3njJ39oTyxM4YzxOVx4YgHj8/RDVyJHdXMnr+yo5fUP63j9w3rqWrsO+9hYp8Gw9ARGpscyK76Csc4qCgKVZHnKSW4vw9VZB5gYZgAwwQxg+D3g84Cv86NFy30RlwopwyC1EFKHQWIOxKWAO+WjX93J4EqyFj67kqzPHc5j/j2JFkOq3MyePZuZM2fy29/+FoBAIEBRURHf+ta3uOWWWz7x+IULF9LW1sazzz7bc+2kk05iypQprFy58lNfL1zlpqm5lZJt24lxGDgcBk7DwOmwPmKcBjGGA6fTINZp4MDAMAwMAxzB2YGQzRKYJj3rLczgf7Sm3/qPNuAD04/h94K/C8PfheH3WB/eDgzfQR+edhyeFgxvKw5PK4anNfi1zp4PADM2HjM2ATMmATM2gUBcGv74DAJxGQTiMwm4U4Nfj//oV4cz+IYN61fThIAPI+C1Mvq9GN42HJ4WHF3NGJ5WHJ5mjK4WHJ5mK1dXi/VDCPOg92xYrxGbQCA2ETM2kYArycrmSiTQndM0MQN+zIAPAn5MTxtGex2O9jqcHXXEdNThbi3H5TnQp99yj+GimSRazDgCARMnARwEcBgmiXSSShsO4+j/MztgJrEpMJL3zVG8FxjNm4HjaSHhkI8dkZnAxMJU5o7LZt64bHJS4o769USGGtM02VHdyrt7Gyg/0EFVUyeVTZ1UNnVQ0diJx9/3v3x0czkduGKsjzgnZMR6GeFuYbirlYKYZvIcTWQHakj3VpHaWUliRwWxnsZ+vwdfbBI+Vyre2GS8san4XUn4YlMIuJJ7PgxXPMQm4HAl4IiNx4iJBQwMhwPDcGI4DOtnvBmw/sJlBjAMA7N7qs1wgsOJaTjBGYvpiMF0xAZHmR2A46DHfuwPI9P86LkJPn/AjxHwHvSrz/p6wG/9GWMGcCakkT7h9H7/vhzK0fz5beuKQo/Hw/r161m6dGnPNYfDwfz581m3bt0hv2fdunUsWbKk17VzzjmHp59++pCP7+rqoqvro2bf3Nx87MEPoXLHW8x97pKwPLfYp85MoczMYb+ZRbmZTbmZSYWZSaWZSa2ZSjOJdNF7uiku1kFBajz5aXFkJrpJi3OS5eok29lOuqODOH8bMf42Yn2tuHxt+P0+WvwxNHudNHudNHhj+NAsZLc3k5YuPy2dXkwgJ9HFcYluMpNcZCS6GJOTxPH5KRxXkEJKXKw9v0EiNjIMg/F5yYccnfQHTKqaO9lb38a++nb2NrRT1dRJVVMn1c2dVDV30u7xf+L7PP6AVYqCf2xUAB+QBCQBhz69O5EO8o16Co168o16Cow6Mmgh2eggmXaSjXaS6SDJ6CCRThLpwGVYrx3jbSXG20qk/XVkW+xxpN9q32GLtpaburo6/H4/ubm5va7n5uaybdu2Q35PVVXVIR9fVVV1yMcvX76cH/7wh6EJfARxsTGf+EPu48yD/pcQjZeZGIf83Hp6AxMDnzWOgC84puAlBg8xeIili1g8xNJpuujATRcuOnHRRhytxNNmxlu/Ek9n8GtdZmzPe42ni3iji3i6SAiOUqQbzWTQQjotpBitxOMhji7i8RBPF06sIWAHJkYwqZcYfDjxEoMXJ+3B1281E6xfSaAl+HkL1rUuXJg9z2StCYynkwQ6SaCLRDpINLo/7yQxmBPAb3SPrzjpMtw0OVJpcqTT7Eyj1ZlGfUwOdTH5eGMSrRE4w/job3MxDiY4HcxOiCU72U1OchzZyW6yk9wUpMWRGh+rBbsiNnM6DArT4ilMi+fk0Z/8ummaeP0m/oCJLxDAH7A+9/oDeHwBunzWr20eH43tXg60e2hosz6aO7w0d3pp7vDR3OmltSuBdl86WwKjed8fwOc3e37E9/wkMCDW6cDpMIh1GMQ5fKQaHaQZbaQ6OkillRTaSAi0EWe2ER/o/mgn1vTgNjtxmV24zS4cBKyf7qaJQQAHJv6eqx/9PDQI4AxejcGPkwAx+Igh0PN59/dbH4ce6Qp89IoEMPDjxI8DL87gP1ufd38EcFAbM5wJof/X2mcRvxd06dKlvUZ6mpubKSoqCvnrFE+eC5NrQ/68IiISeoZh4Irprh5a9xJpbC03WVlZOJ1Oqqure12vrq4mL+/Qw395eXlH9Xi3243b7Q5NYBERERn0bD3RyOVyMX36dFavXt1zLRAIsHr1aubMmXPI75kzZ06vxwO8+OKLh328iIiIRBfbp6WWLFnCNddcw4wZM5g1axYrVqygra2NRYsWAXD11VdTWFjI8uXLAbjxxhuZO3cuv/zlL7ngggt4/PHHeffdd/nd735n59sQERGRQcL2crNw4UJqa2u5/fbbqaqqYsqUKTz//PM9i4b37duH46Ajs08++WQee+wxbrvtNr73ve8xduxYnn766T6dcSMiIiKRz/ZzbgaaDvETEREZeo7mz2/dRUxEREQiisqNiIiIRBSVGxEREYkoKjciIiISUVRuREREJKKo3IiIiEhEUbkRERGRiKJyIyIiIhFF5UZEREQiiu23Xxho3QcyNzc325xERERE+qr7z+2+3Fgh6spNS0sLAEVFRTYnERERkaPV0tJCamrqER8TdfeWCgQCVFRUkJycjGEYIX/+5uZmioqKKCsri7p7V0Xre4/W9w1673rveu/RxO73bpomLS0tFBQU9Lqh9qFE3ciNw+Fg2LBhYX+dlJSUqPs/frdofe/R+r5B713vPfrovdvz3j9txKabFhSLiIhIRFG5ERERkYiichNibrebZcuW4Xa77Y4y4KL1vUfr+wa9d713vfdoMpTee9QtKBYREZHIppEbERERiSgqNyIiIhJRVG5EREQkoqjciIiISERRuQmjHTt28LnPfY6srCxSUlI49dRTefnll+2ONWD+9a9/MXv2bOLj40lPT2fBggV2RxpQXV1dTJkyBcMwKCkpsTtO2JWWlnLttdcycuRI4uPjGT16NMuWLcPj8dgdLSzuvfdeiouLiYuLY/bs2bz99tt2Rwq75cuXM3PmTJKTk8nJyWHBggVs377d7lgD7qc//SmGYXDTTTfZHWVAlJeX86UvfYnMzEzi4+OZNGkS7777rt2xjkjlJowuvPBCfD4f//3vf1m/fj2TJ0/mwgsvpKqqyu5oYffXv/6Vq666ikWLFvHee+/x+uuvc8UVV9gda0B997vfpaCgwO4YA2bbtm0EAgEeeOABNm/ezK9+9StWrlzJ9773PbujhdwTTzzBkiVLWLZsGRs2bGDy5Mmcc8451NTU2B0trF555RVuuOEG3nzzTV588UW8Xi9nn302bW1tdkcbMO+88w4PPPAAJ554ot1RBsSBAwc45ZRTiI2N5d///jdbtmzhl7/8Jenp6XZHOzJTwqK2ttYEzFdffbXnWnNzswmYL774oo3Jws/r9ZqFhYXm73//e7uj2Oa5554zJ0yYYG7evNkEzI0bN9odyRY///nPzZEjR9odI+RmzZpl3nDDDT2f+/1+s6CgwFy+fLmNqQZeTU2NCZivvPKK3VEGREtLizl27FjzxRdfNOfOnWveeOONdkcKu5tvvtk89dRT7Y5x1DRyEyaZmZmMHz+eP/7xj7S1teHz+XjggQfIyclh+vTpdscLqw0bNlBeXo7D4WDq1Knk5+dz3nnn8cEHH9gdbUBUV1dz3XXXsWrVKhISEuyOY6umpiYyMjLsjhFSHo+H9evXM3/+/J5rDoeD+fPns27dOhuTDbympiaAiPt3fDg33HADF1xwQa9/95HuH//4BzNmzOCyyy4jJyeHqVOn8uCDD9od61Op3ISJYRi89NJLbNy4keTkZOLi4rj77rt5/vnnB/9w3jHavXs3AD/4wQ+47bbbePbZZ0lPT2fevHk0NDTYnC68TNPky1/+Mt/4xjeYMWOG3XFs9eGHH3LPPffw9a9/3e4oIVVXV4ff7yc3N7fX9dzc3KiYcu4WCAS46aabOOWUU5g4caLdccLu8ccfZ8OGDSxfvtzuKANq9+7d3H///YwdO5YXXniB66+/nm9/+9s88sgjdkc7IpWbo3TLLbdgGMYRP7Zt24Zpmtxwww3k5OSwdu1a3n77bRYsWMBFF11EZWWl3W+jX/r63gOBAAC33norl156KdOnT+cPf/gDhmHw5JNP2vwu+qev7/2ee+6hpaWFpUuX2h05ZPr63g9WXl7Oueeey2WXXcZ1111nU3IJpxtuuIEPPviAxx9/3O4oYVdWVsaNN97Io48+SlxcnN1xBlQgEGDatGnceeedTJ06la997Wtcd911rFy50u5oR6TbLxyl2tpa6uvrj/iYUaNGsXbtWs4++2wOHDjQ69bwY8eO5dprr+WWW24Jd9SQ6+t7f/311/nMZz7D2rVrOfXUU3u+Nnv2bObPn89PfvKTcEcNub6+9y984Qv885//xDCMnut+vx+n08mVV1456P+2cyh9fe8ulwuAiooK5s2bx0knncTDDz+MwxFZf4fyeDwkJCTw1FNP9doBeM0119DY2MgzzzxjX7gBsnjxYp555hleffVVRo4caXecsHv66ae5+OKLcTqdPdf8fj+GYeBwOOjq6ur1tUgyYsQIzjrrLH7/+9/3XLv//vu54447KC8vtzHZkcXYHWCoyc7OJjs7+1Mf197eDvCJH+wOh6NnZGOo6et7nz59Om63m+3bt/eUG6/XS2lpKSNGjAh3zLDo63v/zW9+wx133NHzeUVFBeeccw5PPPEEs2fPDmfEsOnrewdrxOaMM87oGa2LtGID4HK5mD59OqtXr+4pN4FAgNWrV7N48WJ7w4WZaZp861vf4u9//ztr1qyJimIDcOaZZ7Jp06Ze1xYtWsSECRO4+eabI7bYAJxyyimf2O6/Y8eOwf+z3NblzBGstrbWzMzMNC+55BKzpKTE3L59u/md73zHjI2NNUtKSuyOF3Y33nijWVhYaL7wwgvmtm3bzGuvvdbMyckxGxoa7I42oPbs2RM1u6X2799vjhkzxjzzzDPN/fv3m5WVlT0fkebxxx833W63+fDDD5tbtmwxv/a1r5lpaWlmVVWV3dHC6vrrrzdTU1PNNWvW9Pr3297ebne0ARctu6XefvttMyYmxvzJT35i7ty503z00UfNhIQE809/+pPd0Y5I5SaM3nnnHfPss882MzIyzOTkZPOkk04yn3vuObtjDQiPx2P+v//3/8ycnBwzOTnZnD9/vvnBBx/YHWvARVO5+cMf/mACh/yIRPfcc485fPhw0+VymbNmzTLffPNNuyOF3eH+/f7hD3+wO9qAi5ZyY5qm+c9//tOcOHGi6Xa7zQkTJpi/+93v7I70qbTmRkRERCJK5E2Ii4iISFRTuREREZGIonIjIiIiEUXlRkRERCKKyo2IiIhEFJUbERERiSgqNyIiIhJRVG5EREQkoqjciIiISERRuREREZGIonIjIiIiEUXlRkSGvNraWvLy8rjzzjt7rr3xxhu4XC5Wr15tYzIRsYNunCkiEeG5555jwYIFvPHGG4wfP54pU6bwuc99jrvvvtvuaCIywFRuRCRi3HDDDbz00kvMmDGDTZs28c477+B2u+2OJSIDTOVGRCJGR0cHEydOpKysjPXr1zNp0iS7I4mIDbTmRkQixq5du6ioqCAQCFBaWmp3HBGxiUZuRCQieDweZs2axZQpUxg/fjwrVqxg06ZN5OTk2B1NRAaYyo2IRIT//d//5amnnuK9994jKSmJuXPnkpqayrPPPmt3NBEZYJqWEpEhb82aNaxYsYJVq1aRkpKCw+Fg1apVrF27lvvvv9/ueCIywDRyIyIiIhFFIzciIiISUVRuREREJKKo3IiIiEhEUbkRERGRiKJyIyIiIhFF5UZEREQiisqNiIiIRBSVGxEREYkoKjciIiISUVRuREREJKKo3IiIiEhE+f+gUbiT1o4xOQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# TODO: Make code that automatically slices these up.\n",
    "theta_test = jnp.expand_dims(X[:, X.shape[1] // 2], -1)\n",
    "# d_test = jnp.expand_dims(X[:, 4], -1)\n",
    "d_test = X[:, -len_x:-len_xi]\n",
    "xi_test = X[:, -len_xi:]\n",
    "# xi_test = jnp.expand_dims(X[:, 2], -1)\n",
    "# xi_test = jnp.ones((10000, 1)) * 3\n",
    "# xi_test = jnp.expand_dims(X[:, 1], -1)\n",
    "\n",
    "\n",
    "samples = model_sample.apply(params, \n",
    "                    next(prng_seq),\n",
    "                    num_samples=len(theta_test),\n",
    "                    theta=theta_test,\n",
    "                    d=d_test,\n",
    "                    # d=d_obs,\n",
    "                    xi=xi_test)\n",
    "\n",
    "\n",
    "density_1 = gaussian_kde(samples[:, 0])\n",
    "density_2 = gaussian_kde(samples[:, 1])\n",
    "\n",
    "\n",
    "# Plot the density\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(jnp.min(samples), jnp.max(samples), 100)\n",
    "ax.plot(x, density_1(x), label='x1')\n",
    "ax.plot(x, density_2(x), label='x2')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFI-ACE\n",
    "Manually trained and stepped-through LFI-ACE model.\n",
    "\n",
    "1. Approximate likelihood using normalizing flow. Use a bunch of samples and their corresponding $\\theta$ values. Also, since the `pyro` version only uses one noise element, get rid of the other one that Kleinegesse used.\n",
    "2. Use approximated likelihood in ACE computation.\n",
    "3. Approximate the likelihood using LFI-ACE.\n",
    "\n",
    "This is code where i'm experimenting with the `update`, `loss`, and `params` stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "def sim_data(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack(\n",
    "        (y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d))))\n",
    "    )\n",
    "\n",
    "def lfi_ace_loss_fn(\n",
    "    params: hk.Params, prng_key: PRNGKey, x: Array, theta: Array, d: Array, xi: Array, \n",
    "    M: int, prior_dists, \n",
    ") -> Array:\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, d, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sim_linear_prior(num_samples: int, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Simulate prior samples and return their log_prob.\n",
    "    \"\"\"\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    samples, log_prob = base_distribution.sample_and_log_prob(seed=key, sample_shape=[num_samples])\n",
    "\n",
    "    return samples, log_prob\n",
    "\n",
    "\n",
    "def lfi_pce_loss_fn(\n",
    "    params: hk.Params, prng_key: PRNGKey, x: Array, theta: Array, d: Array, num_samples: int, # xi: Array, \n",
    "    M: int #, prior_dist: Distribution\n",
    ") -> Array:\n",
    "    keys = jrandom.split(prng_key, 2)\n",
    "    xi = params['xi']\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "    # theta_0 = prior_dist.sample((num_samples, 1))\n",
    "    # simulate the outcomes before finding their log_probs\n",
    "    d_sim = jnp.concatenate((d, xi), axis=0)\n",
    "    X = sim_data(d_sim, num_samples, keys[0])  # Do I need to split up the prng_key?\n",
    "\n",
    "    # I'm implicitly returning the prior here, that's a little annoying...\n",
    "    x, theta_0, d, xi = prepare_data(X)  # TODO: Maybe refactor this?\n",
    "\n",
    "    conditional_lp = log_prob.apply(flow_params, x, theta_0, d, xi)\n",
    "    # theta_L = prior_dist.sample((num_samples, M-1))\n",
    "    # Need to make an array of the new theta values\n",
    "    \n",
    "    contrastive_lps = []\n",
    "    for _ in range(M):\n",
    "        theta, _ = sim_linear_prior(num_samples, keys[1])\n",
    "        contrastive_lp = log_prob.apply(flow_params, x, theta, d, xi)\n",
    "        contrastive_lps.append(contrastive_lp)\n",
    "\n",
    "    marginal_log_prbs = jnp.concatenate((conditional_lp, jnp.array(contrastive_lps)))\n",
    "\n",
    "    marginal_lp = jax.nn.logsumexp(marginal_log_prbs, -1) - math.log(num_samples)\n",
    "\n",
    "    return _safe_mean_terms(conditional_lp - marginal_lp)\n",
    "\n",
    "\n",
    "    \n",
    "    theta_L = jnp.concatenate((theta_0, theta_L), axis=-1)\n",
    "    flow_loss = -jnp.mean(likelihood_0)\n",
    "    likelihood_L = log_prob.apply(flow_params, x, theta_L, d, xi)\n",
    "\n",
    "    marginal = jax.nn.logsumexp(likelihoods, -1) - math.log(num_samples)\n",
    "    eig_estimate = (likelihoods.exp() * (likelihoods - marginal)).sum(-1).mean(0)\n",
    "    surrogate_loss = eig_estimate.sum() + flow_loss\n",
    "    return surrogate_loss, eig_estimate\n",
    "\n",
    "    # Take N samples of the model\n",
    "    expanded_design = lexpand(design, N)  # N copies of the model\n",
    "    trace = poutine.trace(model).get_trace(expanded_design)\n",
    "    trace.compute_log_prob()\n",
    "    conditional_lp = sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "    y_dict = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "    # Resample M values of theta and compute conditional probabilities\n",
    "    conditional_model = pyro.condition(model, data=y_dict)\n",
    "    # Using (M, 1) instead of (M, N) - acceptable to re-use thetas between ys because\n",
    "    # theta comes before y in graphical model\n",
    "    reexpanded_design = lexpand(design, M, 1)  # sample M theta\n",
    "    retrace = poutine.trace(conditional_model).get_trace(reexpanded_design)\n",
    "    retrace.compute_log_prob()\n",
    "    marginal_log_probs = torch.cat([lexpand(conditional_lp, 1),\n",
    "                                    sum(retrace.nodes[l][\"log_prob\"] for l in observation_labels)])\n",
    "    marginal_lp = marginal_log_probs.logsumexp(0) - math.log(M+1)\n",
    "\n",
    "    return _safe_mean_terms(conditional_lp - marginal_lp)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    x, theta, d, xi = prepare_data(batch)\n",
    "    grads = jax.grad(loss_fn)(params, prng_key, x, theta, d, xi)\n",
    "    # grads_d = jax.grad(loss_fn, argnums=5)(params, prng_key, x, theta, d, xi)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing it completely manual\n",
    "Just to work out the bugs ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def sim_linear_prior(num_samples: int, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Simulate prior samples and return their log_prob.\n",
    "    \"\"\"\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    samples, log_prob = base_distribution.sample_and_log_prob(seed=key, sample_shape=[num_samples])\n",
    "\n",
    "    return samples, log_prob\n",
    "\n",
    "def jax_lexpand(A, *dimensions):\n",
    "    \"\"\"Expand tensor, adding new dimensions on left.\"\"\"\n",
    "    if jnp.isscalar(A):\n",
    "        A = A * jnp.ones(dimensions)\n",
    "        return A\n",
    "    shape = tuple(dimensions) + A.shape\n",
    "    A = A[jnp.newaxis, ...]\n",
    "    A = jnp.broadcast_to(A, shape)\n",
    "    return A\n",
    "\n",
    "\n",
    "def lfi_pce_eig(params: hk.Params, prng_key: PRNGKey, N: int=100, M: int=10, **kwargs):\n",
    "    keys = jrandom.split(prng_key, 3 + M)\n",
    "    xi = jnp.squeeze(params['xi'])\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "\n",
    "    # simulate the outcomes before finding their log_probs\n",
    "    X = sim_data(d_sim, num_samples, keys[0])  # Do I need to split up the prng_key?\n",
    "\n",
    "    # I'm implicitly returning the prior here, that's a little annoying...\n",
    "    x, theta_0, d, xi = prepare_data(X)  # TODO: Maybe refactor this?\n",
    "\n",
    "    conditional_lp = log_prob.apply(flow_params, x, theta_0, d, xi)\n",
    "\n",
    "    contrastive_lps = []\n",
    "    thetas = []\n",
    "    # TODO: can this be parallelized to speed up the computation?\n",
    "    for i in range(M):\n",
    "        theta, _ = sim_linear_prior(num_samples, keys[i + 1])\n",
    "        thetas.append(theta)\n",
    "        contrastive_lp = log_prob.apply(flow_params, x, theta, d, xi)\n",
    "        contrastive_lps.append(contrastive_lp)\n",
    "\n",
    "    marginal_log_prbs = jnp.concatenate((jax_lexpand(conditional_lp, 1), jnp.array(contrastive_lps)))\n",
    "\n",
    "    marginal_lp = jax.nn.logsumexp(marginal_log_prbs, 0) - math.log(M + 1)\n",
    "\n",
    "    return sum(conditional_lp - marginal_lp) - jnp.mean(contrastive_lp)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_pce(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, N: int, M: int\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    grads = jax.grad(lfi_pce_eig)(params, prng_key, N=num_samples, M=inner_samples)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n",
    "\n",
    "# Boilerplate setup\n",
    "seed = 1231\n",
    "M = 10\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "d = jnp.array([0.])\n",
    "xi = jnp.array([0.1])\n",
    "if d.size == 0:\n",
    "    d_sim = xi\n",
    "else:\n",
    "    d_sim = jnp.concatenate((d, xi), axis=0)\n",
    "# d_sim = jnp.concatenate((d, xi), axis=0)\n",
    "\n",
    "# Params and hyperparams\n",
    "len_x = len(d_sim)\n",
    "len_d = len(d)\n",
    "len_xi = len(xi)\n",
    "\n",
    "theta_shape = (2,)\n",
    "d_shape = (len(d),)\n",
    "xi_shape = (len_xi,)\n",
    "EVENT_SHAPE = (len(d_sim),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "# cond_info_shape = (theta_shape[0], len_d, len_xi)\n",
    "\n",
    "num_samples = 10\n",
    "inner_samples = 10 # AKA M or L in BOED parlance\n",
    "batch_size = 128\n",
    "flow_num_layers = 5 #3 # 10\n",
    "mlp_num_layers = 1 # 3 # 4\n",
    "hidden_size = 8 # 128 # 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 10\n",
    "early_stopping_memory = 10\n",
    "early_stopping_threshold = 5e-2\n",
    "\n",
    "training_steps = 100\n",
    "eval_frequency = 5\n",
    "\n",
    "# Initialzie the params\n",
    "prng_seq = hk.PRNGSequence(42)  # TODO: Put one of \"keys\" here?\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    # np.zeros((1, *d_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "# init_params = params.copy()\n",
    "params['xi'] = xi\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# TODO: put this function in training since d will be changing.\n",
    "# num_samples = 10000\n",
    "# X = sim_data(d_sim, num_samples, key)\n",
    "\n",
    "# shift = X.mean(axis=0)\n",
    "# scale = X.std(axis=0) + 1e-14\n",
    "\n",
    "# # Create tf dataset from sklearn dataset\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# # Splitting into train/validate ds\n",
    "# train = dataset.skip(2000)\n",
    "# val = dataset.take(2000)\n",
    "\n",
    "# # load_dataset(split: tfds.Split, batch_size: int)\n",
    "# train_ds = load_dataset(train, 512)\n",
    "# valid_ds = load_dataset(val, 512)\n",
    "# print(params)\n",
    "# loss_deque = deque(maxlen=early_stopping_memory)\n",
    "# for step in range(training_steps):\n",
    "#     params, opt_state = update_pce(\n",
    "#         params, next(prng_seq), opt_state, N=num_samples, M=M\n",
    "#     )\n",
    "#     # params, opt_state, grads_d = update(\n",
    "#     #     params, next(prng_seq), opt_state, next(train_ds)\n",
    "#     # )\n",
    "\n",
    "#     print(f\"STEP: {step:5d}; Xi: {params['xi']}\")\n",
    "    # if step % eval_frequency == 0:\n",
    "    #     val_loss = eval_fn(params, next(valid_ds))\n",
    "    #     print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "    \n",
    "    #     loss_deque.append(val_loss)\n",
    "    #     avg_abs_diff = jnp.mean(abs(jnp.array(loss_deque) - sum(loss_deque)/len(loss_deque)))\n",
    "    #     if step > warmup_steps and avg_abs_diff < early_stopping_threshold:\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads[\"xi\"] = grads_xi\n",
    "updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "new_params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['conditioner_module/mlp/~/linear_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(3.6019292, dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfi_pce_eig(params, key, N=num_samples, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = jax.grad(lfi_pce_eig)(params, key, N=num_samples, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "new_params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *d_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "# init_params = params.copy()\n",
    "params['xi'] = xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "keys = jrandom.split(key, 3 + M)\n",
    "num_samples = 10\n",
    "\n",
    "xi = params['xi']\n",
    "flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "\n",
    "X = sim_data(d_sim, num_samples, keys[0])  # Do I need to split up the prng_key?\n",
    "\n",
    "# I'm implicitly returning the prior here, that's a little annoying...\n",
    "x, theta_0, d, xi = prepare_data(X)\n",
    "\n",
    "contrastive_lps = []\n",
    "thetas = []\n",
    "for i in range(M):\n",
    "    theta, _ = sim_linear_prior(num_samples, keys[i + 1])\n",
    "    thetas.append(theta)\n",
    "    contrastive_lp = log_prob.apply(flow_params, x, theta, xi)\n",
    "    contrastive_lps.append(contrastive_lp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DeviceArray([[  0.64793307,  12.057756  ],\n",
       "              [ -1.8036    , -21.126171  ],\n",
       "              [  1.133066  ,  11.62901   ],\n",
       "              [ -4.5348973 ,  -7.0127087 ],\n",
       "              [ -5.4115043 ,  -3.4683409 ],\n",
       "              [ -7.423147  ,   7.645422  ],\n",
       "              [  1.5872409 ,  11.221082  ],\n",
       "              [  0.8586352 , -10.206353  ],\n",
       "              [  8.100239  ,  -6.1539264 ],\n",
       "              [-13.176195  ,  10.821227  ]], dtype=float32),\n",
       " DeviceArray([[ -6.1988173 ,  -1.2310148 ],\n",
       "              [-12.849492  , -12.620625  ],\n",
       "              [ -7.4009905 ,  -3.9226563 ],\n",
       "              [ 17.474777  ,  14.818273  ],\n",
       "              [  6.0482283 ,   6.752823  ],\n",
       "              [ 13.28307   ,  -1.4132088 ],\n",
       "              [ -2.7293384 ,   3.487775  ],\n",
       "              [  7.874885  ,  -0.23380284],\n",
       "              [-14.064912  ,  11.756083  ],\n",
       "              [  2.0690577 ,  14.524533  ]], dtype=float32),\n",
       " DeviceArray([[ -3.65689  ,   7.231424 ],\n",
       "              [ -1.9369462,   1.1989701],\n",
       "              [ 12.946467 ,   1.407098 ],\n",
       "              [ -3.5840957, -12.995476 ],\n",
       "              [-12.196788 ,  23.040134 ],\n",
       "              [  3.5272124,  -2.0084808],\n",
       "              [ -2.1063745,  -6.3984814],\n",
       "              [ 14.970311 ,   7.23661  ],\n",
       "              [ -7.285979 ,  -8.209857 ],\n",
       "              [  4.1615047, -17.095598 ]], dtype=float32),\n",
       " DeviceArray([[-13.744735 ,  -1.392093 ],\n",
       "              [  3.3621225,  -1.5808239],\n",
       "              [-17.727169 ,   1.4319452],\n",
       "              [  4.982991 ,  -2.221058 ],\n",
       "              [-14.190501 ,   9.979641 ],\n",
       "              [ 16.577234 ,   3.7251465],\n",
       "              [ -1.0247506, -10.063461 ],\n",
       "              [ 11.156546 , -13.290407 ],\n",
       "              [ 12.168148 ,  -6.211318 ],\n",
       "              [ 20.059345 ,  -8.960184 ]], dtype=float32),\n",
       " DeviceArray([[  3.2364473,  -5.7628765],\n",
       "              [ -1.1260009,   2.75081  ],\n",
       "              [  5.3766537,  -5.6687036],\n",
       "              [-16.1931   ,   4.9224205],\n",
       "              [-20.17436  ,  -4.9318695],\n",
       "              [ -1.8989071, -16.489414 ],\n",
       "              [  3.6700122, -15.912622 ],\n",
       "              [  2.958536 ,  -3.0327377],\n",
       "              [ -7.1398616,  -3.5663493],\n",
       "              [ -7.7596307,  13.272042 ]], dtype=float32),\n",
       " DeviceArray([[  3.3430977 ,   1.6305696 ],\n",
       "              [ -5.9304676 ,   1.5628601 ],\n",
       "              [  3.458716  ,  -1.5521276 ],\n",
       "              [-23.666735  ,   7.204771  ],\n",
       "              [  2.3833272 ,  12.6059885 ],\n",
       "              [  0.08012465,  16.417442  ],\n",
       "              [ 11.023899  ,   1.334738  ],\n",
       "              [ -8.274364  ,   6.487465  ],\n",
       "              [  4.8539615 ,   7.545137  ],\n",
       "              [ 15.07491   ,   9.83883   ]], dtype=float32),\n",
       " DeviceArray([[ -4.6484966 ,   2.0848312 ],\n",
       "              [ -1.5853441 ,   8.249429  ],\n",
       "              [-14.634294  ,  11.465833  ],\n",
       "              [  1.3609746 ,  -5.1135073 ],\n",
       "              [  7.9940386 ,  -3.1233866 ],\n",
       "              [ -6.9010124 ,   6.668855  ],\n",
       "              [  0.25995153,  -3.5660882 ],\n",
       "              [ -8.069077  ,  -4.7544723 ],\n",
       "              [  4.3881006 ,  -1.1735852 ],\n",
       "              [ -7.151715  ,   8.583297  ]], dtype=float32),\n",
       " DeviceArray([[  5.995773 ,  -5.84251  ],\n",
       "              [  3.3989809,   2.707823 ],\n",
       "              [  3.953537 ,  11.817855 ],\n",
       "              [ -9.604214 , -12.563715 ],\n",
       "              [ -0.3455139,  14.815364 ],\n",
       "              [-16.427826 ,  10.514871 ],\n",
       "              [-10.316995 ,   6.9162083],\n",
       "              [ -5.8247757, -15.326024 ],\n",
       "              [ -2.8184185,  -2.100858 ],\n",
       "              [ -8.592105 ,  -4.4695926]], dtype=float32),\n",
       " DeviceArray([[ 4.266702  , -3.480713  ],\n",
       "              [-3.6858387 ,  4.6207247 ],\n",
       "              [-5.331479  , -3.3922307 ],\n",
       "              [ 2.2157893 , -9.628382  ],\n",
       "              [ 7.437256  , -5.187854  ],\n",
       "              [ 2.8552182 , -2.1501591 ],\n",
       "              [10.239139  , -0.30657387],\n",
       "              [ 4.5631547 ,  3.3490367 ],\n",
       "              [-2.8893983 ,  3.3446567 ],\n",
       "              [10.422832  , 20.6276    ]], dtype=float32),\n",
       " DeviceArray([[ -4.398278 ,   3.1264844],\n",
       "              [ -9.21753  ,   7.2552476],\n",
       "              [-11.973638 ,  -5.0422826],\n",
       "              [-13.711408 ,   0.8138804],\n",
       "              [ 21.733585 ,  -7.9876537],\n",
       "              [  5.6295214,  -1.5190306],\n",
       "              [ -3.4423   ,   4.632292 ],\n",
       "              [ -2.259205 ,   3.053364 ],\n",
       "              [ -5.7116723, -15.013002 ],\n",
       "              [ -8.434806 ,   5.2537813]], dtype=float32)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[  0.64793307,  12.057756  ],\n",
       "             [ -1.8036    , -21.126171  ],\n",
       "             [  1.133066  ,  11.62901   ],\n",
       "             [ -4.5348973 ,  -7.0127087 ],\n",
       "             [ -5.4115043 ,  -3.4683409 ],\n",
       "             [ -7.423147  ,   7.645422  ],\n",
       "             [  1.5872409 ,  11.221082  ],\n",
       "             [  0.8586352 , -10.206353  ],\n",
       "             [  8.100239  ,  -6.1539264 ],\n",
       "             [-13.176195  ,  10.821227  ]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-6.6468067, -8.090832 , -6.516926 , -7.5159564, -6.470993 ,\n",
       "             -7.5977902, -6.9273844, -6.67619  , -9.048645 , -6.1884484],            dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob.apply(flow_params, x, thetas[0], xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ -6.1988173 ,  -1.2310148 ],\n",
       "             [-12.849492  , -12.620625  ],\n",
       "             [ -7.4009905 ,  -3.9226563 ],\n",
       "             [ 17.474777  ,  14.818273  ],\n",
       "             [  6.0482283 ,   6.752823  ],\n",
       "             [ 13.28307   ,  -1.4132088 ],\n",
       "             [ -2.7293384 ,   3.487775  ],\n",
       "             [  7.874885  ,  -0.23380284],\n",
       "             [-14.064912  ,  11.756083  ],\n",
       "             [  2.0690577 ,  14.524533  ]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-6.6468067, -8.090832 , -6.516926 , -7.5159564, -6.470993 ,\n",
       "             -7.5977902, -6.9273844, -6.67619  , -9.048645 , -6.1884484],            dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob.apply(flow_params, x, thetas[1], xi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Am I think about this the wrong way? When I generate samples and then check the log_prob earlier while varying the conditioning parameters, it turned out as expected. Here, with different theta values, I get the same `log_prob`. There has to be a bug or issue somewhere else in this code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "              True,  True], dtype=bool)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_lps[0] == contrastive_lps[4]\n",
    "# wtf? get back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_log_prbs = jnp.concatenate((jax_lexpand(conditional_lp, 1), jnp.array(contrastive_lps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 10)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginal_log_prbs.shape  # This may need to have an extra dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "marginal_lp = jax.nn.logsumexp(marginal_log_prbs, 0) - math.log(M + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# marginal_log_prbs -> marginal_lp needs to be over 0th dimension to remove 11\n",
    "marginal_lp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_lp.shape  # Think this needs to be a 1D array not scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_lp - marginal_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_mean_terms(terms):\n",
    "    mask = torch.isnan(terms) | (terms == float('-inf')) | (terms == float('inf'))\n",
    "    if terms.dtype is torch.float32:\n",
    "        nonnan = (~mask).sum(0).float()\n",
    "    elif terms.dtype is torch.float64:\n",
    "        nonnan = (~mask).sum(0).double()\n",
    "    terms[mask] = 0.\n",
    "    loss = terms.sum(0) / nonnan\n",
    "    agg_loss = loss.sum()\n",
    "    return agg_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conditioner_module/linear', 'conditioner_module/linear_1', 'conditioner_module/linear_2', 'conditioner_module/linear_3', 'conditioner_module/linear_4', 'conditioner_module/mlp/~/linear_0', 'conditioner_module/mlp_1/~/linear_0', 'conditioner_module/mlp_2/~/linear_0', 'conditioner_module/mlp_3/~/linear_0'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['xi'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conditioner_module/linear', 'conditioner_module/linear_1', 'conditioner_module/linear_2', 'conditioner_module/linear_3', 'conditioner_module/linear_4', 'conditioner_module/mlp/~/linear_0', 'conditioner_module/mlp_1/~/linear_0', 'conditioner_module/mlp_2/~/linear_0', 'conditioner_module/mlp_3/~/linear_0', 'xi'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = params.pop('xi', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conditioner_module/linear', 'conditioner_module/linear_1', 'conditioner_module/linear_2', 'conditioner_module/linear_3', 'conditioner_module/linear_4', 'conditioner_module/mlp/~/linear_0', 'conditioner_module/mlp_1/~/linear_0', 'conditioner_module/mlp_2/~/linear_0', 'conditioner_module/mlp_3/~/linear_0'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train. Just need a vector of the samples and thetas that produced them. Don't need a simulator or the prior quite yet.\n",
    "- How do I track the training curves in Jax/Haiku? Gave in and am using WANDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:     0; Validation loss: 9.587\n",
      "STEP:    10; Validation loss: 9.453\n",
      "STEP:    20; Validation loss: 9.295\n",
      "STEP:    30; Validation loss: 9.160\n",
      "STEP:    40; Validation loss: 8.990\n",
      "STEP:    50; Validation loss: 8.950\n",
      "STEP:    60; Validation loss: 8.914\n",
      "STEP:    70; Validation loss: 8.899\n",
      "STEP:    80; Validation loss: 8.833\n",
      "STEP:    90; Validation loss: 8.782\n",
      "STEP:   100; Validation loss: 8.803\n",
      "STEP:   110; Validation loss: 8.844\n",
      "STEP:   120; Validation loss: 8.850\n",
      "STEP:   130; Validation loss: 8.681\n",
      "STEP:   140; Validation loss: 8.731\n",
      "STEP:   150; Validation loss: 8.693\n",
      "STEP:   160; Validation loss: 8.693\n",
      "STEP:   170; Validation loss: 8.739\n",
      "STEP:   180; Validation loss: 8.609\n",
      "STEP:   190; Validation loss: 8.722\n",
      "STEP:   200; Validation loss: 8.581\n",
      "STEP:   210; Validation loss: 8.620\n",
      "STEP:   220; Validation loss: 8.705\n"
     ]
    }
   ],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.init(project=\"lfiax_linRegression_ACE\", entity=\"vz_uci\")\n",
    "\n",
    "# TODO: Put this in hydra config file\n",
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "# d = jnp.array([-10.0, 0.0, 5.0, 10.0])\n",
    "# d = jnp.array([1., 2.])\n",
    "# d = jnp.array([1.])\n",
    "# d_obs = jnp.array([0.])\n",
    "d_obs = jnp.array([])\n",
    "# d_prop = jrandom.uniform(key, shape=(1,), minval=-10.0, maxval=10.0)\n",
    "xi = jnp.array([0.])\n",
    "# d_prop = jnp.array([])\n",
    "d_sim = jnp.concatenate((d_obs, xi), axis=0)\n",
    "len_x = len(d_sim)\n",
    "len_d = len(d_obs)\n",
    "len_xi = len(xi)\n",
    "num_samples = 100\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "d_shape = (len(d_obs),)\n",
    "xi_shape = (len_xi,)\n",
    "EVENT_SHAPE = (len(d_sim),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = (theta_shape[0], len_d, len_xi)\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 5 #3 # 10\n",
    "mlp_num_layers = 4 # 3 # 4\n",
    "hidden_size = 128 # 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 100\n",
    "early_stopping_memory = 10\n",
    "early_stopping_threshold = 5e-2\n",
    "\n",
    "training_steps = 500\n",
    "eval_frequency = 10\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 10000\n",
    "# TODO: put this function in training since d will be changing.\n",
    "X = sim_data(d_sim, num_samples, key)\n",
    "\n",
    "shift = X.mean(axis=0)\n",
    "scale = X.std(axis=0) + 1e-14\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *d_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "params['xi'] = xi\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Can change the length of the deque for more/less leniency in measuring the loss\n",
    "loss_deque = deque(maxlen=early_stopping_memory)\n",
    "for step in range(training_steps):\n",
    "    params, opt_state, grads_d = update(\n",
    "        params, next(prng_seq), opt_state, next(train_ds)\n",
    "    )\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "    \n",
    "        loss_deque.append(val_loss)\n",
    "        avg_abs_diff = jnp.mean(abs(jnp.array(loss_deque) - sum(loss_deque)/len(loss_deque)))\n",
    "        if step > warmup_steps and avg_abs_diff < early_stopping_threshold:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([10.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['xi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_prop.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the sampling/log-prob stuff work. Now to actually implment LFI ACE. I've got the flow, now just going to do some of the ACE computations manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_lexpand(A, *dimensions):\n",
    "    \"\"\"Expand tensor, adding new dimensions on left.\"\"\"\n",
    "    if jnp.isscalar(A):\n",
    "        A = A * jnp.ones(dimensions)\n",
    "        return A\n",
    "    shape = tuple(dimensions) + A.shape\n",
    "    A = A[jnp.newaxis, ...]\n",
    "    A = jnp.broadcast_to(A, shape)\n",
    "    return A\n",
    "\n",
    "\n",
    "# Walking through and commenting the code\n",
    "def lfi_ace_eig_loss(model:NormalizingFlow, guide:NormalizingFlow, M, observation_labels, target_labels):\n",
    "    def loss_fn(design, num_particles, evaluation=False, **kwargs):\n",
    "        N = num_particles\n",
    "        # Expand to the number of parallel designs being evaluated\n",
    "        expanded_design = lexpand(design, N)\n",
    "        \n",
    "        # TODO: make a for loop over the proposed models here.\n",
    "        for model in models:\n",
    "            # TODO: would be cool to make sampling from the model's prior a method\n",
    "             # Hmm I need N copies for parallelization. I can figure that out later, though.\n",
    "            model_thetas_0 = model.prior_distribution.sample(num_samples)\n",
    "            y_0, log_prob_y_0 = model.sample_with_log_prob(model_thetas_0, expanded_design)\n",
    "            # Get a dictionary of the expanded y and theta values, just make M copies of y values\n",
    "            # TODO: think of a better way to add these into expanded dictionary values...\n",
    "            y_dict_exp = lexpand(y, M)\n",
    "\n",
    "            # This is essentially the p(y|theta)p(theta)\n",
    "            # Ohh, each model will have two ratios, it's current prob and its prior one\n",
    "            marginal_terms_cross = sum(model.prior_distribution.log_prob(model_thetas_0))\n",
    "            marginal_terms_cross += sum(log_prob_y_0)\n",
    "\n",
    "        # Pray that jax can handle this parallelization - vmap and pmap to the rescue!...?\n",
    "        for _ in range(m):\n",
    "            for model in models:\n",
    "                # Sample from q(theta | y, d) using the guide normalizing flow\n",
    "                theta, log_prob = guide.sample_with_log_prob(y, expanded_design, observation_labels, target_labels)\n",
    "                theta_y_dict = {l: theta[l] for l in target_labels}\n",
    "                theta_y_dict.update(y_dict_exp)\n",
    "\n",
    "                marginal_terms_proposal = -sum(log_prob[l] for l in target_labels)\n",
    "                marginal_terms_proposal += sum(log_prob[l] for l in target_labels)\n",
    "                marginal_terms_proposal += sum(log_prob[l] for l in observation_labels)\n",
    "\n",
    "                marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "                terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "                # At eval time, add p(y | theta, d) terms\n",
    "                if evaluation:\n",
    "                    terms += sum(log_prob[l] for l in observation_labels)\n",
    "                return _safe_mean_terms(terms)\n",
    "\n",
    "        # Sample from q(theta | y, d) using the guide normalizing flow\n",
    "        theta, log_prob = guide.sample_with_log_prob(y, expanded_design, observation_labels, target_labels)\n",
    "        theta_y_dict = {l: theta[l] for l in target_labels}\n",
    "        theta_y_dict.update(y_dict_exp)\n",
    "\n",
    "        marginal_terms_proposal = -sum(log_prob[l] for l in target_labels)\n",
    "        marginal_terms_proposal += sum(log_prob[l] for l in target_labels)\n",
    "        marginal_terms_proposal += sum(log_prob[l] for l in observation_labels)\n",
    "\n",
    "        marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "        terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "        # At eval time, add p(y | theta, d) terms\n",
    "        if evaluation:\n",
    "            terms += sum(log_prob[l] for l in observation_labels)\n",
    "        return _safe_mean_terms(terms)\n",
    "\n",
    "    return loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "p = 20\n",
    "n = 20\n",
    "N = 2 # num_parallel\n",
    "M = 2\n",
    "design = torch.zeros([10, 20, 20])\n",
    "expanded_design = lexpand(design, N)\n",
    "\n",
    "w_prior_loc = torch.zeros(p, device=device)\n",
    "w_prior_scale = scale * torch.ones(p, device=device)\n",
    "sigma_prior_scale = scale * torch.tensor(1., device=device)\n",
    "\n",
    "model_learn_xi = make_regression_model(\n",
    "    w_prior_loc, w_prior_scale, sigma_prior_scale, xi_init)\n",
    "\n",
    "model = model_learn_xi\n",
    "guide = PosteriorGuide(n, p, (N,)).to(device)\n",
    "observation_labels = [\"y\"]\n",
    "target_labels = ['w', 'sigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = poutine.trace(model).get_trace(expanded_design)\n",
    "y_dict_exp = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "y_dict = {l: trace.nodes[l][\"value\"] for l in observation_labels}\n",
    "theta_dict = {l: trace.nodes[l][\"value\"] for l in target_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 20])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dict['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 20])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_dict['w'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_dict['sigma'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 10]), torch.Size([2, 10])]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[trace.nodes[l][\"log_prob\"].shape for l in target_labels]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what's throwing me off. Determining the `log_prob` of the `w` vector compresses it for some reason... Oh, wait, it's the probability of each *vector*. That's why it's compressed to the shape of the `y` and `sigma` shapes. Yea, that makes a lot more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.compute_log_prob()\n",
    "# This is taking the log_prob of a vector from a distribution. I can either use a distribution that \n",
    "# is in a vector format or I think it should be fine to add two separate ones together...\n",
    "marginal_terms_cross = sum(trace.nodes[l][\"log_prob\"] for l in target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_terms_cross += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-48.1695, -70.6690, -58.5410, -57.8346, -82.7463, -79.5401, -34.3802,\n",
       "         -75.5568, -62.8853, -90.3731],\n",
       "        [-68.9690, -46.9697, -79.1669, -81.4540, -75.3442, -28.8084, -77.4504,\n",
       "         -67.5315,   2.8248, -70.4339]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginal_terms_cross"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part would be the denominator of LFI-ACE with the sum of all of the models' log_probs. The numerator might actually be the sum of all the log_probs. The denominator would be the weighted sum... Yea, that makes more sense....\n",
    "\n",
    "Part of the loss could then just be adding on the loss of the prediction of each normalizing flow. That makes one scalar loss. Although, there's an EIG for each of the 'y' outputs, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1\nTrace Shapes:\n Param Sites:\nSample Sites:",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [12], line 108\u001b[0m, in \u001b[0;36mPosteriorGuide.forward\u001b[0;34m(self, y_dict, design_prototype, observation_labels, target_labels)\u001b[0m\n\u001b[1;32m    107\u001b[0m y \u001b[39m=\u001b[39m y_dict[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m.5\u001b[39m\n\u001b[0;32m--> 108\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(y))\n\u001b[1;32m    109\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [12], line 92\u001b[0m, in \u001b[0;36mTensorLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m rmv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39minput\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/contrib/util.py:44\u001b[0m, in \u001b[0;36mrmv\u001b[0;34m(A, b)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m\"\"\"Tensorized matrix vector multiplication of rightmost dimensions.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(A, b\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reguide_trace \u001b[39m=\u001b[39m poutine\u001b[39m.\u001b[39;49mtrace(\n\u001b[1;32m      2\u001b[0m     pyro\u001b[39m.\u001b[39;49mcondition(guide, data\u001b[39m=\u001b[39;49mtheta_dict))\u001b[39m.\u001b[39;49mget_trace(\n\u001b[1;32m      3\u001b[0m     y_dict, expanded_design, observation_labels, target_labels)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Here's a spot where you could update each model's parameters based on log_prob\u001b[39;00m\n\u001b[1;32m      5\u001b[0m reguide_trace\u001b[39m.\u001b[39mcompute_log_prob()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:198\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_trace\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    191\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m    :returns: data structure\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:180\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         exc \u001b[39m=\u001b[39m exc_type(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(exc_value, shapes))\n\u001b[1;32m    179\u001b[0m         exc \u001b[39m=\u001b[39m exc\u001b[39m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39madd_node(\n\u001b[1;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_RETURN\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_RETURN\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, value\u001b[39m=\u001b[39mret\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39madd_node(\n\u001b[1;32m    171\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m_INPUT\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_INPUT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m     exc_type, exc_value, traceback \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_context_wrap\u001b[39m(context, fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [12], line 108\u001b[0m, in \u001b[0;36mPosteriorGuide.forward\u001b[0;34m(self, y_dict, design_prototype, observation_labels, target_labels)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, y_dict, design_prototype, observation_labels, target_labels):\n\u001b[1;32m    107\u001b[0m     y \u001b[39m=\u001b[39m y_dict[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m.5\u001b[39m\n\u001b[0;32m--> 108\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(y))\n\u001b[1;32m    109\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(x))\n\u001b[1;32m    110\u001b[0m     final \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [12], line 92\u001b[0m, in \u001b[0;36mTensorLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m rmv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39minput\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/contrib/util.py:44\u001b[0m, in \u001b[0;36mrmv\u001b[0;34m(A, b)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrmv\u001b[39m(A, b):\n\u001b[1;32m     43\u001b[0m     \u001b[39m\"\"\"Tensorized matrix vector multiplication of rightmost dimensions.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(A, b\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1\nTrace Shapes:\n Param Sites:\nSample Sites:"
     ]
    }
   ],
   "source": [
    "reguide_trace = poutine.trace(\n",
    "    pyro.condition(guide, data=theta_dict)).get_trace(\n",
    "    y_dict, expanded_design, observation_labels, target_labels)\n",
    "# Here's a spot where you could update each model's parameters based on log_prob\n",
    "reguide_trace.compute_log_prob()\n",
    "marginal_terms_cross -= sum(reguide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = poutine.trace(model).get_trace(expanded_design)\n",
    "y_dict_exp = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "y_dict = {l: trace.nodes[l][\"value\"] for l in observation_labels}\n",
    "theta_dict = {l: trace.nodes[l][\"value\"] for l in target_labels}\n",
    "\n",
    "trace.compute_log_prob()\n",
    "marginal_terms_cross = sum(trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_cross += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "reguide_trace = poutine.trace(\n",
    "    pyro.condition(guide, data=theta_dict)).get_trace(\n",
    "    y_dict, expanded_design, observation_labels, target_labels)\n",
    "# Here's a spot where you could update each model's parameters based on log_prob\n",
    "reguide_trace.compute_log_prob()\n",
    "marginal_terms_cross -= sum(reguide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "\n",
    "# Sample M times from q(theta | y, d) for each y\n",
    "reexpanded_design = lexpand(expanded_design, M)\n",
    "guide_trace = poutine.trace(guide).get_trace(\n",
    "    y_dict, reexpanded_design, observation_labels, target_labels\n",
    ")\n",
    "theta_y_dict = {l: guide_trace.nodes[l][\"value\"] for l in target_labels}\n",
    "theta_y_dict.update(y_dict_exp)\n",
    "guide_trace.compute_log_prob()\n",
    "\n",
    "# Re-run that through the model to compute the joint\n",
    "model_trace = poutine.trace(\n",
    "pyro.condition(model, data=theta_y_dict)).get_trace(reexpanded_design)\n",
    "model_trace.compute_log_prob()\n",
    "\n",
    "marginal_terms_proposal = -sum(guide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_proposal += sum(model_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_proposal += sum(model_trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "# At eval time, add p(y | theta, d) terms\n",
    "# if evaluation:\n",
    "terms += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "# breakpoint()\n",
    "_safe_mean_terms(terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdm3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bac022e209f3f8f811ac02bf6d6b971751e1ab224096f1893a92a620959b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
