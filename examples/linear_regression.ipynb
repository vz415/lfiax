{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import distrax\n",
    "import haiku as hk\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from lfiax.flows.nsf import make_nsf\n",
    "\n",
    "from typing import (\n",
    "    Any,\n",
    "    Iterator,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "OptState = Any\n",
    "\n",
    "\n",
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "def sim_true_linear_jax(d: Array, theta_true: Array, key: PRNGKey):\n",
    "    # TODO: check that `theta_true` is the correct size\n",
    "    # TODO: check that function works as expected\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(theta_true)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(theta_true)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(theta_true[:, 0], (len(d), len(theta_true)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(theta_true[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = theta_true[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "def sim_data(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack(\n",
    "        (y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d))))\n",
    "    )\n",
    "\n",
    "\n",
    "def sim_linear_jax_laplace(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    concentration = jnp.ones(noise_shape)\n",
    "    rate = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Gamma(concentration, rate).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = distrax.MultivariateNormalDiag(y, jnp.squeeze(n_n)).sample(seed=keys[1], sample_shape=())\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def sim_data_laplace(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "    theta_shape = (1,)\n",
    "\n",
    "    loc = jnp.zeros(theta_shape)\n",
    "    scale = jnp.ones(theta_shape)\n",
    "\n",
    "    # Leaving in case this fixes future dimensionality issues\n",
    "    # base_distribution = distrax.Independent(\n",
    "    #     distrax.Laplace(loc, scale)\n",
    "    # )\n",
    "    base_distribution = distrax.Laplace(loc, scale)\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    y = sim_linear_jax_laplace(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack(\n",
    "        (y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d))))\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helper functions to simulate data\n",
    "# ----------------------------------------\n",
    "def load_dataset(split: tfds.Split, batch_size: int) -> Iterator[Batch]:\n",
    "    ds = split\n",
    "    ds = ds.shuffle(buffer_size=10 * batch_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=1000)\n",
    "    ds = ds.repeat()\n",
    "    return iter(tfds.as_numpy(ds))\n",
    "\n",
    "\n",
    "def prepare_data(batch: Batch, prng_key: Optional[PRNGKey] = None) -> Array:\n",
    "    # Batch is [y, thetas, d]\n",
    "    data = batch.astype(np.float32)\n",
    "    # Handling the scalar case\n",
    "    if data.shape[1] <= 3:\n",
    "        x = jnp.expand_dims(data[:, :-2], -1)\n",
    "    x = data[:, :len_x]\n",
    "    cond_data = data[:, len_x:]\n",
    "    theta = cond_data[:, :-len_x]\n",
    "    d = cond_data[:, -len_x:-len_xi]\n",
    "    xi = cond_data[:, -len_xi:]\n",
    "    return x, theta, d, xi\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Haiku transform functions for training and evaluation\n",
    "# ----------------------------\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob(data: Array, theta: Array, d: Array, xi: Array) -> Array:\n",
    "    # Get batch\n",
    "    shift = data.mean(axis=0)\n",
    "    scale = data.std(axis=0) + 1e-14\n",
    "\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "        standardize_x=True,\n",
    "        standardize_theta=False,\n",
    "        use_resnet=True,\n",
    "        event_dim=EVENT_DIM,\n",
    "        shift=shift,\n",
    "        scale=scale,\n",
    "    )\n",
    "    return model.log_prob(data, theta, d, xi)\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def model_sample(key: PRNGKey, num_samples: int, theta: Array, d: Array, xi: Array) -> Array:\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "        standardize_x=False,\n",
    "        standardize_theta=False,\n",
    "        use_resnet=True,\n",
    "        event_dim=EVENT_DIM,\n",
    "    )\n",
    "    return model._sample_n(key=key, n=[num_samples], theta=theta, d=d, xi=xi)\n",
    "\n",
    "\n",
    "def loss_fn(\n",
    "    params: hk.Params, prng_key: PRNGKey, x: Array, theta: Array, d: Array, xi: Array\n",
    ") -> Array:\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, d, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_fn(params: hk.Params, batch: Batch) -> Array:\n",
    "    x, theta, d, xi = prepare_data(batch)\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, d, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    # x, cond_data = prepare_data(batch, prng_key)\n",
    "    x, theta, d, xi = prepare_data(batch)\n",
    "    grads = jax.grad(loss_fn)(params, prng_key, x, theta, d, xi)\n",
    "    grads_d = jax.grad(loss_fn, argnums=5)(params, prng_key, x, theta, d, xi)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, grads_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:     0; Validation loss: 4.230\n",
      "STEP:    10; Validation loss: 4.155\n",
      "STEP:    20; Validation loss: 4.267\n",
      "STEP:    30; Validation loss: 4.120\n",
      "STEP:    40; Validation loss: 4.122\n",
      "STEP:    50; Validation loss: 4.140\n",
      "STEP:    60; Validation loss: 4.197\n",
      "STEP:    70; Validation loss: 3.912\n",
      "STEP:    80; Validation loss: 4.003\n",
      "STEP:    90; Validation loss: 3.988\n",
      "STEP:   100; Validation loss: 4.192\n",
      "STEP:   110; Validation loss: 4.106\n",
      "STEP:   120; Validation loss: 4.071\n",
      "STEP:   130; Validation loss: 3.956\n",
      "STEP:   140; Validation loss: 4.015\n",
      "STEP:   150; Validation loss: 4.217\n",
      "STEP:   160; Validation loss: 4.071\n",
      "STEP:   170; Validation loss: 3.981\n",
      "STEP:   180; Validation loss: 4.169\n",
      "STEP:   190; Validation loss: 4.007\n",
      "STEP:   200; Validation loss: 4.088\n",
      "STEP:   210; Validation loss: 4.140\n",
      "STEP:   220; Validation loss: 4.057\n",
      "STEP:   230; Validation loss: 3.902\n",
      "STEP:   240; Validation loss: 4.077\n",
      "STEP:   250; Validation loss: 3.986\n",
      "STEP:   260; Validation loss: 4.116\n",
      "STEP:   270; Validation loss: 4.017\n",
      "STEP:   280; Validation loss: 4.001\n",
      "STEP:   290; Validation loss: 4.042\n",
      "STEP:   300; Validation loss: 4.235\n",
      "STEP:   310; Validation loss: 3.878\n",
      "STEP:   320; Validation loss: 3.903\n",
      "STEP:   330; Validation loss: 4.170\n",
      "STEP:   340; Validation loss: 4.026\n",
      "STEP:   350; Validation loss: 4.081\n",
      "STEP:   360; Validation loss: 3.983\n",
      "STEP:   370; Validation loss: 3.997\n",
      "STEP:   380; Validation loss: 4.180\n",
      "STEP:   390; Validation loss: 4.032\n",
      "STEP:   400; Validation loss: 3.949\n",
      "STEP:   410; Validation loss: 3.973\n",
      "STEP:   420; Validation loss: 4.233\n",
      "STEP:   430; Validation loss: 3.993\n",
      "STEP:   440; Validation loss: 3.963\n",
      "STEP:   450; Validation loss: 4.146\n",
      "STEP:   460; Validation loss: 4.073\n",
      "STEP:   470; Validation loss: 3.965\n",
      "STEP:   480; Validation loss: 3.917\n",
      "STEP:   490; Validation loss: 4.071\n"
     ]
    }
   ],
   "source": [
    "# TODO: Put this in hydra config file\n",
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "# d = jnp.array([-10.0, 0.0, 5.0, 10.0])\n",
    "# d = jnp.array([1., 2.])\n",
    "# d = jnp.array([1.])\n",
    "# d_obs = jnp.array([0.])\n",
    "# d_obs = jnp.array([])\n",
    "# d_prop = jrandom.uniform(key, shape=(1,), minval=-10.0, maxval=10.0)\n",
    "# d_prop = jnp.array([5.])\n",
    "# d_prop = jnp.array([])\n",
    "d_sim = jnp.concatenate((d_obs, d_prop), axis=0)\n",
    "len_x = len(d_sim)\n",
    "len_d = len(d_obs)\n",
    "len_xi = len(d_prop)\n",
    "num_samples = 100\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "d_shape = (len(d_obs),)\n",
    "xi_shape = (len_xi,)\n",
    "EVENT_SHAPE = (len(d_sim),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = (theta_shape[0], len_d, len_xi)\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 5 #3 # 10\n",
    "mlp_num_layers = 4 # 3 # 4\n",
    "hidden_size = 128 # 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 100\n",
    "early_stopping_memory = 10\n",
    "early_stopping_threshold = 5e-2\n",
    "\n",
    "training_steps = 500\n",
    "eval_frequency = 10\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 10000\n",
    "# TODO: put this function in training since d will be changing.\n",
    "X = sim_data_laplace(d_sim, num_samples, key)\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *d_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Can change the length of the deque for more/less leniency in measuring the loss\n",
    "loss_deque = deque(maxlen=early_stopping_memory)\n",
    "for step in range(training_steps):\n",
    "    params, opt_state, grads_d = update(\n",
    "        params, next(prng_seq), opt_state, next(train_ds)\n",
    "    )\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "    \n",
    "        loss_deque.append(val_loss)\n",
    "        avg_abs_diff = jnp.mean(abs(jnp.array(loss_deque) - sum(loss_deque)/len(loss_deque)))\n",
    "        if step > warmup_steps and avg_abs_diff < early_stopping_threshold:\n",
    "            break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and checking outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.expand_dims(X[:, X.shape[1] // 2], -1) == jnp.expand_dims(X[:, 2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[5.],\n",
       "             [5.],\n",
       "             [5.],\n",
       "             ...,\n",
       "             [5.],\n",
       "             [5.],\n",
       "             [5.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, -len_xi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# TODO: Make code that automatically slices these up.\n",
    "theta_test = jnp.expand_dims(X[:, X.shape[1] // 2], -1)\n",
    "# d_test = jnp.expand_dims(X[:, 4], -1)\n",
    "d_test = X[:, -len_x:-len_xi]\n",
    "xi_test = X[:, -len_xi:]\n",
    "# xi_test = jnp.expand_dims(X[:, 2], -1)\n",
    "# xi_test = jnp.ones((10000, 1)) * 3\n",
    "# xi_test = jnp.expand_dims(X[:, 1], -1)\n",
    "\n",
    "\n",
    "samples = model_sample.apply(params, \n",
    "                    next(prng_seq),\n",
    "                    num_samples=len(theta_test),\n",
    "                    theta=theta_test,\n",
    "                    d=d_test,\n",
    "                    # d=d_obs,\n",
    "                    xi=xi_test)\n",
    "\n",
    "\n",
    "density_1 = gaussian_kde(samples[:, 0])\n",
    "density_2 = gaussian_kde(samples[:, 1])\n",
    "\n",
    "\n",
    "# Plot the density\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(jnp.min(samples), jnp.max(samples), 100)\n",
    "ax.plot(x, density_1(x), label='x1')\n",
    "ax.plot(x, density_2(x), label='x2')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular ACE\n",
    "Really just copy/paste of AF's implementation in `pyro`. Since the current `pyro` release doesn't have some of AF's code from SGBOED, have to copy/paste some of his custom functions here.\n",
    "\n",
    "Going to compare the lower/upper bounds of SGBOED on a simple linear regression task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "0\n",
      "eig tensor(2.2699)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "1\n",
      "eig tensor(1.3538)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "2\n",
      "eig tensor(1.0857)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "3\n",
      "eig tensor(-0.7965)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "4\n",
      "eig tensor(1.3518)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "5\n",
      "eig tensor(-0.1831)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "6\n",
      "eig tensor(0.8764)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "7\n",
      "eig tensor(1.7372)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "8\n",
      "eig tensor(2.0288)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "9\n",
      "eig tensor(2.0209)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "10\n",
      "eig tensor(1.4414)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "11\n",
      "eig tensor(1.3914)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "12\n",
      "eig tensor(-0.4504)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "13\n",
      "eig tensor(0.1036)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "14\n",
      "eig tensor(0.9313)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "15\n",
      "eig tensor(0.6978)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "16\n",
      "eig tensor(2.3113)\n",
      "tensor([-0.0066], grad_fn=<SelectBackward0>)\n",
      "17\n",
      "eig tensor(1.8593)\n",
      "tensor([-0.0061], grad_fn=<SelectBackward0>)\n",
      "18\n",
      "eig tensor(1.4697)\n",
      "tensor([-0.0055], grad_fn=<SelectBackward0>)\n",
      "19\n",
      "eig tensor(0.6198)\n",
      "tensor([-0.0049], grad_fn=<SelectBackward0>)\n",
      "20\n",
      "eig tensor(0.2874)\n",
      "tensor([-0.0044], grad_fn=<SelectBackward0>)\n",
      "21\n",
      "eig tensor(2.4385)\n",
      "tensor([-0.0039], grad_fn=<SelectBackward0>)\n",
      "22\n",
      "eig tensor(1.2628)\n",
      "tensor([-0.0034], grad_fn=<SelectBackward0>)\n",
      "23\n",
      "eig tensor(2.2122)\n",
      "tensor([-0.0029], grad_fn=<SelectBackward0>)\n",
      "24\n",
      "eig tensor(1.9329)\n",
      "tensor([-0.0025], grad_fn=<SelectBackward0>)\n",
      "25\n",
      "eig tensor(1.9115)\n",
      "tensor([-0.0021], grad_fn=<SelectBackward0>)\n",
      "26\n",
      "eig tensor(1.9007)\n",
      "tensor([-0.0015], grad_fn=<SelectBackward0>)\n",
      "27\n",
      "eig tensor(2.1512)\n",
      "tensor([-0.0009], grad_fn=<SelectBackward0>)\n",
      "28\n",
      "eig tensor(2.7386)\n",
      "tensor([-0.0004], grad_fn=<SelectBackward0>)\n",
      "29\n",
      "eig tensor(1.9571)\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>)\n",
      "30\n",
      "eig tensor(2.6530)\n",
      "tensor([0.0008], grad_fn=<SelectBackward0>)\n",
      "31\n",
      "eig tensor(2.4668)\n",
      "tensor([0.0014], grad_fn=<SelectBackward0>)\n",
      "32\n",
      "eig tensor(1.8890)\n",
      "tensor([0.0019], grad_fn=<SelectBackward0>)\n",
      "33\n",
      "eig tensor(2.7383)\n",
      "tensor([0.0022], grad_fn=<SelectBackward0>)\n",
      "34\n",
      "eig tensor(2.2506)\n",
      "tensor([0.0024], grad_fn=<SelectBackward0>)\n",
      "35\n",
      "eig tensor(2.1409)\n",
      "tensor([0.0026], grad_fn=<SelectBackward0>)\n",
      "36\n",
      "eig tensor(3.0385)\n",
      "tensor([0.0028], grad_fn=<SelectBackward0>)\n",
      "37\n",
      "eig tensor(2.5979)\n",
      "tensor([0.0030], grad_fn=<SelectBackward0>)\n",
      "38\n",
      "eig tensor(2.4799)\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "39\n",
      "eig tensor(2.6393)\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "40\n",
      "eig tensor(2.9287)\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "41\n",
      "eig tensor(2.4400)\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "42\n",
      "eig tensor(2.3881)\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "43\n",
      "eig tensor(2.4209)\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "44\n",
      "eig tensor(2.5852)\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "45\n",
      "eig tensor(2.5280)\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "46\n",
      "eig tensor(1.9943)\n",
      "tensor([0.0030], grad_fn=<SelectBackward0>)\n",
      "47\n",
      "eig tensor(2.1834)\n",
      "tensor([0.0030], grad_fn=<SelectBackward0>)\n",
      "48\n",
      "eig tensor(2.5161)\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "49\n",
      "eig tensor(1.8794)\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "50\n",
      "eig tensor(2.6235)\n",
      "tensor([0.0035], grad_fn=<SelectBackward0>)\n",
      "51\n",
      "eig tensor(2.4932)\n",
      "tensor([0.0038], grad_fn=<SelectBackward0>)\n",
      "52\n",
      "eig tensor(2.6778)\n",
      "tensor([0.0040], grad_fn=<SelectBackward0>)\n",
      "53\n",
      "eig tensor(2.1564)\n",
      "tensor([0.0042], grad_fn=<SelectBackward0>)\n",
      "54\n",
      "eig tensor(2.7448)\n",
      "tensor([0.0044], grad_fn=<SelectBackward0>)\n",
      "55\n",
      "eig tensor(2.4631)\n",
      "tensor([0.0046], grad_fn=<SelectBackward0>)\n",
      "56\n",
      "eig tensor(2.8219)\n",
      "tensor([0.0046], grad_fn=<SelectBackward0>)\n",
      "57\n",
      "eig tensor(2.1745)\n",
      "tensor([0.0045], grad_fn=<SelectBackward0>)\n",
      "58\n",
      "eig tensor(2.9364)\n",
      "tensor([0.0044], grad_fn=<SelectBackward0>)\n",
      "59\n",
      "eig tensor(2.5219)\n",
      "tensor([0.0043], grad_fn=<SelectBackward0>)\n",
      "60\n",
      "eig tensor(3.0210)\n",
      "tensor([0.0042], grad_fn=<SelectBackward0>)\n",
      "61\n",
      "eig tensor(2.9381)\n",
      "tensor([0.0041], grad_fn=<SelectBackward0>)\n",
      "62\n",
      "eig tensor(2.4062)\n",
      "tensor([0.0040], grad_fn=<SelectBackward0>)\n",
      "63\n",
      "eig tensor(2.7316)\n",
      "tensor([0.0040], grad_fn=<SelectBackward0>)\n",
      "64\n",
      "eig tensor(2.2108)\n",
      "tensor([0.0039], grad_fn=<SelectBackward0>)\n",
      "65\n",
      "eig tensor(2.6323)\n",
      "tensor([0.0038], grad_fn=<SelectBackward0>)\n",
      "66\n",
      "eig tensor(2.3714)\n",
      "tensor([0.0038], grad_fn=<SelectBackward0>)\n",
      "67\n",
      "eig tensor(2.7313)\n",
      "tensor([0.0037], grad_fn=<SelectBackward0>)\n",
      "68\n",
      "eig tensor(2.8404)\n",
      "tensor([0.0037], grad_fn=<SelectBackward0>)\n",
      "69\n",
      "eig tensor(2.2692)\n",
      "tensor([0.0036], grad_fn=<SelectBackward0>)\n",
      "70\n",
      "eig tensor(2.3601)\n",
      "tensor([0.0036], grad_fn=<SelectBackward0>)\n",
      "71\n",
      "eig tensor(2.1339)\n",
      "tensor([0.0036], grad_fn=<SelectBackward0>)\n",
      "72\n",
      "eig tensor(2.3693)\n",
      "tensor([0.0036], grad_fn=<SelectBackward0>)\n",
      "73\n",
      "eig tensor(2.5218)\n",
      "tensor([0.0035], grad_fn=<SelectBackward0>)\n",
      "74\n",
      "eig tensor(2.4095)\n",
      "tensor([0.0035], grad_fn=<SelectBackward0>)\n",
      "75\n",
      "eig tensor(2.6550)\n",
      "tensor([0.0035], grad_fn=<SelectBackward0>)\n",
      "76\n",
      "eig tensor(2.5843)\n",
      "tensor([0.0034], grad_fn=<SelectBackward0>)\n",
      "77\n",
      "eig tensor(3.0500)\n",
      "tensor([0.0034], grad_fn=<SelectBackward0>)\n",
      "78\n",
      "eig tensor(2.8967)\n",
      "tensor([0.0030], grad_fn=<SelectBackward0>)\n",
      "79\n",
      "eig tensor(2.8753)\n",
      "tensor([0.0028], grad_fn=<SelectBackward0>)\n",
      "80\n",
      "eig tensor(2.5199)\n",
      "tensor([0.0022], grad_fn=<SelectBackward0>)\n",
      "81\n",
      "eig tensor(2.9723)\n",
      "tensor([0.0018], grad_fn=<SelectBackward0>)\n",
      "82\n",
      "eig tensor(2.3973)\n",
      "tensor([0.0013], grad_fn=<SelectBackward0>)\n",
      "83\n",
      "eig tensor(3.5052)\n",
      "tensor([0.0009], grad_fn=<SelectBackward0>)\n",
      "84\n",
      "eig tensor(3.0829)\n",
      "tensor([0.0005], grad_fn=<SelectBackward0>)\n",
      "85\n",
      "eig tensor(3.1051)\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>)\n",
      "86\n",
      "eig tensor(3.7133)\n",
      "tensor([-7.4018e-05], grad_fn=<SelectBackward0>)\n",
      "87\n",
      "eig tensor(2.9560)\n",
      "tensor([-0.0010], grad_fn=<SelectBackward0>)\n",
      "88\n",
      "eig tensor(2.8322)\n",
      "tensor([-0.0019], grad_fn=<SelectBackward0>)\n",
      "89\n",
      "eig tensor(2.3829)\n",
      "tensor([-0.0026], grad_fn=<SelectBackward0>)\n",
      "90\n",
      "eig tensor(3.5666)\n",
      "tensor([-0.0034], grad_fn=<SelectBackward0>)\n",
      "91\n",
      "eig tensor(2.9882)\n",
      "tensor([-0.0040], grad_fn=<SelectBackward0>)\n",
      "92\n",
      "eig tensor(2.8684)\n",
      "tensor([-0.0046], grad_fn=<SelectBackward0>)\n",
      "93\n",
      "eig tensor(2.7576)\n",
      "tensor([-0.0051], grad_fn=<SelectBackward0>)\n",
      "94\n",
      "eig tensor(2.9008)\n",
      "tensor([-0.0056], grad_fn=<SelectBackward0>)\n",
      "95\n",
      "eig tensor(2.7886)\n",
      "tensor([-0.0060], grad_fn=<SelectBackward0>)\n",
      "96\n",
      "eig tensor(2.7624)\n",
      "tensor([-0.0064], grad_fn=<SelectBackward0>)\n",
      "97\n",
      "eig tensor(3.0106)\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "98\n",
      "eig tensor(2.9636)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "99\n",
      "eig tensor(3.1000)\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "100\n",
      "eig tensor(1.8559)\n",
      "tensor([-0.0076], grad_fn=<SelectBackward0>)\n",
      "101\n",
      "eig tensor(2.8431)\n",
      "tensor([-0.0079], grad_fn=<SelectBackward0>)\n",
      "102\n",
      "eig tensor(2.7093)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "103\n",
      "eig tensor(2.9526)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "104\n",
      "eig tensor(2.9919)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "105\n",
      "eig tensor(3.2389)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "106\n",
      "eig tensor(2.5965)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "107\n",
      "eig tensor(3.0923)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "108\n",
      "eig tensor(2.3435)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "109\n",
      "eig tensor(2.5613)\n",
      "tensor([-0.0093], grad_fn=<SelectBackward0>)\n",
      "110\n",
      "eig tensor(3.2890)\n",
      "tensor([-0.0094], grad_fn=<SelectBackward0>)\n",
      "111\n",
      "eig tensor(2.0018)\n",
      "tensor([-0.0094], grad_fn=<SelectBackward0>)\n",
      "112\n",
      "eig tensor(2.6557)\n",
      "tensor([-0.0095], grad_fn=<SelectBackward0>)\n",
      "113\n",
      "eig tensor(3.2228)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "114\n",
      "eig tensor(2.5649)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "115\n",
      "eig tensor(2.9954)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "116\n",
      "eig tensor(2.8821)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "117\n",
      "eig tensor(2.5248)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "118\n",
      "eig tensor(2.7606)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "119\n",
      "eig tensor(3.0253)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "120\n",
      "eig tensor(3.0845)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "121\n",
      "eig tensor(1.9567)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "122\n",
      "eig tensor(3.5613)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "123\n",
      "eig tensor(3.7269)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "124\n",
      "eig tensor(2.3912)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "125\n",
      "eig tensor(2.9403)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "126\n",
      "eig tensor(3.1286)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "127\n",
      "eig tensor(2.9060)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "128\n",
      "eig tensor(2.1788)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "129\n",
      "eig tensor(3.3944)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "130\n",
      "eig tensor(3.3249)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "131\n",
      "eig tensor(3.0462)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "132\n",
      "eig tensor(2.0952)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "133\n",
      "eig tensor(3.0286)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "134\n",
      "eig tensor(3.4656)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "135\n",
      "eig tensor(2.8746)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "136\n",
      "eig tensor(2.5940)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "137\n",
      "eig tensor(3.1724)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "138\n",
      "eig tensor(3.7248)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "139\n",
      "eig tensor(2.7237)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "140\n",
      "eig tensor(2.9168)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "141\n",
      "eig tensor(2.5784)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "142\n",
      "eig tensor(3.3708)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "143\n",
      "eig tensor(3.4063)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "144\n",
      "eig tensor(2.7827)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "145\n",
      "eig tensor(2.9726)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "146\n",
      "eig tensor(2.3603)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "147\n",
      "eig tensor(3.0436)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "148\n",
      "eig tensor(3.3989)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "149\n",
      "eig tensor(2.6643)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "150\n",
      "eig tensor(2.7248)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "151\n",
      "eig tensor(3.0270)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "152\n",
      "eig tensor(2.6241)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "153\n",
      "eig tensor(3.6486)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "154\n",
      "eig tensor(3.0882)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "155\n",
      "eig tensor(3.5226)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "156\n",
      "eig tensor(2.6946)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "157\n",
      "eig tensor(3.2243)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "158\n",
      "eig tensor(3.5253)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "159\n",
      "eig tensor(3.8104)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "160\n",
      "eig tensor(3.1051)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "161\n",
      "eig tensor(2.9290)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "162\n",
      "eig tensor(3.1150)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "163\n",
      "eig tensor(3.5002)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "164\n",
      "eig tensor(3.1210)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "165\n",
      "eig tensor(3.1390)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "166\n",
      "eig tensor(3.1953)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "167\n",
      "eig tensor(3.6158)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "168\n",
      "eig tensor(2.4745)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "169\n",
      "eig tensor(3.0942)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "170\n",
      "eig tensor(2.4319)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "171\n",
      "eig tensor(3.4400)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "172\n",
      "eig tensor(3.2619)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "173\n",
      "eig tensor(1.5573)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "174\n",
      "eig tensor(2.7736)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "175\n",
      "eig tensor(2.9254)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "176\n",
      "eig tensor(3.0353)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "177\n",
      "eig tensor(2.3331)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "178\n",
      "eig tensor(2.5136)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "179\n",
      "eig tensor(2.4429)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "180\n",
      "eig tensor(3.0579)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "181\n",
      "eig tensor(2.8077)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "182\n",
      "eig tensor(2.1670)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "183\n",
      "eig tensor(2.4760)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "184\n",
      "eig tensor(2.4025)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "185\n",
      "eig tensor(3.1577)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "186\n",
      "eig tensor(3.4835)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "187\n",
      "eig tensor(2.8135)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "188\n",
      "eig tensor(3.0285)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "189\n",
      "eig tensor(2.2568)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "190\n",
      "eig tensor(3.1784)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "191\n",
      "eig tensor(3.3238)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "192\n",
      "eig tensor(3.4659)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "193\n",
      "eig tensor(3.3157)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "194\n",
      "eig tensor(2.6289)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "195\n",
      "eig tensor(3.2662)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "196\n",
      "eig tensor(2.8978)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "197\n",
      "eig tensor(2.4889)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "198\n",
      "eig tensor(3.4292)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "199\n",
      "eig tensor(2.2367)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "200\n",
      "eig tensor(3.2262)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "201\n",
      "eig tensor(2.5075)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "202\n",
      "eig tensor(2.3691)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "203\n",
      "eig tensor(2.7909)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "204\n",
      "eig tensor(2.6000)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "205\n",
      "eig tensor(2.8905)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "206\n",
      "eig tensor(3.0457)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "207\n",
      "eig tensor(3.4596)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "208\n",
      "eig tensor(3.3457)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "209\n",
      "eig tensor(3.3955)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "210\n",
      "eig tensor(2.2233)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "211\n",
      "eig tensor(3.0667)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "212\n",
      "eig tensor(3.4976)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "213\n",
      "eig tensor(3.0361)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "214\n",
      "eig tensor(2.6534)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "215\n",
      "eig tensor(2.3739)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "216\n",
      "eig tensor(2.7176)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "217\n",
      "eig tensor(2.7229)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "218\n",
      "eig tensor(3.1144)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "219\n",
      "eig tensor(3.0150)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "220\n",
      "eig tensor(3.2073)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "221\n",
      "eig tensor(2.5819)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "222\n",
      "eig tensor(2.7952)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "223\n",
      "eig tensor(2.5344)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "224\n",
      "eig tensor(3.2632)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "225\n",
      "eig tensor(3.0228)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "226\n",
      "eig tensor(3.2953)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "227\n",
      "eig tensor(2.6617)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "228\n",
      "eig tensor(2.7308)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "229\n",
      "eig tensor(2.5702)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "230\n",
      "eig tensor(3.0610)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "231\n",
      "eig tensor(3.3880)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "232\n",
      "eig tensor(3.7232)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "233\n",
      "eig tensor(3.3736)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "234\n",
      "eig tensor(3.2378)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "235\n",
      "eig tensor(3.1074)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "236\n",
      "eig tensor(3.8308)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "237\n",
      "eig tensor(2.9874)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "238\n",
      "eig tensor(2.4320)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "239\n",
      "eig tensor(3.2754)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "240\n",
      "eig tensor(2.8340)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "241\n",
      "eig tensor(1.7470)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "242\n",
      "eig tensor(3.1408)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "243\n",
      "eig tensor(2.7962)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "244\n",
      "eig tensor(3.8382)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "245\n",
      "eig tensor(3.0400)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "246\n",
      "eig tensor(2.3341)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "247\n",
      "eig tensor(2.8338)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "248\n",
      "eig tensor(2.7190)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "249\n",
      "eig tensor(2.4389)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "250\n",
      "eig tensor(2.4230)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "251\n",
      "eig tensor(2.3990)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "252\n",
      "eig tensor(3.1386)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "253\n",
      "eig tensor(2.9328)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "254\n",
      "eig tensor(2.5369)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "255\n",
      "eig tensor(2.5145)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "256\n",
      "eig tensor(3.1486)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "257\n",
      "eig tensor(3.1843)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "258\n",
      "eig tensor(2.5411)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "259\n",
      "eig tensor(2.7255)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "260\n",
      "eig tensor(3.2658)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "261\n",
      "eig tensor(2.9862)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "262\n",
      "eig tensor(2.5359)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "263\n",
      "eig tensor(2.8373)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "264\n",
      "eig tensor(2.9977)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "265\n",
      "eig tensor(2.2297)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "266\n",
      "eig tensor(2.7986)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "267\n",
      "eig tensor(2.8051)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "268\n",
      "eig tensor(3.4839)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "269\n",
      "eig tensor(2.4259)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "270\n",
      "eig tensor(2.5514)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "271\n",
      "eig tensor(2.6932)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "272\n",
      "eig tensor(3.1509)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "273\n",
      "eig tensor(3.5254)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "274\n",
      "eig tensor(2.9069)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "275\n",
      "eig tensor(3.2540)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "276\n",
      "eig tensor(3.5092)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "277\n",
      "eig tensor(2.9604)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "278\n",
      "eig tensor(3.5012)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "279\n",
      "eig tensor(3.2869)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "280\n",
      "eig tensor(3.0086)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "281\n",
      "eig tensor(2.6494)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "282\n",
      "eig tensor(3.0796)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "283\n",
      "eig tensor(2.5979)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "284\n",
      "eig tensor(2.8208)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "285\n",
      "eig tensor(3.4609)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "286\n",
      "eig tensor(3.0304)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "287\n",
      "eig tensor(3.0941)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "288\n",
      "eig tensor(3.4125)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "289\n",
      "eig tensor(0.2991)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "290\n",
      "eig tensor(3.2893)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "291\n",
      "eig tensor(3.0174)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "292\n",
      "eig tensor(2.7554)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "293\n",
      "eig tensor(3.0508)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "294\n",
      "eig tensor(3.0325)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "295\n",
      "eig tensor(2.9140)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "296\n",
      "eig tensor(3.0885)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "297\n",
      "eig tensor(2.7682)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "298\n",
      "eig tensor(2.3047)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "299\n",
      "eig tensor(3.4065)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "300\n",
      "eig tensor(2.3274)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "301\n",
      "eig tensor(3.2476)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "302\n",
      "eig tensor(3.3563)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "303\n",
      "eig tensor(3.5553)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "304\n",
      "eig tensor(2.8158)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "305\n",
      "eig tensor(3.1501)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "306\n",
      "eig tensor(3.4243)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "307\n",
      "eig tensor(3.8464)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "308\n",
      "eig tensor(3.1700)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "309\n",
      "eig tensor(2.5694)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "310\n",
      "eig tensor(2.9627)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "311\n",
      "eig tensor(3.3022)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "312\n",
      "eig tensor(2.6088)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "313\n",
      "eig tensor(2.8127)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "314\n",
      "eig tensor(3.2851)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "315\n",
      "eig tensor(2.7302)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "316\n",
      "eig tensor(2.9214)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "317\n",
      "eig tensor(2.9872)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "318\n",
      "eig tensor(2.2663)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "319\n",
      "eig tensor(3.2645)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "320\n",
      "eig tensor(3.7101)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "321\n",
      "eig tensor(3.2214)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "322\n",
      "eig tensor(2.4896)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "323\n",
      "eig tensor(3.5265)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "324\n",
      "eig tensor(2.6397)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "325\n",
      "eig tensor(3.2275)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "326\n",
      "eig tensor(3.4085)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "327\n",
      "eig tensor(3.1053)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "328\n",
      "eig tensor(3.2713)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "329\n",
      "eig tensor(3.2852)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "330\n",
      "eig tensor(2.4933)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "331\n",
      "eig tensor(4.2791)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "332\n",
      "eig tensor(2.6207)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "333\n",
      "eig tensor(2.5591)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "334\n",
      "eig tensor(3.4023)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "335\n",
      "eig tensor(2.7203)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "336\n",
      "eig tensor(3.1471)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "337\n",
      "eig tensor(2.8317)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "338\n",
      "eig tensor(2.6453)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "339\n",
      "eig tensor(3.3394)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "340\n",
      "eig tensor(2.9882)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "341\n",
      "eig tensor(3.0156)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "342\n",
      "eig tensor(3.6048)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "343\n",
      "eig tensor(3.2268)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "344\n",
      "eig tensor(2.8865)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "345\n",
      "eig tensor(3.0832)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "346\n",
      "eig tensor(3.0688)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "347\n",
      "eig tensor(2.8071)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "348\n",
      "eig tensor(3.3329)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "349\n",
      "eig tensor(3.0409)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "350\n",
      "eig tensor(3.5115)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "351\n",
      "eig tensor(3.5755)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "352\n",
      "eig tensor(3.2306)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "353\n",
      "eig tensor(2.2760)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "354\n",
      "eig tensor(3.4480)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "355\n",
      "eig tensor(3.2288)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "356\n",
      "eig tensor(2.4012)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "357\n",
      "eig tensor(3.6703)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "358\n",
      "eig tensor(3.1157)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "359\n",
      "eig tensor(3.5245)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "360\n",
      "eig tensor(3.4470)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "361\n",
      "eig tensor(2.9641)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "362\n",
      "eig tensor(2.7436)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "363\n",
      "eig tensor(3.1585)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "364\n",
      "eig tensor(3.4895)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "365\n",
      "eig tensor(3.6017)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "366\n",
      "eig tensor(3.0740)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "367\n",
      "eig tensor(2.9702)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "368\n",
      "eig tensor(4.1875)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "369\n",
      "eig tensor(3.5456)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "370\n",
      "eig tensor(3.1050)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "371\n",
      "eig tensor(3.3820)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "372\n",
      "eig tensor(3.0477)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "373\n",
      "eig tensor(3.3235)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "374\n",
      "eig tensor(2.6490)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "375\n",
      "eig tensor(3.4626)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "376\n",
      "eig tensor(2.8772)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "377\n",
      "eig tensor(3.4614)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "378\n",
      "eig tensor(3.2150)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "379\n",
      "eig tensor(2.7923)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "380\n",
      "eig tensor(3.1647)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "381\n",
      "eig tensor(3.0879)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "382\n",
      "eig tensor(3.2479)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "383\n",
      "eig tensor(3.0882)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "384\n",
      "eig tensor(3.5846)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "385\n",
      "eig tensor(2.3705)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "386\n",
      "eig tensor(2.5317)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "387\n",
      "eig tensor(2.9823)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "388\n",
      "eig tensor(2.2536)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "389\n",
      "eig tensor(2.9078)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "390\n",
      "eig tensor(3.3033)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "391\n",
      "eig tensor(2.9372)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "392\n",
      "eig tensor(3.3918)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "393\n",
      "eig tensor(3.2989)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "394\n",
      "eig tensor(2.7040)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "395\n",
      "eig tensor(3.1074)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "396\n",
      "eig tensor(3.7815)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "397\n",
      "eig tensor(3.2110)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "398\n",
      "eig tensor(3.5901)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "399\n",
      "eig tensor(3.3937)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "400\n",
      "eig tensor(2.6304)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "401\n",
      "eig tensor(2.7162)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "402\n",
      "eig tensor(3.4560)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "403\n",
      "eig tensor(2.3362)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "404\n",
      "eig tensor(3.1448)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "405\n",
      "eig tensor(2.9088)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "406\n",
      "eig tensor(2.9998)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "407\n",
      "eig tensor(2.8744)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "408\n",
      "eig tensor(3.7476)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "409\n",
      "eig tensor(2.7990)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "410\n",
      "eig tensor(2.7298)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "411\n",
      "eig tensor(2.9339)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "412\n",
      "eig tensor(3.0255)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "413\n",
      "eig tensor(3.7927)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "414\n",
      "eig tensor(2.8306)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "415\n",
      "eig tensor(2.1328)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "416\n",
      "eig tensor(2.2904)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "417\n",
      "eig tensor(2.6848)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "418\n",
      "eig tensor(2.5805)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "419\n",
      "eig tensor(3.3696)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "420\n",
      "eig tensor(3.8529)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "421\n",
      "eig tensor(3.0389)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "422\n",
      "eig tensor(2.6473)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "423\n",
      "eig tensor(3.0817)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "424\n",
      "eig tensor(3.0331)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "425\n",
      "eig tensor(2.6988)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "426\n",
      "eig tensor(2.9361)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "427\n",
      "eig tensor(2.1412)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "428\n",
      "eig tensor(2.9052)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "429\n",
      "eig tensor(3.0046)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "430\n",
      "eig tensor(3.0123)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "431\n",
      "eig tensor(2.8833)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "432\n",
      "eig tensor(3.0698)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "433\n",
      "eig tensor(3.0493)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "434\n",
      "eig tensor(3.3484)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "435\n",
      "eig tensor(2.9379)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "436\n",
      "eig tensor(2.9268)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "437\n",
      "eig tensor(3.5120)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "438\n",
      "eig tensor(3.6652)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "439\n",
      "eig tensor(2.6689)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "440\n",
      "eig tensor(3.2792)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "441\n",
      "eig tensor(3.4409)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "442\n",
      "eig tensor(2.5440)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "443\n",
      "eig tensor(3.3234)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "444\n",
      "eig tensor(3.4040)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "445\n",
      "eig tensor(3.4671)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "446\n",
      "eig tensor(2.6742)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "447\n",
      "eig tensor(2.9144)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "448\n",
      "eig tensor(3.2102)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "449\n",
      "eig tensor(3.9352)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "450\n",
      "eig tensor(2.7262)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "451\n",
      "eig tensor(4.2161)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "452\n",
      "eig tensor(3.0158)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "453\n",
      "eig tensor(3.2537)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "454\n",
      "eig tensor(3.0026)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "455\n",
      "eig tensor(3.2717)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "456\n",
      "eig tensor(3.4830)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "457\n",
      "eig tensor(3.3779)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "458\n",
      "eig tensor(3.7795)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "459\n",
      "eig tensor(2.9728)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "460\n",
      "eig tensor(3.1504)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "461\n",
      "eig tensor(2.0891)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "462\n",
      "eig tensor(3.4430)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "463\n",
      "eig tensor(3.2713)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "464\n",
      "eig tensor(3.3716)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "465\n",
      "eig tensor(2.8340)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "466\n",
      "eig tensor(2.5966)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "467\n",
      "eig tensor(3.3914)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "468\n",
      "eig tensor(2.1609)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "469\n",
      "eig tensor(2.8333)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "470\n",
      "eig tensor(3.1170)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "471\n",
      "eig tensor(3.6621)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "472\n",
      "eig tensor(2.0271)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "473\n",
      "eig tensor(3.1353)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "474\n",
      "eig tensor(2.6406)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "475\n",
      "eig tensor(3.1349)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "476\n",
      "eig tensor(2.5601)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "477\n",
      "eig tensor(3.4620)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "478\n",
      "eig tensor(2.8390)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "479\n",
      "eig tensor(2.6622)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "480\n",
      "eig tensor(2.8731)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "481\n",
      "eig tensor(2.9159)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "482\n",
      "eig tensor(1.8688)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "483\n",
      "eig tensor(2.8804)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "484\n",
      "eig tensor(3.2156)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "485\n",
      "eig tensor(2.9708)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "486\n",
      "eig tensor(2.7639)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "487\n",
      "eig tensor(2.3138)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "488\n",
      "eig tensor(3.9425)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "489\n",
      "eig tensor(2.7083)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "490\n",
      "eig tensor(3.0697)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "491\n",
      "eig tensor(3.2353)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "492\n",
      "eig tensor(3.0102)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "493\n",
      "eig tensor(3.2316)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "494\n",
      "eig tensor(3.1690)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "495\n",
      "eig tensor(2.9993)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "496\n",
      "eig tensor(2.9455)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "497\n",
      "eig tensor(3.2077)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "498\n",
      "eig tensor(3.4835)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "499\n",
      "eig tensor(2.2651)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "500\n",
      "eig tensor(2.9256)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "501\n",
      "eig tensor(3.1221)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "502\n",
      "eig tensor(3.5242)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "503\n",
      "eig tensor(2.5765)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "504\n",
      "eig tensor(3.4605)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "505\n",
      "eig tensor(3.7290)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "506\n",
      "eig tensor(2.9409)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "507\n",
      "eig tensor(3.0723)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "508\n",
      "eig tensor(2.8246)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "509\n",
      "eig tensor(2.9855)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "510\n",
      "eig tensor(2.9758)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "511\n",
      "eig tensor(3.1246)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "512\n",
      "eig tensor(3.1470)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "513\n",
      "eig tensor(2.8126)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "514\n",
      "eig tensor(4.0287)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "515\n",
      "eig tensor(3.1545)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "516\n",
      "eig tensor(3.1697)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "517\n",
      "eig tensor(2.3026)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "518\n",
      "eig tensor(2.3789)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "519\n",
      "eig tensor(2.2478)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "520\n",
      "eig tensor(3.0920)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "521\n",
      "eig tensor(2.9269)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "522\n",
      "eig tensor(3.4784)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "523\n",
      "eig tensor(3.0167)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "524\n",
      "eig tensor(3.8078)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "525\n",
      "eig tensor(2.6574)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "526\n",
      "eig tensor(2.3241)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "527\n",
      "eig tensor(4.0467)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "528\n",
      "eig tensor(2.9625)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "529\n",
      "eig tensor(3.5625)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "530\n",
      "eig tensor(3.4892)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "531\n",
      "eig tensor(3.5206)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "532\n",
      "eig tensor(3.2916)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "533\n",
      "eig tensor(3.1711)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "534\n",
      "eig tensor(3.0473)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "535\n",
      "eig tensor(3.4273)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "536\n",
      "eig tensor(3.6071)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "537\n",
      "eig tensor(3.0261)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "538\n",
      "eig tensor(3.6213)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "539\n",
      "eig tensor(3.1021)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "540\n",
      "eig tensor(3.0933)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "541\n",
      "eig tensor(2.9587)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "542\n",
      "eig tensor(3.0107)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "543\n",
      "eig tensor(2.6809)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "544\n",
      "eig tensor(3.3512)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "545\n",
      "eig tensor(3.4099)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "546\n",
      "eig tensor(3.2120)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "547\n",
      "eig tensor(3.3626)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "548\n",
      "eig tensor(3.0642)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "549\n",
      "eig tensor(3.1561)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "550\n",
      "eig tensor(2.3519)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "551\n",
      "eig tensor(3.1689)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "552\n",
      "eig tensor(3.8160)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "553\n",
      "eig tensor(3.0937)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "554\n",
      "eig tensor(2.8551)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "555\n",
      "eig tensor(2.6295)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "556\n",
      "eig tensor(3.6243)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "557\n",
      "eig tensor(3.1619)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "558\n",
      "eig tensor(3.1012)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "559\n",
      "eig tensor(3.7324)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "560\n",
      "eig tensor(2.7794)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "561\n",
      "eig tensor(2.5178)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "562\n",
      "eig tensor(3.0444)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "563\n",
      "eig tensor(2.8029)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "564\n",
      "eig tensor(2.2300)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "565\n",
      "eig tensor(2.6873)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "566\n",
      "eig tensor(2.5658)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "567\n",
      "eig tensor(3.1091)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "568\n",
      "eig tensor(2.4130)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "569\n",
      "eig tensor(3.3545)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "570\n",
      "eig tensor(3.8414)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "571\n",
      "eig tensor(3.4641)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "572\n",
      "eig tensor(2.8125)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "573\n",
      "eig tensor(3.1334)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "574\n",
      "eig tensor(2.8729)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "575\n",
      "eig tensor(3.3873)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "576\n",
      "eig tensor(3.6245)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "577\n",
      "eig tensor(3.2245)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "578\n",
      "eig tensor(2.7211)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "579\n",
      "eig tensor(2.9104)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "580\n",
      "eig tensor(3.0676)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "581\n",
      "eig tensor(3.7484)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "582\n",
      "eig tensor(3.7140)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "583\n",
      "eig tensor(2.3728)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "584\n",
      "eig tensor(2.5060)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "585\n",
      "eig tensor(3.5006)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "586\n",
      "eig tensor(3.0304)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "587\n",
      "eig tensor(3.0506)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "588\n",
      "eig tensor(2.7936)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "589\n",
      "eig tensor(3.7144)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "590\n",
      "eig tensor(2.5934)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "591\n",
      "eig tensor(3.2522)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "592\n",
      "eig tensor(2.5241)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "593\n",
      "eig tensor(3.2677)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "594\n",
      "eig tensor(3.0261)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "595\n",
      "eig tensor(3.3527)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "596\n",
      "eig tensor(3.1532)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "597\n",
      "eig tensor(3.4335)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "598\n",
      "eig tensor(2.2639)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "599\n",
      "eig tensor(3.3975)\n",
      "tensor([-0.0095], grad_fn=<SelectBackward0>)\n",
      "600\n",
      "eig tensor(3.1775)\n",
      "tensor([-0.0095], grad_fn=<SelectBackward0>)\n",
      "601\n",
      "eig tensor(2.7149)\n",
      "tensor([-0.0094], grad_fn=<SelectBackward0>)\n",
      "602\n",
      "eig tensor(3.2185)\n",
      "tensor([-0.0093], grad_fn=<SelectBackward0>)\n",
      "603\n",
      "eig tensor(2.4254)\n",
      "tensor([-0.0093], grad_fn=<SelectBackward0>)\n",
      "604\n",
      "eig tensor(3.0692)\n",
      "tensor([-0.0092], grad_fn=<SelectBackward0>)\n",
      "605\n",
      "eig tensor(3.2080)\n",
      "tensor([-0.0092], grad_fn=<SelectBackward0>)\n",
      "606\n",
      "eig tensor(3.0630)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "607\n",
      "eig tensor(3.3462)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "608\n",
      "eig tensor(3.4373)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "609\n",
      "eig tensor(3.1997)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "610\n",
      "eig tensor(2.7663)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "611\n",
      "eig tensor(2.5085)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "612\n",
      "eig tensor(2.7979)\n",
      "tensor([-0.0088], grad_fn=<SelectBackward0>)\n",
      "613\n",
      "eig tensor(3.4695)\n",
      "tensor([-0.0088], grad_fn=<SelectBackward0>)\n",
      "614\n",
      "eig tensor(2.4679)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "615\n",
      "eig tensor(3.3630)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "616\n",
      "eig tensor(2.8820)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "617\n",
      "eig tensor(2.9347)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "618\n",
      "eig tensor(3.7929)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "619\n",
      "eig tensor(3.1195)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "620\n",
      "eig tensor(3.2871)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "621\n",
      "eig tensor(3.0480)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "622\n",
      "eig tensor(3.4456)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "623\n",
      "eig tensor(3.8010)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "624\n",
      "eig tensor(3.4061)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "625\n",
      "eig tensor(3.0897)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "626\n",
      "eig tensor(3.8129)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "627\n",
      "eig tensor(2.5748)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "628\n",
      "eig tensor(3.0095)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "629\n",
      "eig tensor(3.1006)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "630\n",
      "eig tensor(3.1598)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "631\n",
      "eig tensor(2.9519)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "632\n",
      "eig tensor(3.7715)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "633\n",
      "eig tensor(2.9128)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "634\n",
      "eig tensor(2.8506)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "635\n",
      "eig tensor(3.6128)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "636\n",
      "eig tensor(3.6080)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "637\n",
      "eig tensor(3.5493)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "638\n",
      "eig tensor(2.1438)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "639\n",
      "eig tensor(1.8785)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "640\n",
      "eig tensor(3.6318)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "641\n",
      "eig tensor(2.8873)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "642\n",
      "eig tensor(3.1848)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "643\n",
      "eig tensor(2.4414)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "644\n",
      "eig tensor(3.4906)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "645\n",
      "eig tensor(3.2076)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "646\n",
      "eig tensor(3.6758)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "647\n",
      "eig tensor(2.9202)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "648\n",
      "eig tensor(3.5989)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "649\n",
      "eig tensor(2.8909)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "650\n",
      "eig tensor(3.1645)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "651\n",
      "eig tensor(2.6621)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "652\n",
      "eig tensor(2.2108)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "653\n",
      "eig tensor(3.4172)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "654\n",
      "eig tensor(3.8852)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "655\n",
      "eig tensor(2.8947)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "656\n",
      "eig tensor(2.8851)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "657\n",
      "eig tensor(4.1301)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "658\n",
      "eig tensor(2.6340)\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "659\n",
      "eig tensor(3.2642)\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "660\n",
      "eig tensor(2.7039)\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "661\n",
      "eig tensor(3.7953)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "662\n",
      "eig tensor(3.5466)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "663\n",
      "eig tensor(2.8900)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "664\n",
      "eig tensor(2.7866)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "665\n",
      "eig tensor(3.0210)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "666\n",
      "eig tensor(2.6716)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "667\n",
      "eig tensor(2.6865)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "668\n",
      "eig tensor(3.2755)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "669\n",
      "eig tensor(3.3654)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "670\n",
      "eig tensor(2.7957)\n",
      "tensor([-0.0080], grad_fn=<SelectBackward0>)\n",
      "671\n",
      "eig tensor(3.2213)\n",
      "tensor([-0.0080], grad_fn=<SelectBackward0>)\n",
      "672\n",
      "eig tensor(3.0930)\n",
      "tensor([-0.0080], grad_fn=<SelectBackward0>)\n",
      "673\n",
      "eig tensor(3.6033)\n",
      "tensor([-0.0079], grad_fn=<SelectBackward0>)\n",
      "674\n",
      "eig tensor(3.7182)\n",
      "tensor([-0.0079], grad_fn=<SelectBackward0>)\n",
      "675\n",
      "eig tensor(3.2706)\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "676\n",
      "eig tensor(3.0226)\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "677\n",
      "eig tensor(2.1437)\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "678\n",
      "eig tensor(2.9555)\n",
      "tensor([-0.0077], grad_fn=<SelectBackward0>)\n",
      "679\n",
      "eig tensor(2.9827)\n",
      "tensor([-0.0076], grad_fn=<SelectBackward0>)\n",
      "680\n",
      "eig tensor(3.6264)\n",
      "tensor([-0.0075], grad_fn=<SelectBackward0>)\n",
      "681\n",
      "eig tensor(3.0834)\n",
      "tensor([-0.0075], grad_fn=<SelectBackward0>)\n",
      "682\n",
      "eig tensor(3.4779)\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "683\n",
      "eig tensor(3.1047)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "684\n",
      "eig tensor(3.1231)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "685\n",
      "eig tensor(3.5111)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "686\n",
      "eig tensor(3.1398)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "687\n",
      "eig tensor(3.3688)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "688\n",
      "eig tensor(2.7355)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "689\n",
      "eig tensor(3.5222)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "690\n",
      "eig tensor(2.5654)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "691\n",
      "eig tensor(3.1494)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "692\n",
      "eig tensor(2.8794)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "693\n",
      "eig tensor(3.3878)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "694\n",
      "eig tensor(3.0052)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "695\n",
      "eig tensor(3.0114)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "696\n",
      "eig tensor(2.9536)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "697\n",
      "eig tensor(3.3313)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "698\n",
      "eig tensor(3.4267)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "699\n",
      "eig tensor(3.3868)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "700\n",
      "eig tensor(3.7615)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "701\n",
      "eig tensor(2.1693)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "702\n",
      "eig tensor(3.6598)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "703\n",
      "eig tensor(3.1912)\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "704\n",
      "eig tensor(2.9876)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "705\n",
      "eig tensor(3.7997)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "706\n",
      "eig tensor(3.3977)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "707\n",
      "eig tensor(3.5488)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "708\n",
      "eig tensor(3.6049)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "709\n",
      "eig tensor(2.6132)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "710\n",
      "eig tensor(3.3605)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "711\n",
      "eig tensor(2.6615)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "712\n",
      "eig tensor(3.5296)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "713\n",
      "eig tensor(3.4837)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "714\n",
      "eig tensor(3.2913)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "715\n",
      "eig tensor(2.3549)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "716\n",
      "eig tensor(3.4103)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "717\n",
      "eig tensor(2.9043)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "718\n",
      "eig tensor(3.2749)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "719\n",
      "eig tensor(2.8592)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "720\n",
      "eig tensor(2.5026)\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "721\n",
      "eig tensor(2.8116)\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "722\n",
      "eig tensor(2.8585)\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "723\n",
      "eig tensor(3.4520)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "724\n",
      "eig tensor(3.6465)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "725\n",
      "eig tensor(3.5349)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "726\n",
      "eig tensor(2.5333)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "727\n",
      "eig tensor(2.9840)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "728\n",
      "eig tensor(3.1399)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "729\n",
      "eig tensor(2.5983)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "730\n",
      "eig tensor(3.4086)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "731\n",
      "eig tensor(3.0973)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "732\n",
      "eig tensor(3.1531)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "733\n",
      "eig tensor(3.0641)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "734\n",
      "eig tensor(4.1803)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "735\n",
      "eig tensor(3.2774)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "736\n",
      "eig tensor(3.3785)\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "737\n",
      "eig tensor(4.4984)\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "738\n",
      "eig tensor(2.9326)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "739\n",
      "eig tensor(3.9417)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "740\n",
      "eig tensor(3.2976)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "741\n",
      "eig tensor(3.2616)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "742\n",
      "eig tensor(2.3731)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "743\n",
      "eig tensor(2.8114)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "744\n",
      "eig tensor(2.8255)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "745\n",
      "eig tensor(3.3598)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "746\n",
      "eig tensor(3.6499)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "747\n",
      "eig tensor(3.3268)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "748\n",
      "eig tensor(3.4393)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "749\n",
      "eig tensor(3.9487)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "750\n",
      "eig tensor(3.2839)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "751\n",
      "eig tensor(3.7262)\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "752\n",
      "eig tensor(3.8975)\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "753\n",
      "eig tensor(3.6470)\n",
      "tensor([-0.0075], grad_fn=<SelectBackward0>)\n",
      "754\n",
      "eig tensor(3.6933)\n",
      "tensor([-0.0075], grad_fn=<SelectBackward0>)\n",
      "755\n",
      "eig tensor(3.6585)\n",
      "tensor([-0.0077], grad_fn=<SelectBackward0>)\n",
      "756\n",
      "eig tensor(3.5274)\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "757\n",
      "eig tensor(2.1687)\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "758\n",
      "eig tensor(3.2211)\n",
      "tensor([-0.0079], grad_fn=<SelectBackward0>)\n",
      "759\n",
      "eig tensor(3.5213)\n",
      "tensor([-0.0080], grad_fn=<SelectBackward0>)\n",
      "760\n",
      "eig tensor(3.2159)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "761\n",
      "eig tensor(3.3818)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "762\n",
      "eig tensor(2.3412)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "763\n",
      "eig tensor(3.6496)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "764\n",
      "eig tensor(2.8455)\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "765\n",
      "eig tensor(2.8729)\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "766\n",
      "eig tensor(3.5327)\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "767\n",
      "eig tensor(3.5750)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "768\n",
      "eig tensor(2.9121)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "769\n",
      "eig tensor(4.1926)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "770\n",
      "eig tensor(3.5458)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "771\n",
      "eig tensor(3.1372)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "772\n",
      "eig tensor(3.6197)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "773\n",
      "eig tensor(2.1039)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "774\n",
      "eig tensor(3.0387)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "775\n",
      "eig tensor(3.8620)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "776\n",
      "eig tensor(1.4693)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "777\n",
      "eig tensor(3.6795)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "778\n",
      "eig tensor(3.6424)\n",
      "tensor([-0.0088], grad_fn=<SelectBackward0>)\n",
      "779\n",
      "eig tensor(3.3392)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "780\n",
      "eig tensor(2.7919)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "781\n",
      "eig tensor(3.1069)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "782\n",
      "eig tensor(2.6310)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "783\n",
      "eig tensor(3.1959)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "784\n",
      "eig tensor(3.0489)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "785\n",
      "eig tensor(3.1848)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "786\n",
      "eig tensor(2.1107)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "787\n",
      "eig tensor(2.6076)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "788\n",
      "eig tensor(3.8463)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "789\n",
      "eig tensor(3.3356)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "790\n",
      "eig tensor(3.1051)\n",
      "tensor([-0.0092], grad_fn=<SelectBackward0>)\n",
      "791\n",
      "eig tensor(3.5367)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "792\n",
      "eig tensor(2.7336)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "793\n",
      "eig tensor(3.3793)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "794\n",
      "eig tensor(3.1960)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "795\n",
      "eig tensor(3.2117)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "796\n",
      "eig tensor(2.9142)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "797\n",
      "eig tensor(2.9629)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "798\n",
      "eig tensor(3.6937)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "799\n",
      "eig tensor(4.3074)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "800\n",
      "eig tensor(3.0156)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "801\n",
      "eig tensor(2.7498)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "802\n",
      "eig tensor(3.2983)\n",
      "tensor([-0.0088], grad_fn=<SelectBackward0>)\n",
      "803\n",
      "eig tensor(3.0658)\n",
      "tensor([-0.0088], grad_fn=<SelectBackward0>)\n",
      "804\n",
      "eig tensor(3.1637)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "805\n",
      "eig tensor(3.0208)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "806\n",
      "eig tensor(3.3236)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "807\n",
      "eig tensor(3.6910)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "808\n",
      "eig tensor(3.1559)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "809\n",
      "eig tensor(3.7246)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "810\n",
      "eig tensor(3.2113)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "811\n",
      "eig tensor(3.3835)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "812\n",
      "eig tensor(3.0831)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "813\n",
      "eig tensor(3.3143)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "814\n",
      "eig tensor(2.8731)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "815\n",
      "eig tensor(3.5016)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "816\n",
      "eig tensor(3.8013)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "817\n",
      "eig tensor(3.4056)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "818\n",
      "eig tensor(1.7257)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "819\n",
      "eig tensor(3.6071)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "820\n",
      "eig tensor(3.2422)\n",
      "tensor([-0.0094], grad_fn=<SelectBackward0>)\n",
      "821\n",
      "eig tensor(3.3598)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "822\n",
      "eig tensor(4.1470)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "823\n",
      "eig tensor(2.8379)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "824\n",
      "eig tensor(3.2869)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "825\n",
      "eig tensor(2.5519)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "826\n",
      "eig tensor(3.6834)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "827\n",
      "eig tensor(2.8224)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "828\n",
      "eig tensor(3.2942)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "829\n",
      "eig tensor(3.4578)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "830\n",
      "eig tensor(2.8061)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "831\n",
      "eig tensor(3.2035)\n",
      "tensor([-0.0113], grad_fn=<SelectBackward0>)\n",
      "832\n",
      "eig tensor(2.8439)\n",
      "tensor([-0.0114], grad_fn=<SelectBackward0>)\n",
      "833\n",
      "eig tensor(3.0277)\n",
      "tensor([-0.0115], grad_fn=<SelectBackward0>)\n",
      "834\n",
      "eig tensor(3.6638)\n",
      "tensor([-0.0116], grad_fn=<SelectBackward0>)\n",
      "835\n",
      "eig tensor(3.6511)\n",
      "tensor([-0.0116], grad_fn=<SelectBackward0>)\n",
      "836\n",
      "eig tensor(3.3671)\n",
      "tensor([-0.0117], grad_fn=<SelectBackward0>)\n",
      "837\n",
      "eig tensor(3.2326)\n",
      "tensor([-0.0118], grad_fn=<SelectBackward0>)\n",
      "838\n",
      "eig tensor(4.0987)\n",
      "tensor([-0.0119], grad_fn=<SelectBackward0>)\n",
      "839\n",
      "eig tensor(3.4869)\n",
      "tensor([-0.0120], grad_fn=<SelectBackward0>)\n",
      "840\n",
      "eig tensor(3.4267)\n",
      "tensor([-0.0121], grad_fn=<SelectBackward0>)\n",
      "841\n",
      "eig tensor(3.4793)\n",
      "tensor([-0.0122], grad_fn=<SelectBackward0>)\n",
      "842\n",
      "eig tensor(2.5709)\n",
      "tensor([-0.0122], grad_fn=<SelectBackward0>)\n",
      "843\n",
      "eig tensor(2.9152)\n",
      "tensor([-0.0123], grad_fn=<SelectBackward0>)\n",
      "844\n",
      "eig tensor(2.9830)\n",
      "tensor([-0.0124], grad_fn=<SelectBackward0>)\n",
      "845\n",
      "eig tensor(3.1019)\n",
      "tensor([-0.0125], grad_fn=<SelectBackward0>)\n",
      "846\n",
      "eig tensor(3.2156)\n",
      "tensor([-0.0125], grad_fn=<SelectBackward0>)\n",
      "847\n",
      "eig tensor(2.1980)\n",
      "tensor([-0.0126], grad_fn=<SelectBackward0>)\n",
      "848\n",
      "eig tensor(2.5889)\n",
      "tensor([-0.0127], grad_fn=<SelectBackward0>)\n",
      "849\n",
      "eig tensor(3.9051)\n",
      "tensor([-0.0128], grad_fn=<SelectBackward0>)\n",
      "850\n",
      "eig tensor(3.3960)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "851\n",
      "eig tensor(2.8532)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "852\n",
      "eig tensor(3.0878)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "853\n",
      "eig tensor(3.1398)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "854\n",
      "eig tensor(3.5157)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "855\n",
      "eig tensor(3.6568)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "856\n",
      "eig tensor(3.7624)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "857\n",
      "eig tensor(3.2696)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "858\n",
      "eig tensor(3.0377)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "859\n",
      "eig tensor(2.5776)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "860\n",
      "eig tensor(3.0105)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "861\n",
      "eig tensor(3.5547)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "862\n",
      "eig tensor(2.6745)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "863\n",
      "eig tensor(3.3154)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "864\n",
      "eig tensor(3.5091)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "865\n",
      "eig tensor(2.9537)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "866\n",
      "eig tensor(3.9864)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "867\n",
      "eig tensor(3.8490)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "868\n",
      "eig tensor(2.5604)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "869\n",
      "eig tensor(3.2654)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "870\n",
      "eig tensor(3.7234)\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "871\n",
      "eig tensor(3.3573)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "872\n",
      "eig tensor(3.6723)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "873\n",
      "eig tensor(-0.6096)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "874\n",
      "eig tensor(3.0953)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "875\n",
      "eig tensor(3.5757)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "876\n",
      "eig tensor(3.5197)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "877\n",
      "eig tensor(2.8950)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "878\n",
      "eig tensor(3.5935)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "879\n",
      "eig tensor(3.5813)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "880\n",
      "eig tensor(2.9305)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "881\n",
      "eig tensor(3.1363)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "882\n",
      "eig tensor(3.2385)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "883\n",
      "eig tensor(3.5988)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "884\n",
      "eig tensor(3.0072)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "885\n",
      "eig tensor(3.5284)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "886\n",
      "eig tensor(3.4030)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "887\n",
      "eig tensor(3.0974)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "888\n",
      "eig tensor(3.8754)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "889\n",
      "eig tensor(2.7972)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "890\n",
      "eig tensor(2.7556)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "891\n",
      "eig tensor(3.2557)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "892\n",
      "eig tensor(3.8634)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "893\n",
      "eig tensor(3.4404)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "894\n",
      "eig tensor(2.7764)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "895\n",
      "eig tensor(4.2035)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "896\n",
      "eig tensor(2.3184)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "897\n",
      "eig tensor(2.5927)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "898\n",
      "eig tensor(3.1038)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "899\n",
      "eig tensor(4.1943)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "900\n",
      "eig tensor(3.4634)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "901\n",
      "eig tensor(3.9285)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "902\n",
      "eig tensor(3.9666)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "903\n",
      "eig tensor(3.0169)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "904\n",
      "eig tensor(2.7511)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "905\n",
      "eig tensor(3.1263)\n",
      "tensor([-0.0128], grad_fn=<SelectBackward0>)\n",
      "906\n",
      "eig tensor(2.7172)\n",
      "tensor([-0.0128], grad_fn=<SelectBackward0>)\n",
      "907\n",
      "eig tensor(2.7922)\n",
      "tensor([-0.0128], grad_fn=<SelectBackward0>)\n",
      "908\n",
      "eig tensor(3.4618)\n",
      "tensor([-0.0128], grad_fn=<SelectBackward0>)\n",
      "909\n",
      "eig tensor(3.0864)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "910\n",
      "eig tensor(2.3928)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "911\n",
      "eig tensor(2.0236)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "912\n",
      "eig tensor(2.7418)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "913\n",
      "eig tensor(3.2857)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "914\n",
      "eig tensor(2.0690)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "915\n",
      "eig tensor(2.6772)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "916\n",
      "eig tensor(3.8055)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "917\n",
      "eig tensor(2.3823)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "918\n",
      "eig tensor(3.3410)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "919\n",
      "eig tensor(2.7262)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "920\n",
      "eig tensor(1.9796)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "921\n",
      "eig tensor(2.8703)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "922\n",
      "eig tensor(3.2766)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "923\n",
      "eig tensor(2.8688)\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "924\n",
      "eig tensor(3.0346)\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "925\n",
      "eig tensor(3.3677)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "926\n",
      "eig tensor(3.4089)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "927\n",
      "eig tensor(2.4139)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "928\n",
      "eig tensor(3.8964)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "929\n",
      "eig tensor(3.7827)\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "930\n",
      "eig tensor(3.3464)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "931\n",
      "eig tensor(3.1823)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "932\n",
      "eig tensor(2.4557)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "933\n",
      "eig tensor(3.2374)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "934\n",
      "eig tensor(3.4547)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "935\n",
      "eig tensor(3.8796)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "936\n",
      "eig tensor(2.9815)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "937\n",
      "eig tensor(2.5217)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "938\n",
      "eig tensor(3.3398)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "939\n",
      "eig tensor(2.9389)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "940\n",
      "eig tensor(3.4331)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "941\n",
      "eig tensor(3.3589)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "942\n",
      "eig tensor(3.3563)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "943\n",
      "eig tensor(2.2698)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "944\n",
      "eig tensor(2.9772)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "945\n",
      "eig tensor(2.3923)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "946\n",
      "eig tensor(3.1401)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "947\n",
      "eig tensor(3.4986)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "948\n",
      "eig tensor(2.9835)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "949\n",
      "eig tensor(3.2348)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "950\n",
      "eig tensor(3.2439)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "951\n",
      "eig tensor(3.2285)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "952\n",
      "eig tensor(3.0385)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "953\n",
      "eig tensor(4.1308)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "954\n",
      "eig tensor(-0.6163)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "955\n",
      "eig tensor(3.7382)\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "956\n",
      "eig tensor(2.9399)\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "957\n",
      "eig tensor(3.7171)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "958\n",
      "eig tensor(3.4776)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "959\n",
      "eig tensor(2.7739)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "960\n",
      "eig tensor(3.4671)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "961\n",
      "eig tensor(2.6502)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "962\n",
      "eig tensor(2.8837)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "963\n",
      "eig tensor(2.6771)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "964\n",
      "eig tensor(3.4861)\n",
      "tensor([-0.0137], grad_fn=<SelectBackward0>)\n",
      "965\n",
      "eig tensor(3.2180)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "966\n",
      "eig tensor(2.7374)\n",
      "tensor([-0.0138], grad_fn=<SelectBackward0>)\n",
      "967\n",
      "eig tensor(2.7606)\n",
      "tensor([-0.0138], grad_fn=<SelectBackward0>)\n",
      "968\n",
      "eig tensor(3.4119)\n",
      "tensor([-0.0139], grad_fn=<SelectBackward0>)\n",
      "969\n",
      "eig tensor(2.2185)\n",
      "tensor([-0.0140], grad_fn=<SelectBackward0>)\n",
      "970\n",
      "eig tensor(3.4805)\n",
      "tensor([-0.0141], grad_fn=<SelectBackward0>)\n",
      "971\n",
      "eig tensor(2.6491)\n",
      "tensor([-0.0142], grad_fn=<SelectBackward0>)\n",
      "972\n",
      "eig tensor(3.3576)\n",
      "tensor([-0.0142], grad_fn=<SelectBackward0>)\n",
      "973\n",
      "eig tensor(3.1149)\n",
      "tensor([-0.0143], grad_fn=<SelectBackward0>)\n",
      "974\n",
      "eig tensor(2.4953)\n",
      "tensor([-0.0143], grad_fn=<SelectBackward0>)\n",
      "975\n",
      "eig tensor(2.9388)\n",
      "tensor([-0.0143], grad_fn=<SelectBackward0>)\n",
      "976\n",
      "eig tensor(2.9799)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "977\n",
      "eig tensor(3.4531)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "978\n",
      "eig tensor(2.3984)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "979\n",
      "eig tensor(3.3540)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "980\n",
      "eig tensor(2.9796)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "981\n",
      "eig tensor(3.3712)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "982\n",
      "eig tensor(1.3842)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "983\n",
      "eig tensor(2.8387)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "984\n",
      "eig tensor(3.0243)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "985\n",
      "eig tensor(2.8448)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "986\n",
      "eig tensor(3.3795)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "987\n",
      "eig tensor(3.1466)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "988\n",
      "eig tensor(2.7256)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "989\n",
      "eig tensor(3.6202)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "990\n",
      "eig tensor(3.6903)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "991\n",
      "eig tensor(2.5201)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "992\n",
      "eig tensor(2.4579)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "993\n",
      "eig tensor(3.2914)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "994\n",
      "eig tensor(2.4246)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "995\n",
      "eig tensor(2.7213)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "996\n",
      "eig tensor(1.6986)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "997\n",
      "eig tensor(2.8520)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "998\n",
      "eig tensor(2.3008)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "999\n",
      "eig tensor(2.8630)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numbers\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro import poutine\n",
    "from pyro.contrib.util import rmv, lexpand\n",
    "\n",
    "def is_bad(a):\n",
    "    return torch_isnan(a) or torch_isinf(a)\n",
    "    \n",
    "\n",
    "def torch_isnan(x):\n",
    "    \"\"\"\n",
    "    A convenient function to check if a Tensor contains any nan; also works with numbers\n",
    "    \"\"\"\n",
    "    if isinstance(x, numbers.Number):\n",
    "        return x != x\n",
    "    return torch.isnan(x).any()\n",
    "\n",
    "\n",
    "def torch_isinf(x):\n",
    "    \"\"\"\n",
    "    A convenient function to check if a Tensor contains any +inf; also works with numbers\n",
    "    \"\"\"\n",
    "    if isinstance(x, numbers.Number):\n",
    "        return x == float('inf') or x == -float('inf')\n",
    "    return (x == float('inf')).any() or (x == -float('inf')).any()\n",
    "\n",
    "\n",
    "def _safe_mean_terms(terms):\n",
    "    mask = torch.isnan(terms) | (terms == float('-inf')) | (terms == float('inf'))\n",
    "    if terms.dtype is torch.float32:\n",
    "        nonnan = (~mask).sum(0).float()\n",
    "    elif terms.dtype is torch.float64:\n",
    "        nonnan = (~mask).sum(0).double()\n",
    "    terms[mask] = 0.\n",
    "    loss = terms.sum(0) / nonnan\n",
    "    agg_loss = loss.sum()\n",
    "    return agg_loss, loss\n",
    "\n",
    "\n",
    "def make_regression_model(w_loc, w_scale, sigma_scale, xi_init, observation_label=\"y\"):\n",
    "    def regression_model(design_prototype):\n",
    "        design = pyro.param(\"xi\", xi_init)\n",
    "        design = (design / design.norm(dim=-1, p=1, keepdim=True)).expand(design_prototype.shape)\n",
    "        if is_bad(design):\n",
    "            raise ArithmeticError(\"bad design, contains nan or inf\")\n",
    "        batch_shape = design.shape[:-2]\n",
    "        with pyro.plate_stack(\"plate_stack\", batch_shape):\n",
    "            # `w` is shape p, the prior on each component is independent\n",
    "            w = pyro.sample(\"w\", dist.Laplace(w_loc, w_scale).to_event(1))\n",
    "            # `sigma` is scalar\n",
    "            sigma = 1e-6 + pyro.sample(\"sigma\", dist.Exponential(sigma_scale)).unsqueeze(-1)\n",
    "            mean = rmv(design, w)\n",
    "            sd = sigma\n",
    "            y = pyro.sample(observation_label, dist.Normal(mean, sd).to_event(1))\n",
    "            return y\n",
    "\n",
    "    return regression_model\n",
    "\n",
    "\n",
    "class TensorLinear(nn.Module):\n",
    "    __constants__ = ['bias']\n",
    "\n",
    "    def __init__(self, *shape, bias=True):\n",
    "        super(TensorLinear, self).__init__()\n",
    "        self.in_features = shape[-2]\n",
    "        self.out_features = shape[-1]\n",
    "        self.batch_dims = shape[:-2]\n",
    "        self.weight = nn.Parameter(torch.Tensor(*self.batch_dims, self.out_features, self.in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(*self.batch_dims, self.out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return rmv(self.weight, input) + self.bias\n",
    "\n",
    "\n",
    "class PosteriorGuide(nn.Module):\n",
    "    def __init__(self, y_dim, w_dim, batching):\n",
    "        super(PosteriorGuide, self).__init__()\n",
    "        n_hidden = 64\n",
    "        self.linear1 = TensorLinear(*batching, y_dim, n_hidden)\n",
    "        self.linear2 = TensorLinear(*batching, n_hidden, n_hidden)\n",
    "        self.output_layer = TensorLinear(*batching, n_hidden, w_dim + 3)\n",
    "        self.covariance_shape = batching + (w_dim, w_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, y_dict, design_prototype, observation_labels, target_labels):\n",
    "        y = y_dict[\"y\"] - .5\n",
    "        x = self.relu(self.linear1(y))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        final = self.output_layer(x)\n",
    "\n",
    "        posterior_mean = final[..., :-3]\n",
    "        gamma_concentration = 1e-6 + self.softplus(final[..., -3])\n",
    "        gamma_rate = 1. + self.softplus(final[..., -2])\n",
    "        scale_tril_multiplier = 1e-6 + self.softplus(final[..., -1])\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        posterior_scale_tril = pyro.param(\n",
    "            \"posterior_scale_tril\",\n",
    "            torch.eye(posterior_mean.shape[-1], device=posterior_mean.device).expand(self.covariance_shape),\n",
    "            constraint=constraints.lower_cholesky\n",
    "        )\n",
    "        posterior_scale_tril = posterior_scale_tril * scale_tril_multiplier.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        batch_shape = design_prototype.shape[:-2]\n",
    "        with pyro.plate_stack(\"guide_plate_stack\", batch_shape):\n",
    "            pyro.sample(\"sigma\", dist.Gamma(gamma_concentration, gamma_rate))\n",
    "            pyro.sample(\"w\", dist.MultivariateNormal(posterior_mean, scale_tril=posterior_scale_tril))\n",
    "\n",
    "\n",
    "def _ace_eig_loss(model, guide, M, observation_labels, target_labels):\n",
    "    def loss_fn(design, num_particles, evaluation=False, **kwargs):\n",
    "        N = num_particles\n",
    "        expanded_design = lexpand(design, N)\n",
    "        \n",
    "        # Sample from p(y, theta | d)\n",
    "        trace = poutine.trace(model).get_trace(expanded_design)\n",
    "        y_dict_exp = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "        y_dict = {l: trace.nodes[l][\"value\"] for l in observation_labels}\n",
    "        theta_dict = {l: trace.nodes[l][\"value\"] for l in target_labels}\n",
    "\n",
    "        trace.compute_log_prob()\n",
    "        marginal_terms_cross = sum(trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "        marginal_terms_cross += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "        reguide_trace = poutine.trace(\n",
    "            pyro.condition(guide, data=theta_dict)).get_trace(\n",
    "            y_dict, expanded_design, observation_labels, target_labels)\n",
    "        # Here's a spot where you could update each model's parameters based on log_prob\n",
    "        reguide_trace.compute_log_prob()\n",
    "        marginal_terms_cross -= sum(reguide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "\n",
    "        # Sample M times from q(theta | y, d) for each y\n",
    "        reexpanded_design = lexpand(expanded_design, M)\n",
    "        guide_trace = poutine.trace(guide).get_trace(\n",
    "            y_dict, reexpanded_design, observation_labels, target_labels\n",
    "        )\n",
    "        theta_y_dict = {l: guide_trace.nodes[l][\"value\"] for l in target_labels}\n",
    "        theta_y_dict.update(y_dict_exp)\n",
    "        guide_trace.compute_log_prob()\n",
    "\n",
    "        # Re-run that through the model to compute the joint\n",
    "        model_trace = poutine.trace(\n",
    "            pyro.condition(model, data=theta_y_dict)).get_trace(reexpanded_design)\n",
    "        model_trace.compute_log_prob()\n",
    "\n",
    "        marginal_terms_proposal = -sum(guide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "        marginal_terms_proposal += sum(model_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "        marginal_terms_proposal += sum(model_trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "        marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "        terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "        # At eval time, add p(y | theta, d) terms\n",
    "        if evaluation:\n",
    "            terms += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "        return _safe_mean_terms(terms)\n",
    "\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "def neg_loss(loss):\n",
    "    def new_loss(*args, **kwargs):\n",
    "        return (-a for a in loss(*args, **kwargs))\n",
    "    return new_loss\n",
    "\n",
    "\n",
    "def opt_eig_loss_w_history(design, loss_fn, num_samples, num_steps, optim, time_budget):\n",
    "    params = None\n",
    "    est_loss_history = []\n",
    "    xi_history = []\n",
    "    baseline = 0.\n",
    "    t = time.time()\n",
    "    wall_times = []\n",
    "    for step in range(num_steps):\n",
    "        if params is not None:\n",
    "            pyro.infer.util.zero_grads(params)\n",
    "        with poutine.trace(param_only=True) as param_capture:\n",
    "            agg_loss, loss = loss_fn(design, num_samples, evaluation=True, control_variate=baseline)\n",
    "        baseline = -loss.detach()\n",
    "        params = set(site[\"value\"].unconstrained()\n",
    "                     for site in param_capture.trace.nodes.values())\n",
    "        if torch.isnan(agg_loss):\n",
    "            raise ArithmeticError(\"Encountered NaN loss in opt_eig_ape_loss\")\n",
    "        agg_loss.backward(retain_graph=True)\n",
    "        est_loss_history.append(loss.detach())\n",
    "        wall_times.append(time.time() - t)\n",
    "        optim(params)\n",
    "        optim.step()\n",
    "        print(pyro.param(\"xi\")[0, 0, ...])\n",
    "        print(step)\n",
    "        print('eig', baseline.squeeze())\n",
    "        if time_budget and time.time() - t > time_budget:\n",
    "            break\n",
    "\n",
    "    xi_history.append(pyro.param('xi').detach().clone())\n",
    "\n",
    "    est_loss_history = torch.stack(est_loss_history)\n",
    "    xi_history = torch.stack(xi_history)\n",
    "    wall_times = torch.tensor(wall_times)\n",
    "\n",
    "    return xi_history, est_loss_history, wall_times\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Start Pyro code implementation\n",
    "num_steps = 1000\n",
    "num_samples = 10\n",
    "time_budget = 1000\n",
    "seed = 420\n",
    "num_parallel = 1\n",
    "start_lr = 0.001\n",
    "end_lr = 0.001\n",
    "device = 'cpu'\n",
    "n = 20\n",
    "p = 1\n",
    "scale = 1.\n",
    "\n",
    "pyro.clear_param_store()\n",
    "if seed >= 0:\n",
    "    pyro.set_rng_seed(seed)\n",
    "else:\n",
    "    seed = int(torch.rand(tuple()) * 2 ** 30)\n",
    "    pyro.set_rng_seed(seed)\n",
    "\n",
    "xi_init = torch.randn((num_parallel, n, p), device=device)\n",
    "# Change the prior distribution here\n",
    "# prior params\n",
    "w_prior_loc = torch.zeros(p, device=device)\n",
    "w_prior_scale = scale * torch.ones(p, device=device)\n",
    "sigma_prior_scale = scale * torch.tensor(1., device=device)\n",
    "\n",
    "model_learn_xi = make_regression_model(\n",
    "    w_prior_loc, w_prior_scale, sigma_prior_scale, xi_init)\n",
    "\n",
    "contrastive_samples = num_samples\n",
    "\n",
    "# Fix correct loss\n",
    "targets = [\"w\", \"sigma\"]\n",
    "\n",
    "guide = PosteriorGuide(n, p, (num_parallel,)).to(device)\n",
    "eig_loss = _ace_eig_loss(model_learn_xi, guide, contrastive_samples, [\"y\"], targets)\n",
    "loss = neg_loss(eig_loss)\n",
    "\n",
    "gamma = (end_lr / start_lr) ** (1 / num_steps)\n",
    "scheduler = pyro.optim.ExponentialLR({'optimizer': torch.optim.Adam, 'optim_args': {'lr': start_lr},\n",
    "                                        'gamma': gamma})\n",
    "\n",
    "design_prototype = torch.zeros(num_parallel, n, p, device=device)  # this is annoying, code needs refactor\n",
    "\n",
    "xi_history, est_loss_history, wall_times = opt_eig_loss_w_history(\n",
    "    design_prototype, loss, num_samples=num_samples, num_steps=num_steps, optim=scheduler,\n",
    "    time_budget=time_budget)\n",
    "\n",
    "est_eig_history = -est_loss_history\n",
    "\n",
    "results = {'seed': seed, 'xi_history': xi_history.cpu(), 'est_eig_history': est_eig_history.cpu(),\n",
    "            'wall_times': wall_times.cpu()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['seed', 'xi_history', 'est_eig_history', 'wall_times'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFI-ACE\n",
    "Manually trained and stepped-through LFI-ACE model.\n",
    "\n",
    "1. Approximate likelihood using normalizing flow. Use a bunch of samples and their corresponding $\\theta$ values. Also, since the `pyro` version only uses one noise element, get rid of the other one that Kleinegesse used.\n",
    "2. Use approximated likelihood in ACE computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    ''' Linear regression simulator in jax using distrax.'''\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # # sample random gamma noise\n",
    "    # n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "    #     seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    # )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    # y = y + n_g + jnp.squeeze(n_n)\n",
    "    y = y + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "def sim_data(d: Array, num_samples: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack(\n",
    "        (y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d))))\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train. Just need a vector of the samples and thetas that produced them. Don't need a simulator or the prior quite yet.\n",
    "- How do I track the training curves in Jax/Haiku? Gave in and am using WANDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3cxaflp9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5140a3e5a0ee48f587869879c2e7c714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.163413"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td></td></tr><tr><td>step</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>3.5908</td></tr><tr><td>step</td><td>900</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">legendary-lion-5</strong> at: <a href=\"https://wandb.ai/vz_uci/lfiax_linRegression_ACE/runs/3cxaflp9\" target=\"_blank\">https://wandb.ai/vz_uci/lfiax_linRegression_ACE/runs/3cxaflp9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230117_205700-3cxaflp9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3cxaflp9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6419d463a696401294a56d6e0f9e40f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016690957499971168, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/vincent_zaballa/Development/lfiax/examples/wandb/run-20230117_211058-38sidq3h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vz_uci/lfiax_linRegression_ACE/runs/38sidq3h\" target=\"_blank\">young-snowball-6</a></strong> to <a href=\"https://wandb.ai/vz_uci/lfiax_linRegression_ACE\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/vz_uci/lfiax_linRegression_ACE\" target=\"_blank\">https://wandb.ai/vz_uci/lfiax_linRegression_ACE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/vz_uci/lfiax_linRegression_ACE/runs/38sidq3h\" target=\"_blank\">https://wandb.ai/vz_uci/lfiax_linRegression_ACE/runs/38sidq3h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:     0; Validation loss: 3.576\n",
      "STEP:   100; Validation loss: 3.659\n",
      "STEP:   200; Validation loss: 3.628\n",
      "STEP:   300; Validation loss: 3.580\n",
      "STEP:   400; Validation loss: 3.629\n",
      "STEP:   500; Validation loss: 3.598\n",
      "STEP:   600; Validation loss: 3.595\n",
      "STEP:   700; Validation loss: 3.628\n",
      "STEP:   800; Validation loss: 3.600\n",
      "STEP:   900; Validation loss: 3.600\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"lfiax_linRegression_ACE\", entity=\"vz_uci\")\n",
    "\n",
    "# TODO: Put this in hydra config file\n",
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "# d = jnp.array([-10.0, 0.0, 5.0, 10.0])\n",
    "# d = jnp.array([1., 2.])\n",
    "# d = jnp.array([1.])\n",
    "d_obs = jnp.array([1.0])\n",
    "# d_obs = jnp.array([])\n",
    "# d_prop = jrandom.uniform(key, shape=(1,), minval=-10.0, maxval=10.0)\n",
    "d_sim = jnp.concatenate((d_obs, d_prop), axis=0)\n",
    "len_x = len(d_sim)\n",
    "len_d = len(d_obs)\n",
    "len_xi = len(d_prop)\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "d_shape = (len(d_obs),)\n",
    "xi_shape = (len_xi,)\n",
    "EVENT_SHAPE = (len(d_sim),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = (theta_shape[0], len_d, len_xi)\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 10\n",
    "mlp_num_layers = 4\n",
    "hidden_size = 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "training_steps = 1000\n",
    "eval_frequency = 100\n",
    "\n",
    "wandb.config = {\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": training_steps,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"flow_num_layers\": flow_num_layers,\n",
    "  \"mlp_num_layers\": mlp_num_layers,\n",
    "  \"hidden_size\": hidden_size,\n",
    "  \"num_bins\": num_bins,\n",
    "  \"num_samples\": num_samples,\n",
    "  \"optimizer\": \"adam\",\n",
    "  \"xi\": d_prop,\n",
    "  \"x\": d_sim\n",
    "}\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 10000\n",
    "# TODO: put this function in training since d will be changing.\n",
    "X = sim_data(d_sim, num_samples, key)\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *d_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "for step in range(training_steps):\n",
    "    params, opt_state, grads_d = update(\n",
    "        params, next(prng_seq), opt_state, next(train_ds)\n",
    "    )\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "        \n",
    "        wandb.log({\"loss\": val_loss, \"step\": step})\n",
    "\n",
    "        # Optional\n",
    "        # wandb.watch(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.05977392], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_prop.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the sampling/log-prob stuff work. Now to actually implment LFI ACE. I've got the flow, now just going to do some of the ACE computations manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_lexpand(A, *dimensions):\n",
    "    \"\"\"Expand tensor, adding new dimensions on left.\"\"\"\n",
    "    if jnp.isscalar(A):\n",
    "        A = A * jnp.ones(dimensions)\n",
    "        return A\n",
    "    shape = tuple(dimensions) + A.shape\n",
    "    A = A[jnp.newaxis, ...]\n",
    "    A = jnp.broadcast_to(A, shape)\n",
    "    return A\n",
    "\n",
    "\n",
    "# Walking through and commenting the code\n",
    "def lfi_ace_eig_loss(model:NormalizingFlow, guide:NormalizingFlow, M, observation_labels, target_labels):\n",
    "    def loss_fn(design, num_particles, evaluation=False, **kwargs):\n",
    "        N = num_particles\n",
    "        # Expand to the number of parallel designs being evaluated\n",
    "        expanded_design = lexpand(design, N)\n",
    "        \n",
    "        # TODO: make a for loop over the proposed models here.\n",
    "        for model in models:\n",
    "            # TODO: would be cool to make sampling from the model's prior a method\n",
    "             # Hmm I need N copies for parallelization. I can figure that out later, though.\n",
    "            model_thetas_0 = model.prior_distribution.sample(num_samples)\n",
    "            y_0, log_prob_y_0 = model.sample_with_log_prob(model_thetas_0, expanded_design)\n",
    "            # Get a dictionary of the expanded y and theta values, just make M copies of y values\n",
    "            # TODO: think of a better way to add these into expanded dictionary values...\n",
    "            y_dict_exp = lexpand(y, M)\n",
    "\n",
    "            # This is essentially the p(y|theta)p(theta)\n",
    "            # Ohh, each model will have two ratios, it's current prob and its prior one\n",
    "            marginal_terms_cross = sum(model.prior_distribution.log_prob(model_thetas_0))\n",
    "            marginal_terms_cross += sum(log_prob_y_0)\n",
    "\n",
    "        # Pray that jax can handle this parallelization - vmap and pmap to the rescue!...?\n",
    "        for _ in range(m):\n",
    "            for model in models:\n",
    "                # Sample from q(theta | y, d) using the guide normalizing flow\n",
    "                theta, log_prob = guide.sample_with_log_prob(y, expanded_design, observation_labels, target_labels)\n",
    "                theta_y_dict = {l: theta[l] for l in target_labels}\n",
    "                theta_y_dict.update(y_dict_exp)\n",
    "\n",
    "                marginal_terms_proposal = -sum(log_prob[l] for l in target_labels)\n",
    "                marginal_terms_proposal += sum(log_prob[l] for l in target_labels)\n",
    "                marginal_terms_proposal += sum(log_prob[l] for l in observation_labels)\n",
    "\n",
    "                marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "                terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "                # At eval time, add p(y | theta, d) terms\n",
    "                if evaluation:\n",
    "                    terms += sum(log_prob[l] for l in observation_labels)\n",
    "                return _safe_mean_terms(terms)\n",
    "\n",
    "        # Sample from q(theta | y, d) using the guide normalizing flow\n",
    "        theta, log_prob = guide.sample_with_log_prob(y, expanded_design, observation_labels, target_labels)\n",
    "        theta_y_dict = {l: theta[l] for l in target_labels}\n",
    "        theta_y_dict.update(y_dict_exp)\n",
    "\n",
    "        marginal_terms_proposal = -sum(log_prob[l] for l in target_labels)\n",
    "        marginal_terms_proposal += sum(log_prob[l] for l in target_labels)\n",
    "        marginal_terms_proposal += sum(log_prob[l] for l in observation_labels)\n",
    "\n",
    "        marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "        terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "        # At eval time, add p(y | theta, d) terms\n",
    "        if evaluation:\n",
    "            terms += sum(log_prob[l] for l in observation_labels)\n",
    "        return _safe_mean_terms(terms)\n",
    "\n",
    "    return loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "p = 20\n",
    "n = 20\n",
    "N = 2 # num_parallel\n",
    "M = 2\n",
    "design = torch.zeros([10, 20, 20])\n",
    "expanded_design = lexpand(design, N)\n",
    "\n",
    "w_prior_loc = torch.zeros(p, device=device)\n",
    "w_prior_scale = scale * torch.ones(p, device=device)\n",
    "sigma_prior_scale = scale * torch.tensor(1., device=device)\n",
    "\n",
    "model_learn_xi = make_regression_model(\n",
    "    w_prior_loc, w_prior_scale, sigma_prior_scale, xi_init)\n",
    "\n",
    "model = model_learn_xi\n",
    "guide = PosteriorGuide(n, p, (N,)).to(device)\n",
    "observation_labels = [\"y\"]\n",
    "target_labels = ['w', 'sigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = poutine.trace(model).get_trace(expanded_design)\n",
    "y_dict_exp = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "y_dict = {l: trace.nodes[l][\"value\"] for l in observation_labels}\n",
    "theta_dict = {l: trace.nodes[l][\"value\"] for l in target_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 20])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dict['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 20])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_dict['w'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_dict['sigma'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 10]), torch.Size([2, 10])]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[trace.nodes[l][\"log_prob\"].shape for l in target_labels]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what's throwing me off. Determining the `log_prob` of the `w` vector compresses it for some reason... Oh, wait, it's the probability of each *vector*. That's why it's compressed to the shape of the `y` and `sigma` shapes. Yea, that makes a lot more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.compute_log_prob()\n",
    "# This is taking the log_prob of a vector from a distribution. I can either use a distribution that \n",
    "# is in a vector format or I think it should be fine to add two separate ones together...\n",
    "marginal_terms_cross = sum(trace.nodes[l][\"log_prob\"] for l in target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_terms_cross += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-48.1695, -70.6690, -58.5410, -57.8346, -82.7463, -79.5401, -34.3802,\n",
       "         -75.5568, -62.8853, -90.3731],\n",
       "        [-68.9690, -46.9697, -79.1669, -81.4540, -75.3442, -28.8084, -77.4504,\n",
       "         -67.5315,   2.8248, -70.4339]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginal_terms_cross"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part would be the denominator of LFI-ACE with the sum of all of the models' log_probs. The numerator might actually be the sum of all the log_probs. The denominator would be the weighted sum... Yea, that makes more sense....\n",
    "\n",
    "Part of the loss could then just be adding on the loss of the prediction of each normalizing flow. That makes one scalar loss. Although, there's an EIG for each of the 'y' outputs, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1\nTrace Shapes:\n Param Sites:\nSample Sites:",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [12], line 108\u001b[0m, in \u001b[0;36mPosteriorGuide.forward\u001b[0;34m(self, y_dict, design_prototype, observation_labels, target_labels)\u001b[0m\n\u001b[1;32m    107\u001b[0m y \u001b[39m=\u001b[39m y_dict[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m.5\u001b[39m\n\u001b[0;32m--> 108\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(y))\n\u001b[1;32m    109\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [12], line 92\u001b[0m, in \u001b[0;36mTensorLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m rmv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39minput\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/contrib/util.py:44\u001b[0m, in \u001b[0;36mrmv\u001b[0;34m(A, b)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m\"\"\"Tensorized matrix vector multiplication of rightmost dimensions.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(A, b\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reguide_trace \u001b[39m=\u001b[39m poutine\u001b[39m.\u001b[39;49mtrace(\n\u001b[1;32m      2\u001b[0m     pyro\u001b[39m.\u001b[39;49mcondition(guide, data\u001b[39m=\u001b[39;49mtheta_dict))\u001b[39m.\u001b[39;49mget_trace(\n\u001b[1;32m      3\u001b[0m     y_dict, expanded_design, observation_labels, target_labels)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Here's a spot where you could update each model's parameters based on log_prob\u001b[39;00m\n\u001b[1;32m      5\u001b[0m reguide_trace\u001b[39m.\u001b[39mcompute_log_prob()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:198\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_trace\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    191\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m    :returns: data structure\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:180\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         exc \u001b[39m=\u001b[39m exc_type(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(exc_value, shapes))\n\u001b[1;32m    179\u001b[0m         exc \u001b[39m=\u001b[39m exc\u001b[39m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39madd_node(\n\u001b[1;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_RETURN\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_RETURN\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, value\u001b[39m=\u001b[39mret\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39madd_node(\n\u001b[1;32m    171\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m_INPUT\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_INPUT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m     exc_type, exc_value, traceback \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_context_wrap\u001b[39m(context, fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [12], line 108\u001b[0m, in \u001b[0;36mPosteriorGuide.forward\u001b[0;34m(self, y_dict, design_prototype, observation_labels, target_labels)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, y_dict, design_prototype, observation_labels, target_labels):\n\u001b[1;32m    107\u001b[0m     y \u001b[39m=\u001b[39m y_dict[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m.5\u001b[39m\n\u001b[0;32m--> 108\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(y))\n\u001b[1;32m    109\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(x))\n\u001b[1;32m    110\u001b[0m     final \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [12], line 92\u001b[0m, in \u001b[0;36mTensorLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m rmv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39minput\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/contrib/util.py:44\u001b[0m, in \u001b[0;36mrmv\u001b[0;34m(A, b)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrmv\u001b[39m(A, b):\n\u001b[1;32m     43\u001b[0m     \u001b[39m\"\"\"Tensorized matrix vector multiplication of rightmost dimensions.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(A, b\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1\nTrace Shapes:\n Param Sites:\nSample Sites:"
     ]
    }
   ],
   "source": [
    "reguide_trace = poutine.trace(\n",
    "    pyro.condition(guide, data=theta_dict)).get_trace(\n",
    "    y_dict, expanded_design, observation_labels, target_labels)\n",
    "# Here's a spot where you could update each model's parameters based on log_prob\n",
    "reguide_trace.compute_log_prob()\n",
    "marginal_terms_cross -= sum(reguide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = poutine.trace(model).get_trace(expanded_design)\n",
    "y_dict_exp = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "y_dict = {l: trace.nodes[l][\"value\"] for l in observation_labels}\n",
    "theta_dict = {l: trace.nodes[l][\"value\"] for l in target_labels}\n",
    "\n",
    "trace.compute_log_prob()\n",
    "marginal_terms_cross = sum(trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_cross += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "reguide_trace = poutine.trace(\n",
    "    pyro.condition(guide, data=theta_dict)).get_trace(\n",
    "    y_dict, expanded_design, observation_labels, target_labels)\n",
    "# Here's a spot where you could update each model's parameters based on log_prob\n",
    "reguide_trace.compute_log_prob()\n",
    "marginal_terms_cross -= sum(reguide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "\n",
    "# Sample M times from q(theta | y, d) for each y\n",
    "reexpanded_design = lexpand(expanded_design, M)\n",
    "guide_trace = poutine.trace(guide).get_trace(\n",
    "    y_dict, reexpanded_design, observation_labels, target_labels\n",
    ")\n",
    "theta_y_dict = {l: guide_trace.nodes[l][\"value\"] for l in target_labels}\n",
    "theta_y_dict.update(y_dict_exp)\n",
    "guide_trace.compute_log_prob()\n",
    "\n",
    "# Re-run that through the model to compute the joint\n",
    "model_trace = poutine.trace(\n",
    "pyro.condition(model, data=theta_y_dict)).get_trace(reexpanded_design)\n",
    "model_trace.compute_log_prob()\n",
    "\n",
    "marginal_terms_proposal = -sum(guide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_proposal += sum(model_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_proposal += sum(model_trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "# At eval time, add p(y | theta, d) terms\n",
    "# if evaluation:\n",
    "terms += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "# breakpoint()\n",
    "_safe_mean_terms(terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy_d = jnp.array([3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m jnp\u001b[39m.\u001b[39;49misscalar(testy_d)\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "jnp.isscalar(testy_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/api.py:1666\u001b[0m, in \u001b[0;36m_mapped_axis_size.<locals>._get_axis_size\u001b[0;34m(name, shape, axis)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1666\u001b[0m   \u001b[39mreturn\u001b[39;00m shape[axis]\n\u001b[1;32m   1667\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [119], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m vmap_fun \u001b[39m=\u001b[39m vmap(\u001b[39mlambda\u001b[39;00m f, x: jnp\u001b[39m.\u001b[39mvectorize(f)(x))\n\u001b[1;32m     14\u001b[0m \u001b[39m# apply the vmap function to the list of functions and the range of inputs\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m result \u001b[39m=\u001b[39m pmap(vmap_fun)(functions, \u001b[39mrange\u001b[39;49m(\u001b[39m10\u001b[39;49m))\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/api.py:1669\u001b[0m, in \u001b[0;36m_mapped_axis_size.<locals>._get_axis_size\u001b[0;34m(name, shape, axis)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1668\u001b[0m   min_rank \u001b[39m=\u001b[39m axis \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m axis \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39maxis\n\u001b[0;32m-> 1669\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1670\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m was requested to map its argument along axis \u001b[39m\u001b[39m{\u001b[39;00maxis\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1671\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhich implies that its rank should be at least \u001b[39m\u001b[39m{\u001b[39;00mmin_rank\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1672\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is only \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m (its shape is \u001b[39m\u001b[39m{\u001b[39;00mshape\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: pmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())"
     ]
    }
   ],
   "source": [
    "from jax import pmap, vmap\n",
    "\n",
    "def compute1(x):\n",
    "  return x ** 2\n",
    "\n",
    "def compute2(x):\n",
    "  return x ** 3\n",
    "\n",
    "functions = [compute1, compute2]\n",
    "\n",
    "# create a vmap function that takes a function and an array as input\n",
    "vmap_fun = vmap(lambda f, x: jnp.vectorize(f)(x))\n",
    "\n",
    "# apply the vmap function to the list of functions and the range of inputs\n",
    "result = pmap(vmap_fun)(functions, range(10))\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdm3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bac022e209f3f8f811ac02bf6d6b971751e1ab224096f1893a92a620959b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
