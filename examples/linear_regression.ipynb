{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import distrax\n",
    "import haiku as hk\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from lfiax.flows.nsf import make_nsf\n",
    "\n",
    "from typing import (\n",
    "    Any,\n",
    "    Iterator,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "OptState = Any\n",
    "\n",
    "\n",
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helper functions to simulate data\n",
    "# ----------------------------------------\n",
    "def load_dataset(split: tfds.Split, batch_size: int) -> Iterator[Batch]:\n",
    "    ds = split\n",
    "    ds = ds.shuffle(buffer_size=10 * batch_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=1000)\n",
    "    ds = ds.repeat()\n",
    "    return iter(tfds.as_numpy(ds))\n",
    "\n",
    "\n",
    "def sim_data(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack((y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d)))))\n",
    "\n",
    "\n",
    "def prepare_data(batch: Batch, prng_key: Optional[PRNGKey] = None) -> Array:\n",
    "    # Batch is [y, thetas, d]\n",
    "    data = batch.astype(np.float32)\n",
    "    # Handling the scalar case\n",
    "    if data.shape[1] <= 3:\n",
    "        y = jnp.expand_dims(data[:, :-2], -1)\n",
    "    # Use known length of x to split up the cond_data\n",
    "    # data_shape = data.shape\n",
    "    # start = [0, 0]\n",
    "    # stop = [data_shape[0], len_x]\n",
    "    # y = lax.dynamic_slice(data, start, stop)\n",
    "    y = data[:, :len_x]\n",
    "    cond_data = data[:, len_x:]\n",
    "    theta = cond_data[:, :-len_x]\n",
    "    x = cond_data[:, -len_x:len_xi]\n",
    "    xi = cond_data[:, -len_xi:]\n",
    "    # return x, cond_data\n",
    "    return y, theta, x, xi\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Haiku transform functions for training and evaluation\n",
    "# ----------------------------\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob(data: Array, cond_data: Array) -> Array:\n",
    "    # Get batch\n",
    "    shift = data.mean(axis=0)\n",
    "    scale = data.std(axis=0) + 1e-14\n",
    "    \n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "        standardize_x=True,\n",
    "        standardize_z=True,\n",
    "        use_resnet=True,\n",
    "        event_dim=EVENT_DIM,\n",
    "        shift=shift,\n",
    "        scale=scale,\n",
    "    )\n",
    "    return model.log_prob(data, cond_data)\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def model_sample(key: PRNGKey, num_samples: int, cond_data: Array) -> Array:\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "    )\n",
    "    z = jnp.repeat(cond_data, num_samples, axis=0)\n",
    "    z = jnp.expand_dims(z, -1)\n",
    "    return model._sample_n(key=key, n=[num_samples], z=z)\n",
    "\n",
    "\n",
    "def loss_fn(params: hk.Params, prng_key: PRNGKey, y: Array, theta: Array, x: Array, xi: Array) -> Array:\n",
    "    # x, cond_data = prepare_data(batch, prng_key)\n",
    "    # I wonder if this will work...\n",
    "    cond_data = jnp.column_stack((theta, x, xi))\n",
    "    # Loss is average negative log likelihood.\n",
    "    loss = -jnp.mean(log_prob.apply(params, y, cond_data))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_fn(params: hk.Params, batch: Batch) -> Array:\n",
    "    y, theta, x, xi = prepare_data(batch)\n",
    "    # loss = -jnp.mean(log_prob.apply(params, x, cond_data))\n",
    "    loss = -jnp.mean(log_prob.apply(params, y, (theta, x, xi)))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    # x, cond_data = prepare_data(batch, prng_key)\n",
    "    y, theta, x, xi = prepare_data(batch)\n",
    "    grads = jax.grad(loss_fn)(params, prng_key, y, theta, x, xi)\n",
    "    grads_d = jax.grad(loss_fn, argnums=5)(params, prng_key, y, theta, x, xi)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "key = jrandom.PRNGKey(seed)\n",
    "d_prop = jrandom.uniform(key, shape=(1,), minval=-10., maxval=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_obs = jnp.array([1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.concatenate((d_obs, d_prop), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'conditioner_module/mlp/~/linear_0/w' with retrieved shape (6, 500) does not match shape=[5, 500] dtype=dtype('float32')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [141], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m opt_state \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39minit(params)\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(training_steps):\n\u001b[0;32m---> 60\u001b[0m     params, opt_state \u001b[39m=\u001b[39m update(params, \u001b[39mnext\u001b[39;49m(prng_seq), opt_state, \u001b[39mnext\u001b[39;49m(train_ds))\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m eval_frequency \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     63\u001b[0m         val_loss \u001b[39m=\u001b[39m eval_fn(params, \u001b[39mnext\u001b[39m(valid_ds))\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [140], line 178\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(params, prng_key, opt_state, batch)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m# x, cond_data = prepare_data(batch, prng_key)\u001b[39;00m\n\u001b[1;32m    177\u001b[0m y, theta, x, xi \u001b[39m=\u001b[39m prepare_data(batch)\n\u001b[0;32m--> 178\u001b[0m grads \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mgrad(loss_fn)(params, prng_key, y, theta, x, xi)\n\u001b[1;32m    179\u001b[0m grads_d \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mgrad(loss_fn, argnums\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)(params, prng_key, y, theta, x, xi)\n\u001b[1;32m    180\u001b[0m updates, new_opt_state \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mupdate(grads, opt_state)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [140], line 159\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, prng_key, y, theta, x, xi)\u001b[0m\n\u001b[1;32m    157\u001b[0m cond_data \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mcolumn_stack((theta, x, xi))\n\u001b[1;32m    158\u001b[0m \u001b[39m# Loss is average negative log likelihood.\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mjnp\u001b[39m.\u001b[39mmean(log_prob\u001b[39m.\u001b[39;49mapply(params, y, cond_data))\n\u001b[1;32m    160\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/multi_transform.py:298\u001b[0m, in \u001b[0;36mwithout_apply_rng.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_fn\u001b[39m(params, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    297\u001b[0m   check_rng_kwarg(kwargs)\n\u001b[0;32m--> 298\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mapply(params, \u001b[39mNone\u001b[39;49;00m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/transform.py:128\u001b[0m, in \u001b[0;36mwithout_state.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    122\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    123\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mHaiku transform adds three arguments (params, state, rng) to apply. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mIf the functions you are transforming use the same names you must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mpass them positionally (e.g. `f.apply(.., my_state)` and not by \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mname (e.g. `f.apply(.., state=my_state)`)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m out, state \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mapply(params, {}, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m state:\n\u001b[1;32m    130\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf your transformed function uses `hk.\u001b[39m\u001b[39m{\u001b[39m\u001b[39mget,set}_state` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mthen use `hk.transform_with_state`.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/transform.py:357\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39mwith\u001b[39;00m base\u001b[39m.\u001b[39mnew_context(params\u001b[39m=\u001b[39mparams, state\u001b[39m=\u001b[39mstate, rng\u001b[39m=\u001b[39mrng) \u001b[39mas\u001b[39;00m ctx:\n\u001b[1;32m    356\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    358\u001b[0m   \u001b[39mexcept\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    359\u001b[0m     \u001b[39mraise\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [140], line 136\u001b[0m, in \u001b[0;36mlog_prob\u001b[0;34m(data, cond_data)\u001b[0m\n\u001b[1;32m    121\u001b[0m scale \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mstd(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m1e-14\u001b[39m\n\u001b[1;32m    123\u001b[0m model \u001b[39m=\u001b[39m make_nsf(\n\u001b[1;32m    124\u001b[0m     event_shape\u001b[39m=\u001b[39mEVENT_SHAPE,\n\u001b[1;32m    125\u001b[0m     cond_info_shape\u001b[39m=\u001b[39mcond_info_shape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     scale\u001b[39m=\u001b[39mscale,\n\u001b[1;32m    135\u001b[0m )\n\u001b[0;32m--> 136\u001b[0m \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mlog_prob(data, cond_data)\n",
      "File \u001b[0;32m~/Development/lfiax/src/lfiax/distributions/transformed_conditional.py:32\u001b[0m, in \u001b[0;36mConditionalTransformed.log_prob\u001b[0;34m(self, value, z)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, value: Array, z: Array) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m     31\u001b[0m     \u001b[39m\"\"\"See `Distribution.log_prob`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     x, ildj_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbijector\u001b[39m.\u001b[39;49minverse_and_log_det(value, z)\n\u001b[1;32m     33\u001b[0m     lp_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution\u001b[39m.\u001b[39mlog_prob(x)\n\u001b[1;32m     34\u001b[0m     lp_y \u001b[39m=\u001b[39m lp_x \u001b[39m+\u001b[39m ildj_y\n",
      "File \u001b[0;32m~/Development/lfiax/src/lfiax/bijectors/inverse_conditional.py:31\u001b[0m, in \u001b[0;36mConditionalInverse.inverse_and_log_det\u001b[0;34m(self, y, z)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minverse_and_log_det\u001b[39m(\u001b[39mself\u001b[39m, y: Array, z: Array) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Array, Array]:\n\u001b[1;32m     30\u001b[0m     \u001b[39m\"\"\"Computes x = f^{-1}(y) and log|det J(f^{-1})(y)|.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bijector\u001b[39m.\u001b[39;49mforward_and_log_det(y, z)\n",
      "File \u001b[0;32m~/Development/lfiax/src/lfiax/bijectors/chain_conditional.py:31\u001b[0m, in \u001b[0;36mConditionalChain.forward_and_log_det\u001b[0;34m(self, x, z)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_and_log_det\u001b[39m(\u001b[39mself\u001b[39m, x: Array, z: Array) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Array, Array]:\n\u001b[1;32m     30\u001b[0m     \u001b[39m\"\"\"Computes y = f(x|z) and log|det J(f)(x|z)|.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     x, log_det \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bijectors[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mforward_and_log_det(x, z)\n\u001b[1;32m     32\u001b[0m     \u001b[39mfor\u001b[39;00m bijector \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bijectors[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]):\n\u001b[1;32m     33\u001b[0m         x, ld \u001b[39m=\u001b[39m bijector\u001b[39m.\u001b[39mforward_and_log_det(x, z)\n",
      "File \u001b[0;32m~/Development/lfiax/src/lfiax/bijectors/masked_coupling_conditional.py:47\u001b[0m, in \u001b[0;36mMaskedConditionalCoupling.forward_and_log_det\u001b[0;34m(self, x, z)\u001b[0m\n\u001b[1;32m     45\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conditioner(z)\n\u001b[1;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conditioner(masked_x, z)\n\u001b[1;32m     48\u001b[0m y0, log_d \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_bijector(params)\u001b[39m.\u001b[39mforward_and_log_det(x)\n\u001b[1;32m     49\u001b[0m y \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mwhere(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_mask, x, y0)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/module.py:434\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     local_module_name \u001b[39m=\u001b[39m module_name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    432\u001b[0m     f \u001b[39m=\u001b[39m stateful\u001b[39m.\u001b[39mnamed_call(f, name\u001b[39m=\u001b[39mlocal_module_name)\n\u001b[0;32m--> 434\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    436\u001b[0m \u001b[39m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39mif\u001b[39;00m module_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/module.py:273\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 273\u001b[0m   \u001b[39mreturn\u001b[39;00m bound_method(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    275\u001b[0m ctx \u001b[39m=\u001b[39m MethodContext(module\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m                     method_name\u001b[39m=\u001b[39mmethod_name,\n\u001b[1;32m    277\u001b[0m                     orig_method\u001b[39m=\u001b[39mbound_method)\n\u001b[1;32m    278\u001b[0m interceptor_stack_copy \u001b[39m=\u001b[39m interceptor_stack\u001b[39m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/Development/lfiax/src/lfiax/nets/conditioners.py:29\u001b[0m, in \u001b[0;36mconditioner_mlp.<locals>.ConditionerModule.__call__\u001b[0;34m(self, x, z)\u001b[0m\n\u001b[1;32m     27\u001b[0m x \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mconcatenate((x, z), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m resnet:\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mfor\u001b[39;00m hidden \u001b[39min\u001b[39;00m hidden_sizes:\n\u001b[1;32m     30\u001b[0m         x \u001b[39m=\u001b[39m hk\u001b[39m.\u001b[39mnets\u001b[39m.\u001b[39mMLP([hidden], activate_final\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(x)\n\u001b[1;32m     31\u001b[0m         x \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m hk\u001b[39m.\u001b[39mLinear(hidden)(hk\u001b[39m.\u001b[39mFlatten()(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/module.py:434\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     local_module_name \u001b[39m=\u001b[39m module_name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    432\u001b[0m     f \u001b[39m=\u001b[39m stateful\u001b[39m.\u001b[39mnamed_call(f, name\u001b[39m=\u001b[39mlocal_module_name)\n\u001b[0;32m--> 434\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    436\u001b[0m \u001b[39m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39mif\u001b[39;00m module_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/module.py:273\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 273\u001b[0m   \u001b[39mreturn\u001b[39;00m bound_method(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    275\u001b[0m ctx \u001b[39m=\u001b[39m MethodContext(module\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m                     method_name\u001b[39m=\u001b[39mmethod_name,\n\u001b[1;32m    277\u001b[0m                     orig_method\u001b[39m=\u001b[39mbound_method)\n\u001b[1;32m    278\u001b[0m interceptor_stack_copy \u001b[39m=\u001b[39m interceptor_stack\u001b[39m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/nets/mlp.py:113\u001b[0m, in \u001b[0;36mMLP.__call__\u001b[0;34m(self, inputs, dropout_rate, rng)\u001b[0m\n\u001b[1;32m    111\u001b[0m out \u001b[39m=\u001b[39m inputs\n\u001b[1;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m--> 113\u001b[0m   out \u001b[39m=\u001b[39m layer(out)\n\u001b[1;32m    114\u001b[0m   \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m (num_layers \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivate_final:\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Only perform dropout if we are activating the output.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m dropout_rate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/module.py:434\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     local_module_name \u001b[39m=\u001b[39m module_name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    432\u001b[0m     f \u001b[39m=\u001b[39m stateful\u001b[39m.\u001b[39mnamed_call(f, name\u001b[39m=\u001b[39mlocal_module_name)\n\u001b[0;32m--> 434\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    436\u001b[0m \u001b[39m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39mif\u001b[39;00m module_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/module.py:273\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 273\u001b[0m   \u001b[39mreturn\u001b[39;00m bound_method(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    275\u001b[0m ctx \u001b[39m=\u001b[39m MethodContext(module\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m                     method_name\u001b[39m=\u001b[39mmethod_name,\n\u001b[1;32m    277\u001b[0m                     orig_method\u001b[39m=\u001b[39mbound_method)\n\u001b[1;32m    278\u001b[0m interceptor_stack_copy \u001b[39m=\u001b[39m interceptor_stack\u001b[39m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/basic.py:176\u001b[0m, in \u001b[0;36mLinear.__call__\u001b[0;34m(self, inputs, precision)\u001b[0m\n\u001b[1;32m    174\u001b[0m   stddev \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size)\n\u001b[1;32m    175\u001b[0m   w_init \u001b[39m=\u001b[39m hk\u001b[39m.\u001b[39minitializers\u001b[39m.\u001b[39mTruncatedNormal(stddev\u001b[39m=\u001b[39mstddev)\n\u001b[0;32m--> 176\u001b[0m w \u001b[39m=\u001b[39m hk\u001b[39m.\u001b[39;49mget_parameter(\u001b[39m\"\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m\"\u001b[39;49m, [input_size, output_size], dtype, init\u001b[39m=\u001b[39;49mw_init)\n\u001b[1;32m    178\u001b[0m out \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mdot(inputs, w, precision\u001b[39m=\u001b[39mprecision)\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_bias:\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/base.py:448\u001b[0m, in \u001b[0;36mreplaceable.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    447\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 448\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapped\u001b[39m.\u001b[39;49m_current(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/base.py:540\u001b[0m, in \u001b[0;36mget_parameter\u001b[0;34m(name, shape, dtype, init)\u001b[0m\n\u001b[1;32m    537\u001b[0m param \u001b[39m=\u001b[39m check_not_none(param, \u001b[39m\"\u001b[39m\u001b[39mParameters cannot be `None`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m \u001b[39mtuple\u001b[39m(shape):\n\u001b[0;32m--> 540\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    541\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfq_name\u001b[39m!r}\u001b[39;00m\u001b[39m with retrieved shape \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m.\u001b[39mshape\u001b[39m!r}\u001b[39;00m\u001b[39m does not match \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    542\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape=\u001b[39m\u001b[39m{\u001b[39;00mshape\u001b[39m!r}\u001b[39;00m\u001b[39m dtype=\u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    544\u001b[0m \u001b[39mreturn\u001b[39;00m param\n",
      "\u001b[0;31mValueError\u001b[0m: 'conditioner_module/mlp/~/linear_0/w' with retrieved shape (6, 500) does not match shape=[5, 500] dtype=dtype('float32')"
     ]
    }
   ],
   "source": [
    "# TODO: Put this in hydra config file\n",
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "# d = jnp.array([-10.0, 0.0, 5.0, 10.0])\n",
    "# d = jnp.array([1., 2.])\n",
    "# d = jnp.array([1.])\n",
    "d_obs = jnp.array([1.])\n",
    "d_prop = jrandom.uniform(key, shape=(1,), minval=-10., maxval=10.)\n",
    "d = jnp.concatenate((d_obs, d_prop), axis=0)\n",
    "len_x = len(d_obs) + len(d_prop)\n",
    "len_xi = len(d_prop)\n",
    "num_samples = 100\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "EVENT_SHAPE = (len(d),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = (theta_shape[0] + len(d),)\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 10\n",
    "mlp_num_layers = 4\n",
    "hidden_size = 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "training_steps = 10  # 00\n",
    "eval_frequency = 100\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 10000\n",
    "# TODO: put this function in training since d will be changing.\n",
    "X = sim_data(d, num_samples, key)\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *cond_info_shape)),\n",
    ")\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "for step in range(training_steps):\n",
    "    params, opt_state = update(params, next(prng_seq), opt_state, next(train_ds))\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using a more simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8141.749\n",
      "7026.248\n",
      "6101.3345\n",
      "5327.9395\n",
      "4675.831\n",
      "4121.5415\n",
      "3646.7786\n",
      "3237.2224\n",
      "2881.5994\n",
      "2570.981\n",
      "2298.2488\n",
      "2057.681\n",
      "1844.6412\n",
      "1655.3422\n",
      "1486.6582\n",
      "1335.9877\n",
      "1201.1442\n",
      "1080.2745\n",
      "971.79266\n",
      "874.3323\n",
      "786.706\n",
      "707.87585\n",
      "636.92914\n",
      "573.0588\n",
      "515.5483\n",
      "463.7596\n",
      "417.122\n",
      "375.12442\n",
      "337.30768\n",
      "303.2594\n",
      "272.60788\n",
      "245.01825\n",
      "220.18892\n",
      "197.8472\n",
      "177.74779\n",
      "159.66864\n",
      "143.40965\n",
      "128.79007\n",
      "115.64697\n",
      "103.83316\n",
      "93.215836\n",
      "83.67557\n",
      "75.1043\n",
      "67.404594\n",
      "60.48888\n",
      "54.27826\n",
      "48.701363\n",
      "43.694206\n",
      "39.19906\n",
      "35.16405\n",
      "31.542425\n",
      "28.292137\n",
      "25.37536\n",
      "22.758108\n",
      "20.409803\n",
      "18.302986\n",
      "16.412924\n",
      "14.717433\n",
      "13.196639\n",
      "11.832554\n",
      "10.609089\n",
      "9.511839\n",
      "8.527853\n",
      "7.6454344\n",
      "6.8541503\n",
      "6.144577\n",
      "5.50838\n",
      "4.9379315\n",
      "4.426497\n",
      "3.9679482\n",
      "3.5568316\n",
      "3.1882825\n",
      "2.857861\n",
      "2.5616286\n",
      "2.2960923\n",
      "2.0580654\n",
      "1.8446738\n",
      "1.6533967\n",
      "1.4819403\n",
      "1.3282497\n",
      "1.1904765\n",
      "1.0669953\n",
      "0.95632344\n",
      "0.85711676\n",
      "0.76819175\n",
      "0.68849045\n",
      "0.6170555\n",
      "0.55302685\n",
      "0.49564597\n",
      "0.4442091\n",
      "0.39811057\n",
      "0.35679317\n",
      "0.319765\n",
      "0.28658006\n",
      "0.2568317\n",
      "0.23017737\n",
      "0.20628563\n",
      "0.18487637\n",
      "0.16568606\n",
      "0.14848359\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# create our dataset\n",
    "X, y = make_regression(n_features=3)\n",
    "X, X_test, y, y_test = train_test_split(X, y)\n",
    "\n",
    "\n",
    "# model weights\n",
    "params = {\n",
    "    'w': jnp.zeros(X.shape[1:]),\n",
    "    'b': 0.\n",
    "}\n",
    "\n",
    "\n",
    "def forward(params, X):\n",
    "    return jnp.dot(X, params['w']) + params['b']\n",
    "\n",
    "\n",
    "def loss_fn(params, X, y):\n",
    "    err = forward(params, X) - y\n",
    "    return jnp.mean(jnp.square(err))  # mse\n",
    "\n",
    "\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "grad_xs = jax.grad(loss_fn, argnums=1)\n",
    "\n",
    "\n",
    "def update(params, grads):\n",
    "    return jax.tree_map(lambda p, g: p - 0.05 * g, params, grads)\n",
    "\n",
    "\n",
    "# the main training loop\n",
    "for _ in range(100):\n",
    "    loss = loss_fn(params, X_test, y_test)\n",
    "    print(loss)\n",
    "\n",
    "    grads = grad_fn(params, X, y)\n",
    "    # print(grads)\n",
    "    grads_x = grad_xs(params, X, y)\n",
    "    # print(grads_x)\n",
    "    params = update(params, grads)\n",
    "# jvp = jvp_fn(params, X, y)\n",
    "# vjp_fn, grad_inputs = jvp_fn(params, X, y)\n",
    "# print(grad_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 6.62432110e-04,  6.52491627e-03,  4.58412757e-03],\n",
       "             [ 2.30209646e-03,  2.26755124e-02,  1.59308463e-02],\n",
       "             [ 6.78365293e-04,  6.68185716e-03,  4.69438732e-03],\n",
       "             [-2.70380464e-04, -2.66323122e-03, -1.87107245e-03],\n",
       "             [ 5.83249319e-04,  5.74496994e-03,  4.03617043e-03],\n",
       "             [ 6.56638294e-05,  6.46784727e-04,  4.54403315e-04],\n",
       "             [ 3.39906896e-04,  3.34806228e-03,  2.35220557e-03],\n",
       "             [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "             [ 5.02135139e-04,  4.94600087e-03,  3.47484881e-03],\n",
       "             [ 1.54503126e-04,  1.52184651e-03,  1.06918428e-03],\n",
       "             [ 2.39479850e-04,  2.35886197e-03,  1.65723567e-03],\n",
       "             [ 1.12545874e-03,  1.10857002e-02,  7.78833916e-03],\n",
       "             [ 5.52348676e-04,  5.44060115e-03,  3.82233388e-03],\n",
       "             [-8.92255572e-04, -8.78866389e-03, -6.17453968e-03],\n",
       "             [ 6.74744151e-05,  6.64618856e-04,  4.66932834e-04],\n",
       "             [ 1.32776122e-03,  1.30783683e-02,  9.18830279e-03],\n",
       "             [ 1.66863378e-03,  1.64359417e-02,  1.15471901e-02],\n",
       "             [ 5.18551155e-04,  5.10769710e-03,  3.58844991e-03],\n",
       "             [ 2.24222662e-03,  2.20857970e-02,  1.55165372e-02],\n",
       "             [-5.67798968e-04, -5.59278578e-03, -3.92925227e-03],\n",
       "             [ 5.40760921e-05,  5.32646256e-04,  3.74214491e-04],\n",
       "             [ 7.91828497e-04,  7.79946288e-03,  5.47956955e-03],\n",
       "             [ 1.06414023e-03,  1.04817171e-02,  7.36400671e-03],\n",
       "             [ 2.06165118e-04,  2.03071395e-03,  1.42669282e-03],\n",
       "             [ 9.58885066e-04,  9.44495946e-03,  6.63562538e-03],\n",
       "             [ 2.21712003e-03,  2.18384974e-02,  1.53427953e-02],\n",
       "             [ 3.72255978e-04,  3.66669893e-03,  2.57606595e-03],\n",
       "             [ 1.36735279e-03,  1.34683410e-02,  9.46228113e-03],\n",
       "             [ 1.90053938e-03,  1.87201984e-02,  1.31520117e-02],\n",
       "             [ 8.71977012e-04,  8.58892035e-03,  6.03420893e-03],\n",
       "             [ 3.58269201e-04,  3.52893001e-03,  2.47927546e-03],\n",
       "             [-1.21671219e-04, -1.19845406e-03, -8.41982663e-04],\n",
       "             [-4.13778704e-04, -4.07569529e-03, -2.86340923e-03],\n",
       "             [ 7.30027270e-04,  7.19072437e-03,  5.05189551e-03],\n",
       "             [ 4.90547449e-04,  4.83186264e-03,  3.39466007e-03],\n",
       "             [ 1.56434416e-03,  1.54086947e-02,  1.08254906e-02],\n",
       "             [ 8.11141392e-04,  7.98969343e-03,  5.61321713e-03],\n",
       "             [ 2.39093578e-03,  2.35505719e-02,  1.65456254e-02],\n",
       "             [ 2.47567135e-04,  2.43852125e-03,  1.71320082e-03],\n",
       "             [ 1.31327659e-04,  1.29356945e-03,  9.08806629e-04],\n",
       "             [-3.48597678e-04, -3.43366596e-03, -2.41234712e-03],\n",
       "             [ 1.07765931e-03,  1.06148785e-02,  7.45756039e-03],\n",
       "             [ 8.12469167e-04,  8.00277200e-03,  5.62240556e-03],\n",
       "             [-3.32181720e-04, -3.27196973e-03, -2.29874603e-03],\n",
       "             [-1.62228287e-04, -1.59793871e-03, -1.12264347e-03],\n",
       "             [ 9.79163568e-04,  9.64470208e-03,  6.77595567e-03],\n",
       "             [ 1.71788165e-03,  1.69210304e-02,  1.18879927e-02],\n",
       "             [ 3.57288489e-04,  3.51926987e-03,  2.47248868e-03],\n",
       "             [ 7.08783104e-04,  6.98147062e-03,  4.90488298e-03],\n",
       "             [ 2.40976596e-03,  2.37360485e-02,  1.66759342e-02],\n",
       "             [ 5.17585489e-04,  5.09818550e-03,  3.58176744e-03],\n",
       "             [ 6.23806380e-04,  6.14445517e-03,  4.31683147e-03],\n",
       "             [ 1.01972057e-03,  1.00441864e-02,  7.05661625e-03],\n",
       "             [ 1.12014764e-03,  1.10333869e-02,  7.75158638e-03],\n",
       "             [ 3.24456574e-04,  3.19587742e-03,  2.24528695e-03],\n",
       "             [-8.36248219e-04, -8.23699404e-03, -5.78696001e-03],\n",
       "             [ 1.22829992e-03,  1.20986793e-02,  8.50001536e-03],\n",
       "             [ 8.92255572e-04,  8.78866389e-03,  6.17453968e-03],\n",
       "             [ 5.02135139e-04,  4.94600087e-03,  3.47484881e-03],\n",
       "             [ 2.02785348e-04,  1.99742336e-03,  1.40330428e-03],\n",
       "             [ 2.05223612e-03,  2.02144012e-02,  1.42017752e-02],\n",
       "             [ 2.47687829e-04,  2.43971008e-03,  1.71403610e-03],\n",
       "             [ 1.81541167e-04,  1.78816949e-03,  1.25629152e-03],\n",
       "             [-2.51067577e-05, -2.47300050e-04, -1.73742446e-04],\n",
       "             [ 5.21448092e-04,  5.13623189e-03,  3.60849709e-03],\n",
       "             [ 8.90324300e-04,  8.76964070e-03,  6.16117474e-03],\n",
       "             [ 2.70380460e-05,  2.66323128e-04,  1.87107245e-04],\n",
       "             [ 9.88819986e-04,  9.73981712e-03,  6.84277946e-03],\n",
       "             [ 6.51085807e-04,  6.41315570e-03,  4.50560916e-03],\n",
       "             [ 2.47204996e-04,  2.43495428e-03,  1.71069487e-03],\n",
       "             [ 1.25340663e-03,  1.23459790e-02,  8.67375731e-03],\n",
       "             [ 1.09298897e-04,  1.07658748e-03,  7.56364374e-04],\n",
       "             [-8.80667823e-04, -8.67452472e-03, -6.09435048e-03],\n",
       "             [ 1.58365714e-04,  1.55989255e-03,  1.09591393e-03],\n",
       "             [ 2.09351745e-03,  2.06210203e-02,  1.44874472e-02]],            dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "print(jnp.min(grads_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Using a non-tuple sequence for multidimensional indexing is not allowed; use `arr[tuple(seq)]` instead of `arr[seq]`. See https://github.com/google/jax/issues/4564 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [58], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m forward_grads_X \u001b[39m=\u001b[39m forward_grad(params, X)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Compute the Jacobian of the loss function with respect to X using the chain rule\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m loss_grads_X \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mdot(grad_fn(params[\u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m], X, y), forward_grads_X)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [57], line 24\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, X, y)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_fn\u001b[39m(params, X, y):\n\u001b[0;32m---> 24\u001b[0m     err \u001b[39m=\u001b[39m forward(params, X) \u001b[39m-\u001b[39m y\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mmean(jnp\u001b[39m.\u001b[39msquare(err))\n",
      "Cell \u001b[0;32mIn [57], line 20\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(params, X)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(params, X):\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mdot(X, params[\u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m+\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:3649\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3643\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(aval, core\u001b[39m.\u001b[39mDShapedArray) \u001b[39mand\u001b[39;00m aval\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m () \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3644\u001b[0m         dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3645\u001b[0m         \u001b[39mnot\u001b[39;00m dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, dtypes\u001b[39m.\u001b[39mbool_) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3646\u001b[0m         \u001b[39misinstance\u001b[39m(arr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mint\u001b[39m)):\n\u001b[1;32m   3647\u001b[0m       \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39mdynamic_index_in_dim(arr, idx, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 3649\u001b[0m treedef, static_idx, dynamic_idx \u001b[39m=\u001b[39m _split_index_for_jit(idx, arr\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m   3650\u001b[0m \u001b[39mreturn\u001b[39;00m _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m   3651\u001b[0m                unique_indices, mode, fill_value)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:3724\u001b[0m, in \u001b[0;36m_split_index_for_jit\u001b[0;34m(idx, shape)\u001b[0m\n\u001b[1;32m   3719\u001b[0m \u001b[39m\"\"\"Splits indices into necessarily-static and dynamic parts.\u001b[39;00m\n\u001b[1;32m   3720\u001b[0m \n\u001b[1;32m   3721\u001b[0m \u001b[39mUsed to pass indices into `jit`-ted function.\u001b[39;00m\n\u001b[1;32m   3722\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3723\u001b[0m \u001b[39m# Convert list indices to tuples in cases (deprecated by NumPy.)\u001b[39;00m\n\u001b[0;32m-> 3724\u001b[0m idx \u001b[39m=\u001b[39m _eliminate_deprecated_list_indexing(idx)\n\u001b[1;32m   3726\u001b[0m \u001b[39m# Expand any (concrete) boolean indices. We can then use advanced integer\u001b[39;00m\n\u001b[1;32m   3727\u001b[0m \u001b[39m# indexing logic to handle them.\u001b[39;00m\n\u001b[1;32m   3728\u001b[0m idx \u001b[39m=\u001b[39m _expand_bool_indices(idx, shape)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:4004\u001b[0m, in \u001b[0;36m_eliminate_deprecated_list_indexing\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m   4000\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4001\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mUsing a non-tuple sequence for multidimensional indexing is not allowed; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4002\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39muse `arr[array(seq)]` instead of `arr[seq]`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4003\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/google/jax/issues/4564 for more information.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 4004\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m   4005\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4006\u001b[0m   idx \u001b[39m=\u001b[39m (idx,)\n",
      "\u001b[0;31mTypeError\u001b[0m: Using a non-tuple sequence for multidimensional indexing is not allowed; use `arr[tuple(seq)]` instead of `arr[seq]`. See https://github.com/google/jax/issues/4564 for more information."
     ]
    }
   ],
   "source": [
    "forward_grad = jax.jacfwd(forward)\n",
    "\n",
    "# Compute the gradients of the forward pass with respect to X\n",
    "forward_grads_X = forward_grad(params, X)\n",
    "\n",
    "# Compute the Jacobian of the loss function with respect to X using the chain rule\n",
    "loss_grads_X = jnp.dot(grad_fn(params['w'], X, y), forward_grads_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdm3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bac022e209f3f8f811ac02bf6d6b971751e1ab224096f1893a92a620959b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
