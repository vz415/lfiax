{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import distrax\n",
    "import haiku as hk\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from lfiax.flows.nsf import make_nsf\n",
    "\n",
    "from typing import (\n",
    "    Any,\n",
    "    Iterator,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "OptState = Any\n",
    "\n",
    "\n",
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helper functions to simulate data\n",
    "# ----------------------------------------\n",
    "def load_dataset(split: tfds.Split, batch_size: int) -> Iterator[Batch]:\n",
    "    ds = split\n",
    "    ds = ds.shuffle(buffer_size=10 * batch_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=1000)\n",
    "    ds = ds.repeat()\n",
    "    return iter(tfds.as_numpy(ds))\n",
    "\n",
    "\n",
    "def sim_data(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack((y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d)))))\n",
    "\n",
    "\n",
    "def prepare_data(batch: Batch, len_x: int, prng_key: Optional[PRNGKey] = None) -> Array:\n",
    "    # Batch is [y, thetas, d]\n",
    "    data = batch.astype(np.float32)\n",
    "    # Handling the scalar case\n",
    "    if data.shape[1] <= 3:\n",
    "        x = jnp.expand_dims(data[:, :-2], -1)\n",
    "    # Use known length of x to split up the cond_data\n",
    "    x = data[:, :len_x]\n",
    "    cond_data = data[:, len_x:]\n",
    "    return x, cond_data\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Haiku transform functions for training and evaluation\n",
    "# ----------------------------\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob(data: Array, cond_data: Array) -> Array:\n",
    "    # Get batch\n",
    "    shift = data.mean(axis=0)\n",
    "    scale = data.std(axis=0) + 1e-14\n",
    "\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "        standardize_x=True,\n",
    "        standardize_z=True,\n",
    "        use_resnet=True,\n",
    "        event_dim=EVENT_DIM,\n",
    "        shift=shift,\n",
    "        scale=scale,\n",
    "    )\n",
    "    return model.log_prob(data, cond_data)\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def model_sample(key: PRNGKey, num_samples: int, cond_data: Array) -> Array:\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "    )\n",
    "    z = jnp.repeat(cond_data, num_samples, axis=0)\n",
    "    z = jnp.expand_dims(z, -1)\n",
    "    return model._sample_n(key=key, n=[num_samples], z=z)\n",
    "\n",
    "\n",
    "def loss_fn(params: hk.Params, prng_key: PRNGKey, batch: Batch, len_x: int) -> Array:\n",
    "    # TODO: pass the length of x here\n",
    "    x, cond_data = prepare_data(batch, len_x, prng_key)\n",
    "    # Loss is average negative log likelihood.\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, cond_data))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_fn(params: hk.Params, batch: Batch) -> Array:\n",
    "    x, cond_data = prepare_data(batch)\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, cond_data))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch, len_x: int\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    grads = jax.grad(loss_fn)(params, prng_key, batch, len_x)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(4,), (512, 8), ()]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/util.py:222\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m   \u001b[39mreturn\u001b[39;00m cached(config\u001b[39m.\u001b[39;49m_trace_context(), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/util.py:215\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcached\u001b[39m(_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 215\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/lax/lax.py:141\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39m@cache\u001b[39m()\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_broadcast_shapes_cached\u001b[39m(\u001b[39m*\u001b[39mshapes: Tuple[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]:\n\u001b[0;32m--> 141\u001b[0m   \u001b[39mreturn\u001b[39;00m _broadcast_shapes_uncached(\u001b[39m*\u001b[39;49mshapes)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/lax/lax.py:157\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m result_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(shapes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[39mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(4,), (512, 8), ()]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m opt_state \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39minit(params)\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(training_steps):\n\u001b[0;32m---> 53\u001b[0m     params, opt_state \u001b[39m=\u001b[39m update(params, \u001b[39mnext\u001b[39;49m(prng_seq), opt_state, \u001b[39mnext\u001b[39;49m(train_ds))\n\u001b[1;32m     55\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m eval_frequency \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     56\u001b[0m         val_loss \u001b[39m=\u001b[39m eval_fn(params, \u001b[39mnext\u001b[39m(valid_ds))\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [22], line 163\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(params, prng_key, opt_state, batch)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m@jax\u001b[39m\u001b[39m.\u001b[39mjit\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\n\u001b[1;32m    160\u001b[0m     params: hk\u001b[39m.\u001b[39mParams, prng_key: PRNGKey, opt_state: OptState, batch: Batch\n\u001b[1;32m    161\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[hk\u001b[39m.\u001b[39mParams, OptState]:\n\u001b[1;32m    162\u001b[0m     \u001b[39m\"\"\"Single SGD update step.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     grads \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mgrad(loss_fn)(params, prng_key, batch)\n\u001b[1;32m    164\u001b[0m     updates, new_opt_state \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mupdate(grads, opt_state)\n\u001b[1;32m    165\u001b[0m     new_params \u001b[39m=\u001b[39m optax\u001b[39m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [22], line 147\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, prng_key, batch)\u001b[0m\n\u001b[1;32m    145\u001b[0m x, thetas \u001b[39m=\u001b[39m prepare_data(batch, prng_key)\n\u001b[1;32m    146\u001b[0m \u001b[39m# Loss is average negative log likelihood.\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mjnp\u001b[39m.\u001b[39mmean(log_prob\u001b[39m.\u001b[39;49mapply(params, x, thetas))\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/multi_transform.py:298\u001b[0m, in \u001b[0;36mwithout_apply_rng.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_fn\u001b[39m(params, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    297\u001b[0m   check_rng_kwarg(kwargs)\n\u001b[0;32m--> 298\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mapply(params, \u001b[39mNone\u001b[39;49;00m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/transform.py:128\u001b[0m, in \u001b[0;36mwithout_state.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    122\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    123\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mHaiku transform adds three arguments (params, state, rng) to apply. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mIf the functions you are transforming use the same names you must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mpass them positionally (e.g. `f.apply(.., my_state)` and not by \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mname (e.g. `f.apply(.., state=my_state)`)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m out, state \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mapply(params, {}, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m state:\n\u001b[1;32m    130\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf your transformed function uses `hk.\u001b[39m\u001b[39m{\u001b[39m\u001b[39mget,set}_state` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mthen use `hk.transform_with_state`.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/transform.py:357\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39mwith\u001b[39;00m base\u001b[39m.\u001b[39mnew_context(params\u001b[39m=\u001b[39mparams, state\u001b[39m=\u001b[39mstate, rng\u001b[39m=\u001b[39mrng) \u001b[39mas\u001b[39;00m ctx:\n\u001b[1;32m    356\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    358\u001b[0m   \u001b[39mexcept\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    359\u001b[0m     \u001b[39mraise\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [22], line 126\u001b[0m, in \u001b[0;36mlog_prob\u001b[0;34m(data, cond_data)\u001b[0m\n\u001b[1;32m    111\u001b[0m scale \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mstd(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m1e-14\u001b[39m\n\u001b[1;32m    113\u001b[0m model \u001b[39m=\u001b[39m make_nsf(\n\u001b[1;32m    114\u001b[0m     event_shape\u001b[39m=\u001b[39mEVENT_SHAPE,\n\u001b[1;32m    115\u001b[0m     cond_info_shape\u001b[39m=\u001b[39mcond_info_shape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     scale\u001b[39m=\u001b[39mscale,\n\u001b[1;32m    125\u001b[0m )\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mlog_prob(data, cond_data)\n",
      "File \u001b[0;32m~/Development/lfiax/src/lfiax/distributions/transformed_conditional.py:32\u001b[0m, in \u001b[0;36mConditionalTransformed.log_prob\u001b[0;34m(self, value, z)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, value: Array, z: Array) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m     31\u001b[0m     \u001b[39m\"\"\"See `Distribution.log_prob`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     x, ildj_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbijector\u001b[39m.\u001b[39;49minverse_and_log_det(value, z)\n\u001b[1;32m     33\u001b[0m     lp_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution\u001b[39m.\u001b[39mlog_prob(x)\n\u001b[1;32m     34\u001b[0m     lp_y \u001b[39m=\u001b[39m lp_x \u001b[39m+\u001b[39m ildj_y\n",
      "File \u001b[0;32m~/Development/lfiax/src/lfiax/bijectors/inverse_conditional.py:31\u001b[0m, in \u001b[0;36mConditionalInverse.inverse_and_log_det\u001b[0;34m(self, y, z)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minverse_and_log_det\u001b[39m(\u001b[39mself\u001b[39m, y: Array, z: Array) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Array, Array]:\n\u001b[1;32m     30\u001b[0m     \u001b[39m\"\"\"Computes x = f^{-1}(y) and log|det J(f^{-1})(y)|.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bijector\u001b[39m.\u001b[39;49mforward_and_log_det(y, z)\n",
      "File \u001b[0;32m~/Development/lfiax/src/lfiax/bijectors/chain_conditional.py:31\u001b[0m, in \u001b[0;36mConditionalChain.forward_and_log_det\u001b[0;34m(self, x, z)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_and_log_det\u001b[39m(\u001b[39mself\u001b[39m, x: Array, z: Array) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Array, Array]:\n\u001b[1;32m     30\u001b[0m     \u001b[39m\"\"\"Computes y = f(x|z) and log|det J(f)(x|z)|.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     x, log_det \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bijectors[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mforward_and_log_det(x, z)\n\u001b[1;32m     32\u001b[0m     \u001b[39mfor\u001b[39;00m bijector \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bijectors[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]):\n\u001b[1;32m     33\u001b[0m         x, ld \u001b[39m=\u001b[39m bijector\u001b[39m.\u001b[39mforward_and_log_det(x, z)\n",
      "File \u001b[0;32m~/Development/lfiax/src/lfiax/bijectors/masked_coupling_conditional.py:42\u001b[0m, in \u001b[0;36mMaskedConditionalCoupling.forward_and_log_det\u001b[0;34m(self, x, z)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m\"\"\"Computes y = f(x|z) and log|det J(f)(x|z)|.\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_forward_input_shape(x)\n\u001b[0;32m---> 42\u001b[0m masked_x \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39;49mwhere(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event_mask, x, \u001b[39m0.0\u001b[39;49m)\n\u001b[1;32m     43\u001b[0m \u001b[39m# TODO: Better logic to detect when scalar x\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m masked_x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:1025\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x, y, size, fill_value)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[39mif\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1024\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msize and fill_value arguments cannot be used in three-term where function.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m _where(condition, x, y)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/numpy/util.py:423\u001b[0m, in \u001b[0;36m_where\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    421\u001b[0m   condition \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mne(condition, lax_internal\u001b[39m.\u001b[39m_zero(condition))\n\u001b[1;32m    422\u001b[0m x, y \u001b[39m=\u001b[39m _promote_dtypes(x, y)\n\u001b[0;32m--> 423\u001b[0m condition, x, y \u001b[39m=\u001b[39m _broadcast_arrays(condition, x, y)\n\u001b[1;32m    424\u001b[0m \u001b[39mtry\u001b[39;00m: is_always_empty \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39mis_empty_shape(np\u001b[39m.\u001b[39mshape(x))\n\u001b[1;32m    425\u001b[0m \u001b[39mexcept\u001b[39;00m: is_always_empty \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# can fail with dynamic shapes\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/numpy/util.py:381\u001b[0m, in \u001b[0;36m_broadcast_arrays\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m shapes \u001b[39mor\u001b[39;00m \u001b[39mall\u001b[39m(core\u001b[39m.\u001b[39msymbolic_equal_shape(shapes[\u001b[39m0\u001b[39m], s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m shapes):\n\u001b[1;32m    378\u001b[0m   \u001b[39m# TODO(mattjj): remove the array(arg) here\u001b[39;00m\n\u001b[1;32m    379\u001b[0m   \u001b[39mreturn\u001b[39;00m [arg \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, ndarray) \u001b[39mor\u001b[39;00m np\u001b[39m.\u001b[39misscalar(arg) \u001b[39melse\u001b[39;00m _asarray(arg)\n\u001b[1;32m    380\u001b[0m           \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[0;32m--> 381\u001b[0m result_shape \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39;49mbroadcast_shapes(\u001b[39m*\u001b[39;49mshapes)\n\u001b[1;32m    382\u001b[0m \u001b[39mreturn\u001b[39;00m [_broadcast_to(arg, result_shape) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/_src/lax/lax.py:157\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    155\u001b[0m result_shape \u001b[39m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m result_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(shapes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[39mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(4,), (512, 8), ()]"
     ]
    }
   ],
   "source": [
    "# TODO: Put this in hydra config file\n",
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "d = jnp.array([-10.0, 0.0, 5.0, 10.0])\n",
    "# d = jnp.array([1., 2.])\n",
    "# d = jnp.array([1.])\n",
    "len_x = len(d)\n",
    "num_samples = 100\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "EVENT_SHAPE = (len(d),)\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = theta_shape\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 10\n",
    "mlp_num_layers = 4\n",
    "hidden_size = 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "training_steps = 10  # 00\n",
    "eval_frequency = 100\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 10000\n",
    "X = sim_data(d, num_samples, key)\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *cond_info_shape)),\n",
    ")\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "for step in range(training_steps):\n",
    "    params, opt_state = update(params, next(prng_seq), opt_state, next(train_ds), len_x)\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-93.362236,  16.603159,  81.72911 , 133.5148  ,  17.897623,\n",
       "              11.547189, -10.      ,   0.      ,   5.      ,  10.      ],            dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_ds\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdm3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bac022e209f3f8f811ac02bf6d6b971751e1ab224096f1893a92a620959b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
