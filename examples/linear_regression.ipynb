{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import distrax\n",
    "import haiku as hk\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# TODO: Import NSF from LFIAX\n",
    "from lfiax.flows.nsf import make_nsf\n",
    "\n",
    "from typing import Any, Iterator, Mapping, Optional, Sequence, Tuple, Callable, Union\n",
    "\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "OptState = Any\n",
    "\n",
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "    \n",
    "    n_n = distrax.Independent(\n",
    "          distrax.MultivariateNormalDiag(mu_noise, sigma_noise)).sample(\n",
    "              seed=keys[0],\n",
    "              sample_shape=[len(d),\n",
    "                            len(priors)])\n",
    "    \n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2., 1./2.).sample(seed=keys[1], \n",
    "                                          sample_shape=[len(d),\n",
    "                                                        len(priors)\n",
    "                                                        ]\n",
    "                                          )\n",
    "    \n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:,0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:,1], 0) \n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:,1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helper functions to simulate data\n",
    "# ----------------------------------------\n",
    "def load_dataset(split: tfds.Split, batch_size: int) -> Iterator[Batch]:\n",
    "  ds = split\n",
    "  ds = ds.shuffle(buffer_size=10 * batch_size)\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(buffer_size=1000)\n",
    "  ds = ds.repeat()\n",
    "  return iter(tfds.as_numpy(ds))\n",
    "\n",
    "\n",
    "def sim_data(d: Array, priors: Array, key: PRNGKey):\n",
    "  '''\n",
    "  Returns data in a format suitable for normalizing flow training.\n",
    "  Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "  '''\n",
    "  keys = jrandom.split(key, 2)\n",
    "  \n",
    "  theta_shape = (2,)\n",
    "\n",
    "  mu = jnp.zeros(theta_shape)\n",
    "  sigma = (3 ** 2) * jnp.ones(theta_shape) \n",
    "\n",
    "  base_distribution = distrax.Independent( # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma))\n",
    "\n",
    "  priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "  \n",
    "  y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "  \n",
    "  return jnp.column_stack( (y.T, jnp.squeeze(priors)) )\n",
    "\n",
    "\n",
    "def prepare_data(batch: Batch, prng_key: Optional[PRNGKey] = None) -> Array:\n",
    "  data = batch.astype(np.float32)\n",
    "  # Handling the scalar case\n",
    "  if data.shape[1] <= 3:\n",
    "      x = jnp.expand_dims(data[:, :-2], -1)\n",
    "  x = data[:, :-2]\n",
    "  thetas = data[:, -2:]\n",
    "  return x, thetas\n",
    "\n",
    "# ----------------------------\n",
    "# Haiku transform functions for training and evaluation\n",
    "# ----------------------------\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob(data: Array, cond_data: Array) -> Array:\n",
    "  # Get batch \n",
    "  shift = data.mean(axis=0)\n",
    "  scale = data.std(axis=0) + 1e-14\n",
    "  \n",
    "  model = make_nsf(\n",
    "      event_shape=EVENT_SHAPE,\n",
    "      cond_info_shape=cond_info_shape,\n",
    "      num_layers=flow_num_layers,\n",
    "      hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "      num_bins=num_bins,\n",
    "      standardize=True,\n",
    "      shift=shift,\n",
    "      scale=scale)\n",
    "  return model.log_prob(data, cond_data)\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def model_sample(key: PRNGKey, num_samples: int, cond_data: Array) -> Array:\n",
    "  model = make_nsf(\n",
    "      event_shape=EVENT_SHAPE,\n",
    "      cond_info_shape=cond_info_shape,\n",
    "      num_layers=flow_num_layers,\n",
    "      hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "      num_bins=num_bins)\n",
    "  z = jnp.repeat(cond_data, num_samples, axis=0)\n",
    "  z = jnp.expand_dims(z, -1)\n",
    "  return model._sample_n(key=key, \n",
    "                         n=[num_samples],\n",
    "                         z=z)\n",
    "\n",
    "def loss_fn(params: hk.Params, prng_key: PRNGKey, batch: Batch) -> Array:\n",
    "  x, thetas = prepare_data(batch, prng_key)\n",
    "  # Loss is average negative log likelihood.\n",
    "  loss = -jnp.mean(log_prob.apply(params, x, thetas))\n",
    "  return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_fn(params: hk.Params, batch: Batch) -> Array:\n",
    "  x, thetas = prepare_data(batch) \n",
    "  loss = -jnp.mean(log_prob.apply(params, x, thetas))\n",
    "  return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(params: hk.Params,\n",
    "            prng_key: PRNGKey,\n",
    "            opt_state: OptState,\n",
    "            batch: Batch) -> Tuple[hk.Params, OptState]:\n",
    "  \"\"\"Single SGD update step.\"\"\"\n",
    "  grads = jax.grad(loss_fn)(params, prng_key, batch)\n",
    "  updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "  new_params = optax.apply_updates(params, updates)\n",
    "  return new_params, new_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincent_zaballa/anaconda3/envs/sdm3/lib/python3.9/site-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  param = init(shape, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:     0; Validation loss: 3.980\n"
     ]
    }
   ],
   "source": [
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "# d = jnp.array([-10.,0., 5., 10.])\n",
    "# d = jnp.array([1., 2.])\n",
    "d = jnp.array([1.])\n",
    "num_samples = 100\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "EVENT_SHAPE = (len(d),) \n",
    "cond_info_shape = theta_shape\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 10\n",
    "mlp_num_layers = 4\n",
    "hidden_size = 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# using 100,000 steps could take long (about 2 hours) but will give better results. \n",
    "# You can try with 10,000 steps to run it fast but result may not be very good\n",
    "\n",
    "training_steps =  10#00\n",
    "eval_frequency =  100\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 10000\n",
    "X = sim_data(d, num_samples, key)\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(next(prng_seq), \n",
    "                    np.zeros((1, *EVENT_SHAPE)), \n",
    "                    np.zeros((1, *cond_info_shape)))\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "\n",
    "for step in range(training_steps):\n",
    "    params, opt_state = update(params, next(prng_seq), opt_state,\n",
    "                                next(train_ds))\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdm3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bac022e209f3f8f811ac02bf6d6b971751e1ab224096f1893a92a620959b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
