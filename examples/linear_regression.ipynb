{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import distrax\n",
    "import haiku as hk\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from lfiax.flows.nsf import make_nsf\n",
    "\n",
    "from typing import (\n",
    "    Any,\n",
    "    Iterator,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "OptState = Any\n",
    "\n",
    "\n",
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "def sim_true_linear_jax(d: Array, theta_true: Array, key: PRNGKey):\n",
    "    # TODO: check that `theta_true` is the correct size\n",
    "    # TODO: check that function works as expected\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(theta_true)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(theta_true)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(theta_true[:, 0], (len(d), len(theta_true)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(theta_true[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = theta_true[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "def sim_data(d: Array, num_samples: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas, d]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack(\n",
    "        (y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d))))\n",
    "    )\n",
    "\n",
    "\n",
    "def sim_linear_jax_laplace(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    concentration = jnp.ones(noise_shape)\n",
    "    rate = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Gamma(concentration, rate).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = distrax.MultivariateNormalDiag(y, jnp.squeeze(n_n)).sample(seed=keys[1], sample_shape=())\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def sim_data_laplace(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "    theta_shape = (1,)\n",
    "\n",
    "    loc = jnp.zeros(theta_shape)\n",
    "    scale = jnp.ones(theta_shape)\n",
    "\n",
    "    # Leaving in case this fixes future dimensionality issues\n",
    "    # base_distribution = distrax.Independent(\n",
    "    #     distrax.Laplace(loc, scale)\n",
    "    # )\n",
    "    base_distribution = distrax.Laplace(loc, scale)\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    y = sim_linear_jax_laplace(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack(\n",
    "        (y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d))))\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helper functions to simulate data\n",
    "# ----------------------------------------\n",
    "def load_dataset(split: tfds.Split, batch_size: int) -> Iterator[Batch]:\n",
    "    ds = split\n",
    "    ds = ds.shuffle(buffer_size=10 * batch_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=1000)\n",
    "    ds = ds.repeat()\n",
    "    return iter(tfds.as_numpy(ds))\n",
    "\n",
    "\n",
    "def prepare_data(batch: Batch, prng_key: Optional[PRNGKey] = None) -> Array:\n",
    "    # Batch is [y, thetas, d]\n",
    "    data = batch.astype(np.float32)\n",
    "    # Handling the scalar case\n",
    "    if data.shape[1] <= 3:\n",
    "        x = jnp.expand_dims(data[:, :-2], -1)\n",
    "    x = data[:, :len_x]\n",
    "    cond_data = data[:, len_x:]\n",
    "    theta = cond_data[:, :-len_x]\n",
    "    d = cond_data[:, -len_x:-len_xi]\n",
    "    xi = cond_data[:, -len_xi:]\n",
    "    return x, theta, d, xi\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Haiku transform functions for training and evaluation\n",
    "# ----------------------------\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob(data: Array, theta: Array, d: Array, xi: Array) -> Array:\n",
    "    # Get batch\n",
    "    shift = data.mean(axis=0)\n",
    "    scale = data.std(axis=0) + 1e-14\n",
    "\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "        standardize_x=True,\n",
    "        standardize_theta=False,\n",
    "        use_resnet=True,\n",
    "        event_dim=EVENT_DIM,\n",
    "        shift=shift,\n",
    "        scale=scale,\n",
    "    )\n",
    "    return model.log_prob(data, theta, d, xi)\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob_scalar(data: Array, theta: Array, xi: Array) -> Array:\n",
    "    # Get batch\n",
    "    shift = data.mean(axis=0)\n",
    "    scale = data.std(axis=0) + 1e-14\n",
    "    \n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "        standardize_x=True,\n",
    "        standardize_theta=False,\n",
    "        use_resnet=True,\n",
    "        event_dim=EVENT_DIM,\n",
    "        shift=shift,\n",
    "        scale=scale,\n",
    "    )\n",
    "    return model.log_prob(data, theta, xi)\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def model_sample(key: PRNGKey, num_samples: int, cond_data: Array) -> Array:\n",
    "    # TODO: update this method?\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "    )\n",
    "    z = jnp.repeat(cond_data, num_samples, axis=0)\n",
    "    z = jnp.expand_dims(z, -1)\n",
    "    return model._sample_n(key=key, n=[num_samples], z=z)\n",
    "\n",
    "\n",
    "def loss_fn(\n",
    "    params: hk.Params, prng_key: PRNGKey, x: Array, theta: Array, d: Array, xi: Array\n",
    ") -> Array:\n",
    "    # Loss is average negative log likelihood.\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, d, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def unified_loss_fn(\n",
    "    params: hk.Params, prng_key: PRNGKey, x: Array, theta: Array, d: Array\n",
    ") -> Array:\n",
    "    xi = params['xi']\n",
    "    xi = jnp.broadcast_to(xi, (len(x), len(xi)))\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "    # Loss is average negative log likelihood.\n",
    "    loss = -jnp.mean(log_prob.apply(flow_params, x, theta, d, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_fn(params: hk.Params, batch: Batch) -> Array:\n",
    "    x, theta, d, xi = prepare_data(batch)\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, d, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    # x, cond_data = prepare_data(batch, prng_key)\n",
    "    x, theta, d, xi = prepare_data(batch)\n",
    "    # grads = jax.grad(loss_fn)(params, prng_key, x, theta, d, xi)\n",
    "    # grads_d = jax.grad(loss_fn, argnums=5)(params, prng_key, x, theta, d, xi)\n",
    "    grads = jax.grad(unified_loss_fn)(params, prng_key, x, theta, d)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, grads#, grads_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar\n",
      "STEP:     0; Validation loss: 3.686\n",
      "STEP:    10; Validation loss: 3.724\n",
      "STEP:    20; Validation loss: 3.638\n",
      "STEP:    30; Validation loss: 3.655\n",
      "STEP:    40; Validation loss: 3.713\n",
      "STEP:    50; Validation loss: 3.623\n",
      "STEP:    60; Validation loss: 3.631\n",
      "STEP:    70; Validation loss: 3.739\n",
      "STEP:    80; Validation loss: 3.604\n",
      "STEP:    90; Validation loss: 3.710\n",
      "STEP:   100; Validation loss: 3.721\n",
      "STEP:   110; Validation loss: 3.665\n"
     ]
    }
   ],
   "source": [
    "# TODO: Put this in hydra config file\n",
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "# d = jnp.array([-10.0, 0.0, 5.0, 10.0])\n",
    "# d = jnp.array([1., 2.])\n",
    "# d = jnp.array([1.])\n",
    "# d_obs = jnp.array([0.])\n",
    "d_obs = jnp.array([])\n",
    "# d_prop = jrandom.uniform(key, shape=(1,), minval=-10.0, maxval=10.0)\n",
    "d_prop = jnp.array([0.1])\n",
    "# d_prop = jnp.array([])\n",
    "d_sim = jnp.concatenate((d_obs, d_prop), axis=0)\n",
    "len_x = len(d_sim)\n",
    "len_d = len(d_obs)\n",
    "len_xi = len(d_prop)\n",
    "num_samples = 100\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "d_shape = (len(d_obs),)\n",
    "xi_shape = (len_xi,)\n",
    "EVENT_SHAPE = (len(d_sim),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = (theta_shape[0], len_d, len_xi)\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 5 #3 # 10\n",
    "mlp_num_layers = 4 # 3 # 4\n",
    "hidden_size = 128 # 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 100\n",
    "early_stopping_memory = 10\n",
    "early_stopping_threshold = 5e-2\n",
    "\n",
    "training_steps = 500\n",
    "eval_frequency = 10\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 512*20\n",
    "# TODO: put this function in training since d will be changing.\n",
    "X = sim_data(d_sim, num_samples, key)\n",
    "\n",
    "shift = X.mean(axis=0)\n",
    "scale = X.std(axis=0) + 1e-14\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *d_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "params['xi']  = d_prop  # Just shape of batch for now\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Can change the length of the deque for more/less leniency in measuring the loss\n",
    "loss_deque = deque(maxlen=early_stopping_memory)\n",
    "for step in range(training_steps):\n",
    "    # params, opt_state, grads_d = update(\n",
    "    params, opt_state, loss = update(\n",
    "        params, next(prng_seq), opt_state, next(train_ds)\n",
    "    )\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "    \n",
    "        loss_deque.append(val_loss)\n",
    "        avg_abs_diff = jnp.mean(abs(jnp.array(loss_deque) - sum(loss_deque)/len(loss_deque)))\n",
    "        if step > warmup_steps and avg_abs_diff < early_stopping_threshold:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(1, 0), dtype=float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((1, *d_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.1], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['xi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conditioner_module/linear', 'conditioner_module/linear_1', 'conditioner_module/linear_2', 'conditioner_module/linear_3', 'conditioner_module/linear_4', 'conditioner_module/mlp/~/linear_0', 'conditioner_module/mlp_1/~/linear_0', 'conditioner_module/mlp_2/~/linear_0', 'conditioner_module/mlp_3/~/linear_0', 'xi'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "              0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'w': DeviceArray([[ 0.11641986,  0.06062162,  0.08570641, ..., -0.02661132,\n",
       "                0.00522166, -0.05321172],\n",
       "              [-0.09612878,  0.13764288,  0.12323083, ..., -0.13470094,\n",
       "               -0.13767847,  0.03184697],\n",
       "              [ 0.00642909, -0.06131384, -0.02754444, ...,  0.00922625,\n",
       "               -0.02335455,  0.13359617],\n",
       "              ...,\n",
       "              [-0.03620361,  0.15946916,  0.05472334, ...,  0.06316594,\n",
       "                0.05481415, -0.04159248],\n",
       "              [ 0.088457  ,  0.07783046,  0.03833425, ...,  0.00539787,\n",
       "               -0.00172452,  0.14950891],\n",
       "              [ 0.09904982, -0.00662384,  0.1275596 , ...,  0.02878463,\n",
       "               -0.02180895,  0.03315656]], dtype=float32)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['conditioner_module/mlp_2/~/linear_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and checking outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.expand_dims(X[:, X.shape[1] // 2], -1) == jnp.expand_dims(X[:, 2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[5.],\n",
       "             [5.],\n",
       "             [5.],\n",
       "             ...,\n",
       "             [5.],\n",
       "             [5.],\n",
       "             [5.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, -len_xi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# TODO: Make code that automatically slices these up.\n",
    "theta_test = jnp.expand_dims(X[:, X.shape[1] // 2], -1)\n",
    "# d_test = jnp.expand_dims(X[:, 4], -1)\n",
    "d_test = X[:, -len_x:-len_xi]\n",
    "xi_test = X[:, -len_xi:]\n",
    "# xi_test = jnp.expand_dims(X[:, 2], -1)\n",
    "# xi_test = jnp.ones((10000, 1)) * 3\n",
    "# xi_test = jnp.expand_dims(X[:, 1], -1)\n",
    "\n",
    "\n",
    "samples = model_sample.apply(params, \n",
    "                    next(prng_seq),\n",
    "                    num_samples=len(theta_test),\n",
    "                    theta=theta_test,\n",
    "                    d=d_test,\n",
    "                    # d=d_obs,\n",
    "                    xi=xi_test)\n",
    "\n",
    "\n",
    "density_1 = gaussian_kde(samples[:, 0])\n",
    "density_2 = gaussian_kde(samples[:, 1])\n",
    "\n",
    "\n",
    "# Plot the density\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(jnp.min(samples), jnp.max(samples), 100)\n",
    "ax.plot(x, density_1(x), label='x1')\n",
    "ax.plot(x, density_2(x), label='x2')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFI-ACE\n",
    "Manually trained and stepped-through LFI-ACE model.\n",
    "\n",
    "1. Approximate likelihood using normalizing flow. Use a bunch of samples and their corresponding $\\theta$ values. Also, since the `pyro` version only uses one noise element, get rid of the other one that Kleinegesse used.\n",
    "2. Use approximated likelihood in ACE computation.\n",
    "3. Approximate the likelihood using LFI-ACE.\n",
    "\n",
    "This is code where i'm experimenting with the `update`, `loss`, and `params` stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "def sim_data(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack(\n",
    "        (y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d))))\n",
    "    )\n",
    "\n",
    "def lfi_ace_loss_fn(\n",
    "    params: hk.Params, prng_key: PRNGKey, x: Array, theta: Array, d: Array, xi: Array, \n",
    "    M: int, prior_dists, \n",
    ") -> Array:\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, d, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sim_linear_prior(num_samples: int, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Simulate prior samples and return their log_prob.\n",
    "    \"\"\"\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    samples, log_prob = base_distribution.sample_and_log_prob(seed=key, sample_shape=[num_samples])\n",
    "\n",
    "    return samples, log_prob\n",
    "\n",
    "\n",
    "def lfi_pce_loss_fn(\n",
    "    params: hk.Params, prng_key: PRNGKey, x: Array, theta: Array, d: Array, num_samples: int, # xi: Array, \n",
    "    M: int #, prior_dist: Distribution\n",
    ") -> Array:\n",
    "    keys = jrandom.split(prng_key, 2)\n",
    "    xi = params['xi']\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "    # theta_0 = prior_dist.sample((num_samples, 1))\n",
    "    # simulate the outcomes before finding their log_probs\n",
    "    d_sim = jnp.concatenate((d, xi), axis=0)\n",
    "    X = sim_data(d_sim, num_samples, keys[0])  # Do I need to split up the prng_key?\n",
    "\n",
    "    # I'm implicitly returning the prior here, that's a little annoying...\n",
    "    x, theta_0, d, xi = prepare_data(X)  # TODO: Maybe refactor this?\n",
    "\n",
    "    conditional_lp = log_prob.apply(flow_params, x, theta_0, d, xi)\n",
    "    # theta_L = prior_dist.sample((num_samples, M-1))\n",
    "    # Need to make an array of the new theta values\n",
    "    \n",
    "    contrastive_lps = []\n",
    "    for _ in range(M):\n",
    "        theta, _ = sim_linear_prior(num_samples, keys[1])\n",
    "        contrastive_lp = log_prob.apply(flow_params, x, theta, d, xi)\n",
    "        contrastive_lps.append(contrastive_lp)\n",
    "\n",
    "    marginal_log_prbs = jnp.concatenate((conditional_lp, jnp.array(contrastive_lps)))\n",
    "\n",
    "    marginal_lp = jax.nn.logsumexp(marginal_log_prbs, -1) - math.log(num_samples)\n",
    "\n",
    "    return _safe_mean_terms(conditional_lp - marginal_lp)\n",
    "\n",
    "\n",
    "    \n",
    "    theta_L = jnp.concatenate((theta_0, theta_L), axis=-1)\n",
    "    flow_loss = -jnp.mean(likelihood_0)\n",
    "    likelihood_L = log_prob.apply(flow_params, x, theta_L, d, xi)\n",
    "\n",
    "    marginal = jax.nn.logsumexp(likelihoods, -1) - math.log(num_samples)\n",
    "    eig_estimate = (likelihoods.exp() * (likelihoods - marginal)).sum(-1).mean(0)\n",
    "    surrogate_loss = eig_estimate.sum() + flow_loss\n",
    "    return surrogate_loss, eig_estimate\n",
    "\n",
    "    # Take N samples of the model\n",
    "    expanded_design = lexpand(design, N)  # N copies of the model\n",
    "    trace = poutine.trace(model).get_trace(expanded_design)\n",
    "    trace.compute_log_prob()\n",
    "    conditional_lp = sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "    y_dict = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "    # Resample M values of theta and compute conditional probabilities\n",
    "    conditional_model = pyro.condition(model, data=y_dict)\n",
    "    # Using (M, 1) instead of (M, N) - acceptable to re-use thetas between ys because\n",
    "    # theta comes before y in graphical model\n",
    "    reexpanded_design = lexpand(design, M, 1)  # sample M theta\n",
    "    retrace = poutine.trace(conditional_model).get_trace(reexpanded_design)\n",
    "    retrace.compute_log_prob()\n",
    "    marginal_log_probs = torch.cat([lexpand(conditional_lp, 1),\n",
    "                                    sum(retrace.nodes[l][\"log_prob\"] for l in observation_labels)])\n",
    "    marginal_lp = marginal_log_probs.logsumexp(0) - math.log(M+1)\n",
    "\n",
    "    return _safe_mean_terms(conditional_lp - marginal_lp)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    x, theta, d, xi = prepare_data(batch)\n",
    "    grads = jax.grad(loss_fn)(params, prng_key, x, theta, d, xi)\n",
    "    # grads_d = jax.grad(loss_fn, argnums=5)(params, prng_key, x, theta, d, xi)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing it completely manual\n",
    "Just to work out the bugs ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def sim_linear_prior(num_samples: int, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Simulate prior samples and return their log_prob.\n",
    "    \"\"\"\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    samples, log_prob = base_distribution.sample_and_log_prob(seed=key, sample_shape=[num_samples])\n",
    "\n",
    "    return samples, log_prob\n",
    "\n",
    "def jax_lexpand(A, *dimensions):\n",
    "    \"\"\"Expand tensor, adding new dimensions on left.\"\"\"\n",
    "    if jnp.isscalar(A):\n",
    "        A = A * jnp.ones(dimensions)\n",
    "        return A\n",
    "    shape = tuple(dimensions) + A.shape\n",
    "    A = A[jnp.newaxis, ...]\n",
    "    A = jnp.broadcast_to(A, shape)\n",
    "    return A\n",
    "\n",
    "\n",
    "def lfi_pce_eig(params: hk.Params, prng_key: PRNGKey, N: int=100, M: int=10, **kwargs):\n",
    "    keys = jrandom.split(prng_key, 3 + M)\n",
    "    xi = jnp.squeeze(params['xi'])\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "\n",
    "    # simulate the outcomes before finding their log_probs\n",
    "    X = sim_data(d_sim, num_samples, keys[0])  # Do I need to split up the prng_key?\n",
    "\n",
    "    # I'm implicitly returning the prior here, that's a little annoying...\n",
    "    x, theta_0, d, xi = prepare_data(X)  # TODO: Maybe refactor this?\n",
    "\n",
    "    conditional_lp = log_prob.apply(flow_params, x, theta_0, d, xi)\n",
    "\n",
    "    contrastive_lps = []\n",
    "    thetas = []\n",
    "    # TODO: can this be parallelized to speed up the computation?\n",
    "    for i in range(M):\n",
    "        theta, _ = sim_linear_prior(num_samples, keys[i + 1])\n",
    "        thetas.append(theta)\n",
    "        contrastive_lp = log_prob.apply(flow_params, x, theta, d, xi)\n",
    "        contrastive_lps.append(contrastive_lp)\n",
    "\n",
    "    marginal_log_prbs = jnp.concatenate((jax_lexpand(conditional_lp, 1), jnp.array(contrastive_lps)))\n",
    "\n",
    "    marginal_lp = jax.nn.logsumexp(marginal_log_prbs, 0) - math.log(M + 1)\n",
    "\n",
    "    return sum(conditional_lp - marginal_lp) - jnp.mean(contrastive_lp)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_pce(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, N: int, M: int\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    grads = jax.grad(lfi_pce_eig)(params, prng_key, N=num_samples, M=inner_samples)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n",
    "\n",
    "# Boilerplate setup\n",
    "seed = 1231\n",
    "M = 10\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "d = jnp.array([0.])\n",
    "xi = jnp.array([0.1])\n",
    "if d.size == 0:\n",
    "    d_sim = xi\n",
    "else:\n",
    "    d_sim = jnp.concatenate((d, xi), axis=0)\n",
    "# d_sim = jnp.concatenate((d, xi), axis=0)\n",
    "\n",
    "# Params and hyperparams\n",
    "len_x = len(d_sim)\n",
    "len_d = len(d)\n",
    "len_xi = len(xi)\n",
    "\n",
    "theta_shape = (2,)\n",
    "d_shape = (len(d),)\n",
    "xi_shape = (len_xi,)\n",
    "EVENT_SHAPE = (len(d_sim),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = (theta_shape[0], len_d, len_xi)\n",
    "\n",
    "num_samples = 10\n",
    "inner_samples = 10 # AKA M or L in BOED parlance\n",
    "batch_size = 128\n",
    "flow_num_layers = 5 #3 # 10\n",
    "mlp_num_layers = 1 # 3 # 4\n",
    "hidden_size = 8 # 128 # 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 10\n",
    "early_stopping_memory = 10\n",
    "early_stopping_threshold = 5e-2\n",
    "\n",
    "training_steps = 100\n",
    "eval_frequency = 5\n",
    "\n",
    "# Initialzie the params\n",
    "prng_seq = hk.PRNGSequence(42)  # TODO: Put one of \"keys\" here?\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *d_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "# init_params = params.copy()\n",
    "params['xi'] = xi\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# TODO: put this function in training since d will be changing.\n",
    "# num_samples = 10000\n",
    "# X = sim_data(d_sim, num_samples, key)\n",
    "\n",
    "# shift = X.mean(axis=0)\n",
    "# scale = X.std(axis=0) + 1e-14\n",
    "\n",
    "# # Create tf dataset from sklearn dataset\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# # Splitting into train/validate ds\n",
    "# train = dataset.skip(2000)\n",
    "# val = dataset.take(2000)\n",
    "\n",
    "# # load_dataset(split: tfds.Split, batch_size: int)\n",
    "# train_ds = load_dataset(train, 512)\n",
    "# valid_ds = load_dataset(val, 512)\n",
    "# print(params)\n",
    "# loss_deque = deque(maxlen=early_stopping_memory)\n",
    "# for step in range(training_steps):\n",
    "#     params, opt_state = update_pce(\n",
    "#         params, next(prng_seq), opt_state, N=num_samples, M=M\n",
    "#     )\n",
    "#     # params, opt_state, grads_d = update(\n",
    "#     #     params, next(prng_seq), opt_state, next(train_ds)\n",
    "#     # )\n",
    "\n",
    "#     print(f\"STEP: {step:5d}; Xi: {params['xi']}\")\n",
    "    # if step % eval_frequency == 0:\n",
    "    #     val_loss = eval_fn(params, next(valid_ds))\n",
    "    #     print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "    \n",
    "    #     loss_deque.append(val_loss)\n",
    "    #     avg_abs_diff = jnp.mean(abs(jnp.array(loss_deque) - sum(loss_deque)/len(loss_deque)))\n",
    "    #     if step > warmup_steps and avg_abs_diff < early_stopping_threshold:\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads[\"xi\"] = grads_xi\n",
    "updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "new_params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['conditioner_module/mlp/~/linear_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(3.6019292, dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfi_pce_eig(params, key, N=num_samples, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = jax.grad(lfi_pce_eig)(params, key, N=num_samples, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "new_params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *d_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "# init_params = params.copy()\n",
    "params['xi'] = xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "keys = jrandom.split(key, 3 + M)\n",
    "num_samples = 10\n",
    "\n",
    "xi = params['xi']\n",
    "flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "\n",
    "X = sim_data(d_sim, num_samples, keys[0])  # Do I need to split up the prng_key?\n",
    "\n",
    "# I'm implicitly returning the prior here, that's a little annoying...\n",
    "x, theta_0, d, xi = prepare_data(X)\n",
    "\n",
    "contrastive_lps = []\n",
    "thetas = []\n",
    "for i in range(M):\n",
    "    theta, _ = sim_linear_prior(num_samples, keys[i + 1])\n",
    "    thetas.append(theta)\n",
    "    contrastive_lp = log_prob.apply(flow_params, x, theta, d, xi)\n",
    "    contrastive_lps.append(contrastive_lp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DeviceArray([[  0.64793307,  12.057756  ],\n",
       "              [ -1.8036    , -21.126171  ],\n",
       "              [  1.133066  ,  11.62901   ],\n",
       "              [ -4.5348973 ,  -7.0127087 ],\n",
       "              [ -5.4115043 ,  -3.4683409 ],\n",
       "              [ -7.423147  ,   7.645422  ],\n",
       "              [  1.5872409 ,  11.221082  ],\n",
       "              [  0.8586352 , -10.206353  ],\n",
       "              [  8.100239  ,  -6.1539264 ],\n",
       "              [-13.176195  ,  10.821227  ]], dtype=float32),\n",
       " DeviceArray([[ -6.1988173 ,  -1.2310148 ],\n",
       "              [-12.849492  , -12.620625  ],\n",
       "              [ -7.4009905 ,  -3.9226563 ],\n",
       "              [ 17.474777  ,  14.818273  ],\n",
       "              [  6.0482283 ,   6.752823  ],\n",
       "              [ 13.28307   ,  -1.4132088 ],\n",
       "              [ -2.7293384 ,   3.487775  ],\n",
       "              [  7.874885  ,  -0.23380284],\n",
       "              [-14.064912  ,  11.756083  ],\n",
       "              [  2.0690577 ,  14.524533  ]], dtype=float32),\n",
       " DeviceArray([[ -3.65689  ,   7.231424 ],\n",
       "              [ -1.9369462,   1.1989701],\n",
       "              [ 12.946467 ,   1.407098 ],\n",
       "              [ -3.5840957, -12.995476 ],\n",
       "              [-12.196788 ,  23.040134 ],\n",
       "              [  3.5272124,  -2.0084808],\n",
       "              [ -2.1063745,  -6.3984814],\n",
       "              [ 14.970311 ,   7.23661  ],\n",
       "              [ -7.285979 ,  -8.209857 ],\n",
       "              [  4.1615047, -17.095598 ]], dtype=float32),\n",
       " DeviceArray([[-13.744735 ,  -1.392093 ],\n",
       "              [  3.3621225,  -1.5808239],\n",
       "              [-17.727169 ,   1.4319452],\n",
       "              [  4.982991 ,  -2.221058 ],\n",
       "              [-14.190501 ,   9.979641 ],\n",
       "              [ 16.577234 ,   3.7251465],\n",
       "              [ -1.0247506, -10.063461 ],\n",
       "              [ 11.156546 , -13.290407 ],\n",
       "              [ 12.168148 ,  -6.211318 ],\n",
       "              [ 20.059345 ,  -8.960184 ]], dtype=float32),\n",
       " DeviceArray([[  3.2364473,  -5.7628765],\n",
       "              [ -1.1260009,   2.75081  ],\n",
       "              [  5.3766537,  -5.6687036],\n",
       "              [-16.1931   ,   4.9224205],\n",
       "              [-20.17436  ,  -4.9318695],\n",
       "              [ -1.8989071, -16.489414 ],\n",
       "              [  3.6700122, -15.912622 ],\n",
       "              [  2.958536 ,  -3.0327377],\n",
       "              [ -7.1398616,  -3.5663493],\n",
       "              [ -7.7596307,  13.272042 ]], dtype=float32),\n",
       " DeviceArray([[  3.3430977 ,   1.6305696 ],\n",
       "              [ -5.9304676 ,   1.5628601 ],\n",
       "              [  3.458716  ,  -1.5521276 ],\n",
       "              [-23.666735  ,   7.204771  ],\n",
       "              [  2.3833272 ,  12.6059885 ],\n",
       "              [  0.08012465,  16.417442  ],\n",
       "              [ 11.023899  ,   1.334738  ],\n",
       "              [ -8.274364  ,   6.487465  ],\n",
       "              [  4.8539615 ,   7.545137  ],\n",
       "              [ 15.07491   ,   9.83883   ]], dtype=float32),\n",
       " DeviceArray([[ -4.6484966 ,   2.0848312 ],\n",
       "              [ -1.5853441 ,   8.249429  ],\n",
       "              [-14.634294  ,  11.465833  ],\n",
       "              [  1.3609746 ,  -5.1135073 ],\n",
       "              [  7.9940386 ,  -3.1233866 ],\n",
       "              [ -6.9010124 ,   6.668855  ],\n",
       "              [  0.25995153,  -3.5660882 ],\n",
       "              [ -8.069077  ,  -4.7544723 ],\n",
       "              [  4.3881006 ,  -1.1735852 ],\n",
       "              [ -7.151715  ,   8.583297  ]], dtype=float32),\n",
       " DeviceArray([[  5.995773 ,  -5.84251  ],\n",
       "              [  3.3989809,   2.707823 ],\n",
       "              [  3.953537 ,  11.817855 ],\n",
       "              [ -9.604214 , -12.563715 ],\n",
       "              [ -0.3455139,  14.815364 ],\n",
       "              [-16.427826 ,  10.514871 ],\n",
       "              [-10.316995 ,   6.9162083],\n",
       "              [ -5.8247757, -15.326024 ],\n",
       "              [ -2.8184185,  -2.100858 ],\n",
       "              [ -8.592105 ,  -4.4695926]], dtype=float32),\n",
       " DeviceArray([[ 4.266702  , -3.480713  ],\n",
       "              [-3.6858387 ,  4.6207247 ],\n",
       "              [-5.331479  , -3.3922307 ],\n",
       "              [ 2.2157893 , -9.628382  ],\n",
       "              [ 7.437256  , -5.187854  ],\n",
       "              [ 2.8552182 , -2.1501591 ],\n",
       "              [10.239139  , -0.30657387],\n",
       "              [ 4.5631547 ,  3.3490367 ],\n",
       "              [-2.8893983 ,  3.3446567 ],\n",
       "              [10.422832  , 20.6276    ]], dtype=float32),\n",
       " DeviceArray([[ -4.398278 ,   3.1264844],\n",
       "              [ -9.21753  ,   7.2552476],\n",
       "              [-11.973638 ,  -5.0422826],\n",
       "              [-13.711408 ,   0.8138804],\n",
       "              [ 21.733585 ,  -7.9876537],\n",
       "              [  5.6295214,  -1.5190306],\n",
       "              [ -3.4423   ,   4.632292 ],\n",
       "              [ -2.259205 ,   3.053364 ],\n",
       "              [ -5.7116723, -15.013002 ],\n",
       "              [ -8.434806 ,   5.2537813]], dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[  0.64793307,  12.057756  ],\n",
       "             [ -1.8036    , -21.126171  ],\n",
       "             [  1.133066  ,  11.62901   ],\n",
       "             [ -4.5348973 ,  -7.0127087 ],\n",
       "             [ -5.4115043 ,  -3.4683409 ],\n",
       "             [ -7.423147  ,   7.645422  ],\n",
       "             [  1.5872409 ,  11.221082  ],\n",
       "             [  0.8586352 , -10.206353  ],\n",
       "             [  8.100239  ,  -6.1539264 ],\n",
       "             [-13.176195  ,  10.821227  ]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-3.253583 , -3.9614515, -3.3008242, -3.7383153, -3.2508907,\n",
       "             -3.7140958, -3.7958465, -3.1544094, -4.623407 , -3.2264678],            dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob.apply(flow_params, x, thetas[0], d, xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ -6.1988173 ,  -1.2310148 ],\n",
       "             [-12.849492  , -12.620625  ],\n",
       "             [ -7.4009905 ,  -3.9226563 ],\n",
       "             [ 17.474777  ,  14.818273  ],\n",
       "             [  6.0482283 ,   6.752823  ],\n",
       "             [ 13.28307   ,  -1.4132088 ],\n",
       "             [ -2.7293384 ,   3.487775  ],\n",
       "             [  7.874885  ,  -0.23380284],\n",
       "             [-14.064912  ,  11.756083  ],\n",
       "             [  2.0690577 ,  14.524533  ]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-3.253583 , -3.9614515, -3.3008242, -3.7383153, -3.2508907,\n",
       "             -3.7140958, -3.7958465, -3.1544094, -4.623407 , -3.2264678],            dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob.apply(flow_params, x, thetas[1], d, xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "              True,  True], dtype=bool)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_lps[0] == contrastive_lps[4]\n",
    "# wtf? get back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_log_prbs = jnp.concatenate((jax_lexpand(conditional_lp, 1), jnp.array(contrastive_lps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 10)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginal_log_prbs.shape  # This may need to have an extra dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "marginal_lp = jax.nn.logsumexp(marginal_log_prbs, 0) - math.log(M + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# marginal_log_prbs -> marginal_lp needs to be over 0th dimension to remove 11\n",
    "marginal_lp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_lp.shape  # Think this needs to be a 1D array not scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_lp - marginal_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_mean_terms(terms):\n",
    "    mask = torch.isnan(terms) | (terms == float('-inf')) | (terms == float('inf'))\n",
    "    if terms.dtype is torch.float32:\n",
    "        nonnan = (~mask).sum(0).float()\n",
    "    elif terms.dtype is torch.float64:\n",
    "        nonnan = (~mask).sum(0).double()\n",
    "    terms[mask] = 0.\n",
    "    loss = terms.sum(0) / nonnan\n",
    "    agg_loss = loss.sum()\n",
    "    return agg_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conditioner_module/linear', 'conditioner_module/linear_1', 'conditioner_module/linear_2', 'conditioner_module/linear_3', 'conditioner_module/linear_4', 'conditioner_module/mlp/~/linear_0', 'conditioner_module/mlp_1/~/linear_0', 'conditioner_module/mlp_2/~/linear_0', 'conditioner_module/mlp_3/~/linear_0'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['xi'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conditioner_module/linear', 'conditioner_module/linear_1', 'conditioner_module/linear_2', 'conditioner_module/linear_3', 'conditioner_module/linear_4', 'conditioner_module/mlp/~/linear_0', 'conditioner_module/mlp_1/~/linear_0', 'conditioner_module/mlp_2/~/linear_0', 'conditioner_module/mlp_3/~/linear_0', 'xi'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = params.pop('xi', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conditioner_module/linear', 'conditioner_module/linear_1', 'conditioner_module/linear_2', 'conditioner_module/linear_3', 'conditioner_module/linear_4', 'conditioner_module/mlp/~/linear_0', 'conditioner_module/mlp_1/~/linear_0', 'conditioner_module/mlp_2/~/linear_0', 'conditioner_module/mlp_3/~/linear_0'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train. Just need a vector of the samples and thetas that produced them. Don't need a simulator or the prior quite yet.\n",
    "- How do I track the training curves in Jax/Haiku? Gave in and am using WANDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:     0; Validation loss: 9.587\n",
      "STEP:    10; Validation loss: 9.453\n",
      "STEP:    20; Validation loss: 9.295\n",
      "STEP:    30; Validation loss: 9.160\n",
      "STEP:    40; Validation loss: 8.990\n",
      "STEP:    50; Validation loss: 8.950\n",
      "STEP:    60; Validation loss: 8.914\n",
      "STEP:    70; Validation loss: 8.899\n",
      "STEP:    80; Validation loss: 8.833\n",
      "STEP:    90; Validation loss: 8.782\n",
      "STEP:   100; Validation loss: 8.803\n",
      "STEP:   110; Validation loss: 8.844\n",
      "STEP:   120; Validation loss: 8.850\n",
      "STEP:   130; Validation loss: 8.681\n",
      "STEP:   140; Validation loss: 8.731\n",
      "STEP:   150; Validation loss: 8.693\n",
      "STEP:   160; Validation loss: 8.693\n",
      "STEP:   170; Validation loss: 8.739\n",
      "STEP:   180; Validation loss: 8.609\n",
      "STEP:   190; Validation loss: 8.722\n",
      "STEP:   200; Validation loss: 8.581\n",
      "STEP:   210; Validation loss: 8.620\n",
      "STEP:   220; Validation loss: 8.705\n"
     ]
    }
   ],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.init(project=\"lfiax_linRegression_ACE\", entity=\"vz_uci\")\n",
    "\n",
    "# TODO: Put this in hydra config file\n",
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "# d = jnp.array([-10.0, 0.0, 5.0, 10.0])\n",
    "# d = jnp.array([1., 2.])\n",
    "# d = jnp.array([1.])\n",
    "# d_obs = jnp.array([0.])\n",
    "d_obs = jnp.array([])\n",
    "# d_prop = jrandom.uniform(key, shape=(1,), minval=-10.0, maxval=10.0)\n",
    "xi = jnp.array([0.])\n",
    "# d_prop = jnp.array([])\n",
    "d_sim = jnp.concatenate((d_obs, xi), axis=0)\n",
    "len_x = len(d_sim)\n",
    "len_d = len(d_obs)\n",
    "len_xi = len(xi)\n",
    "num_samples = 100\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "d_shape = (len(d_obs),)\n",
    "xi_shape = (len_xi,)\n",
    "EVENT_SHAPE = (len(d_sim),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = (theta_shape[0], len_d, len_xi)\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 5 #3 # 10\n",
    "mlp_num_layers = 4 # 3 # 4\n",
    "hidden_size = 128 # 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 100\n",
    "early_stopping_memory = 10\n",
    "early_stopping_threshold = 5e-2\n",
    "\n",
    "training_steps = 500\n",
    "eval_frequency = 10\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 10000\n",
    "# TODO: put this function in training since d will be changing.\n",
    "X = sim_data(d_sim, num_samples, key)\n",
    "\n",
    "shift = X.mean(axis=0)\n",
    "scale = X.std(axis=0) + 1e-14\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *d_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "params['xi'] = xi\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Can change the length of the deque for more/less leniency in measuring the loss\n",
    "loss_deque = deque(maxlen=early_stopping_memory)\n",
    "for step in range(training_steps):\n",
    "    params, opt_state, grads_d = update(\n",
    "        params, next(prng_seq), opt_state, next(train_ds)\n",
    "    )\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "    \n",
    "        loss_deque.append(val_loss)\n",
    "        avg_abs_diff = jnp.mean(abs(jnp.array(loss_deque) - sum(loss_deque)/len(loss_deque)))\n",
    "        if step > warmup_steps and avg_abs_diff < early_stopping_threshold:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([10.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['xi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_prop.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the sampling/log-prob stuff work. Now to actually implment LFI ACE. I've got the flow, now just going to do some of the ACE computations manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_lexpand(A, *dimensions):\n",
    "    \"\"\"Expand tensor, adding new dimensions on left.\"\"\"\n",
    "    if jnp.isscalar(A):\n",
    "        A = A * jnp.ones(dimensions)\n",
    "        return A\n",
    "    shape = tuple(dimensions) + A.shape\n",
    "    A = A[jnp.newaxis, ...]\n",
    "    A = jnp.broadcast_to(A, shape)\n",
    "    return A\n",
    "\n",
    "\n",
    "# Walking through and commenting the code\n",
    "def lfi_ace_eig_loss(model:NormalizingFlow, guide:NormalizingFlow, M, observation_labels, target_labels):\n",
    "    def loss_fn(design, num_particles, evaluation=False, **kwargs):\n",
    "        N = num_particles\n",
    "        # Expand to the number of parallel designs being evaluated\n",
    "        expanded_design = lexpand(design, N)\n",
    "        \n",
    "        # TODO: make a for loop over the proposed models here.\n",
    "        for model in models:\n",
    "            # TODO: would be cool to make sampling from the model's prior a method\n",
    "             # Hmm I need N copies for parallelization. I can figure that out later, though.\n",
    "            model_thetas_0 = model.prior_distribution.sample(num_samples)\n",
    "            y_0, log_prob_y_0 = model.sample_with_log_prob(model_thetas_0, expanded_design)\n",
    "            # Get a dictionary of the expanded y and theta values, just make M copies of y values\n",
    "            # TODO: think of a better way to add these into expanded dictionary values...\n",
    "            y_dict_exp = lexpand(y, M)\n",
    "\n",
    "            # This is essentially the p(y|theta)p(theta)\n",
    "            # Ohh, each model will have two ratios, it's current prob and its prior one\n",
    "            marginal_terms_cross = sum(model.prior_distribution.log_prob(model_thetas_0))\n",
    "            marginal_terms_cross += sum(log_prob_y_0)\n",
    "\n",
    "        # Pray that jax can handle this parallelization - vmap and pmap to the rescue!...?\n",
    "        for _ in range(m):\n",
    "            for model in models:\n",
    "                # Sample from q(theta | y, d) using the guide normalizing flow\n",
    "                theta, log_prob = guide.sample_with_log_prob(y, expanded_design, observation_labels, target_labels)\n",
    "                theta_y_dict = {l: theta[l] for l in target_labels}\n",
    "                theta_y_dict.update(y_dict_exp)\n",
    "\n",
    "                marginal_terms_proposal = -sum(log_prob[l] for l in target_labels)\n",
    "                marginal_terms_proposal += sum(log_prob[l] for l in target_labels)\n",
    "                marginal_terms_proposal += sum(log_prob[l] for l in observation_labels)\n",
    "\n",
    "                marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "                terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "                # At eval time, add p(y | theta, d) terms\n",
    "                if evaluation:\n",
    "                    terms += sum(log_prob[l] for l in observation_labels)\n",
    "                return _safe_mean_terms(terms)\n",
    "\n",
    "        # Sample from q(theta | y, d) using the guide normalizing flow\n",
    "        theta, log_prob = guide.sample_with_log_prob(y, expanded_design, observation_labels, target_labels)\n",
    "        theta_y_dict = {l: theta[l] for l in target_labels}\n",
    "        theta_y_dict.update(y_dict_exp)\n",
    "\n",
    "        marginal_terms_proposal = -sum(log_prob[l] for l in target_labels)\n",
    "        marginal_terms_proposal += sum(log_prob[l] for l in target_labels)\n",
    "        marginal_terms_proposal += sum(log_prob[l] for l in observation_labels)\n",
    "\n",
    "        marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "        terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "        # At eval time, add p(y | theta, d) terms\n",
    "        if evaluation:\n",
    "            terms += sum(log_prob[l] for l in observation_labels)\n",
    "        return _safe_mean_terms(terms)\n",
    "\n",
    "    return loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "p = 20\n",
    "n = 20\n",
    "N = 2 # num_parallel\n",
    "M = 2\n",
    "design = torch.zeros([10, 20, 20])\n",
    "expanded_design = lexpand(design, N)\n",
    "\n",
    "w_prior_loc = torch.zeros(p, device=device)\n",
    "w_prior_scale = scale * torch.ones(p, device=device)\n",
    "sigma_prior_scale = scale * torch.tensor(1., device=device)\n",
    "\n",
    "model_learn_xi = make_regression_model(\n",
    "    w_prior_loc, w_prior_scale, sigma_prior_scale, xi_init)\n",
    "\n",
    "model = model_learn_xi\n",
    "guide = PosteriorGuide(n, p, (N,)).to(device)\n",
    "observation_labels = [\"y\"]\n",
    "target_labels = ['w', 'sigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = poutine.trace(model).get_trace(expanded_design)\n",
    "y_dict_exp = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "y_dict = {l: trace.nodes[l][\"value\"] for l in observation_labels}\n",
    "theta_dict = {l: trace.nodes[l][\"value\"] for l in target_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 20])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dict['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 20])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_dict['w'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_dict['sigma'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 10]), torch.Size([2, 10])]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[trace.nodes[l][\"log_prob\"].shape for l in target_labels]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what's throwing me off. Determining the `log_prob` of the `w` vector compresses it for some reason... Oh, wait, it's the probability of each *vector*. That's why it's compressed to the shape of the `y` and `sigma` shapes. Yea, that makes a lot more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.compute_log_prob()\n",
    "# This is taking the log_prob of a vector from a distribution. I can either use a distribution that \n",
    "# is in a vector format or I think it should be fine to add two separate ones together...\n",
    "marginal_terms_cross = sum(trace.nodes[l][\"log_prob\"] for l in target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_terms_cross += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-48.1695, -70.6690, -58.5410, -57.8346, -82.7463, -79.5401, -34.3802,\n",
       "         -75.5568, -62.8853, -90.3731],\n",
       "        [-68.9690, -46.9697, -79.1669, -81.4540, -75.3442, -28.8084, -77.4504,\n",
       "         -67.5315,   2.8248, -70.4339]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginal_terms_cross"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part would be the denominator of LFI-ACE with the sum of all of the models' log_probs. The numerator might actually be the sum of all the log_probs. The denominator would be the weighted sum... Yea, that makes more sense....\n",
    "\n",
    "Part of the loss could then just be adding on the loss of the prediction of each normalizing flow. That makes one scalar loss. Although, there's an EIG for each of the 'y' outputs, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1\nTrace Shapes:\n Param Sites:\nSample Sites:",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [12], line 108\u001b[0m, in \u001b[0;36mPosteriorGuide.forward\u001b[0;34m(self, y_dict, design_prototype, observation_labels, target_labels)\u001b[0m\n\u001b[1;32m    107\u001b[0m y \u001b[39m=\u001b[39m y_dict[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m.5\u001b[39m\n\u001b[0;32m--> 108\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(y))\n\u001b[1;32m    109\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [12], line 92\u001b[0m, in \u001b[0;36mTensorLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m rmv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39minput\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/contrib/util.py:44\u001b[0m, in \u001b[0;36mrmv\u001b[0;34m(A, b)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m\"\"\"Tensorized matrix vector multiplication of rightmost dimensions.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(A, b\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reguide_trace \u001b[39m=\u001b[39m poutine\u001b[39m.\u001b[39;49mtrace(\n\u001b[1;32m      2\u001b[0m     pyro\u001b[39m.\u001b[39;49mcondition(guide, data\u001b[39m=\u001b[39;49mtheta_dict))\u001b[39m.\u001b[39;49mget_trace(\n\u001b[1;32m      3\u001b[0m     y_dict, expanded_design, observation_labels, target_labels)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Here's a spot where you could update each model's parameters based on log_prob\u001b[39;00m\n\u001b[1;32m      5\u001b[0m reguide_trace\u001b[39m.\u001b[39mcompute_log_prob()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:198\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_trace\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    191\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m    :returns: data structure\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:180\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         exc \u001b[39m=\u001b[39m exc_type(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(exc_value, shapes))\n\u001b[1;32m    179\u001b[0m         exc \u001b[39m=\u001b[39m exc\u001b[39m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39madd_node(\n\u001b[1;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_RETURN\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_RETURN\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, value\u001b[39m=\u001b[39mret\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39madd_node(\n\u001b[1;32m    171\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m_INPUT\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_INPUT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m     exc_type, exc_value, traceback \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_context_wrap\u001b[39m(context, fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [12], line 108\u001b[0m, in \u001b[0;36mPosteriorGuide.forward\u001b[0;34m(self, y_dict, design_prototype, observation_labels, target_labels)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, y_dict, design_prototype, observation_labels, target_labels):\n\u001b[1;32m    107\u001b[0m     y \u001b[39m=\u001b[39m y_dict[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m.5\u001b[39m\n\u001b[0;32m--> 108\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(y))\n\u001b[1;32m    109\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(x))\n\u001b[1;32m    110\u001b[0m     final \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [12], line 92\u001b[0m, in \u001b[0;36mTensorLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m rmv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39minput\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/contrib/util.py:44\u001b[0m, in \u001b[0;36mrmv\u001b[0;34m(A, b)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrmv\u001b[39m(A, b):\n\u001b[1;32m     43\u001b[0m     \u001b[39m\"\"\"Tensorized matrix vector multiplication of rightmost dimensions.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(A, b\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1\nTrace Shapes:\n Param Sites:\nSample Sites:"
     ]
    }
   ],
   "source": [
    "reguide_trace = poutine.trace(\n",
    "    pyro.condition(guide, data=theta_dict)).get_trace(\n",
    "    y_dict, expanded_design, observation_labels, target_labels)\n",
    "# Here's a spot where you could update each model's parameters based on log_prob\n",
    "reguide_trace.compute_log_prob()\n",
    "marginal_terms_cross -= sum(reguide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = poutine.trace(model).get_trace(expanded_design)\n",
    "y_dict_exp = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "y_dict = {l: trace.nodes[l][\"value\"] for l in observation_labels}\n",
    "theta_dict = {l: trace.nodes[l][\"value\"] for l in target_labels}\n",
    "\n",
    "trace.compute_log_prob()\n",
    "marginal_terms_cross = sum(trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_cross += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "reguide_trace = poutine.trace(\n",
    "    pyro.condition(guide, data=theta_dict)).get_trace(\n",
    "    y_dict, expanded_design, observation_labels, target_labels)\n",
    "# Here's a spot where you could update each model's parameters based on log_prob\n",
    "reguide_trace.compute_log_prob()\n",
    "marginal_terms_cross -= sum(reguide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "\n",
    "# Sample M times from q(theta | y, d) for each y\n",
    "reexpanded_design = lexpand(expanded_design, M)\n",
    "guide_trace = poutine.trace(guide).get_trace(\n",
    "    y_dict, reexpanded_design, observation_labels, target_labels\n",
    ")\n",
    "theta_y_dict = {l: guide_trace.nodes[l][\"value\"] for l in target_labels}\n",
    "theta_y_dict.update(y_dict_exp)\n",
    "guide_trace.compute_log_prob()\n",
    "\n",
    "# Re-run that through the model to compute the joint\n",
    "model_trace = poutine.trace(\n",
    "pyro.condition(model, data=theta_y_dict)).get_trace(reexpanded_design)\n",
    "model_trace.compute_log_prob()\n",
    "\n",
    "marginal_terms_proposal = -sum(guide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_proposal += sum(model_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_proposal += sum(model_trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "# At eval time, add p(y | theta, d) terms\n",
    "# if evaluation:\n",
    "terms += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "# breakpoint()\n",
    "_safe_mean_terms(terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdm3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bac022e209f3f8f811ac02bf6d6b971751e1ab224096f1893a92a620959b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
