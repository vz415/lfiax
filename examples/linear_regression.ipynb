{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import distrax\n",
    "import haiku as hk\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from lfiax.flows.nsf import make_nsf\n",
    "\n",
    "from typing import (\n",
    "    Any,\n",
    "    Iterator,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "OptState = Any\n",
    "\n",
    "\n",
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "def sim_true_linear_jax(d: Array, theta_true: Array, key: PRNGKey):\n",
    "    # TODO: check that `theta_true` is the correct size\n",
    "    # TODO: check that function works as expected\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(theta_true)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(theta_true)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(theta_true[:, 0], (len(d), len(theta_true)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(theta_true[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = theta_true[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "def sim_data(d: Array, num_samples: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas, d]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack(\n",
    "        (y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d))))\n",
    "    )\n",
    "\n",
    "\n",
    "def sim_linear_jax_laplace(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    concentration = jnp.ones(noise_shape)\n",
    "    rate = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Gamma(concentration, rate).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = distrax.MultivariateNormalDiag(y, jnp.squeeze(n_n)).sample(seed=keys[1], sample_shape=())\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def sim_data_laplace(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "    theta_shape = (1,)\n",
    "\n",
    "    loc = jnp.zeros(theta_shape)\n",
    "    scale = jnp.ones(theta_shape)\n",
    "\n",
    "    # Leaving in case this fixes future dimensionality issues\n",
    "    # base_distribution = distrax.Independent(\n",
    "    #     distrax.Laplace(loc, scale)\n",
    "    # )\n",
    "    base_distribution = distrax.Laplace(loc, scale)\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    y = sim_linear_jax_laplace(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack(\n",
    "        (y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d))))\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helper functions to simulate data\n",
    "# ----------------------------------------\n",
    "def load_dataset(split: tfds.Split, batch_size: int) -> Iterator[Batch]:\n",
    "    ds = split\n",
    "    ds = ds.shuffle(buffer_size=10 * batch_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=1000)\n",
    "    ds = ds.repeat()\n",
    "    return iter(tfds.as_numpy(ds))\n",
    "\n",
    "\n",
    "def prepare_data(batch: Batch, prng_key: Optional[PRNGKey] = None) -> Array:\n",
    "    # Batch is [y, thetas, d]\n",
    "    data = batch.astype(np.float32)\n",
    "    # Handling the scalar case\n",
    "    if data.shape[1] <= 3:\n",
    "        x = jnp.expand_dims(data[:, :-2], -1)\n",
    "    x = data[:, :len_x]\n",
    "    cond_data = data[:, len_x:]\n",
    "    theta = cond_data[:, :-len_x]\n",
    "    d = cond_data[:, -len_x:-len_xi]\n",
    "    xi = cond_data[:, -len_xi:]\n",
    "    return x, theta, d, xi\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Haiku transform functions for training and evaluation\n",
    "# ----------------------------\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob(data: Array, theta: Array, d: Array, xi: Array) -> Array:\n",
    "    # Get batch\n",
    "    shift = data.mean(axis=0)\n",
    "    scale = data.std(axis=0) + 1e-14\n",
    "\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "        standardize_x=True,\n",
    "        standardize_theta=False,\n",
    "        use_resnet=True,\n",
    "        event_dim=EVENT_DIM,\n",
    "        shift=shift,\n",
    "        scale=scale,\n",
    "    )\n",
    "    return model.log_prob(data, theta, d, xi)\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def model_sample(key: PRNGKey, num_samples: int, cond_data: Array) -> Array:\n",
    "    # TODO: update this method?\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "    )\n",
    "    z = jnp.repeat(cond_data, num_samples, axis=0)\n",
    "    z = jnp.expand_dims(z, -1)\n",
    "    return model._sample_n(key=key, n=[num_samples], z=z)\n",
    "\n",
    "\n",
    "def loss_fn(\n",
    "    params: hk.Params, prng_key: PRNGKey, x: Array, theta: Array, d: Array, xi: Array\n",
    ") -> Array:\n",
    "    # Loss is average negative log likelihood.\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, d, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_fn(params: hk.Params, batch: Batch) -> Array:\n",
    "    x, theta, d, xi = prepare_data(batch)\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, d, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    # x, cond_data = prepare_data(batch, prng_key)\n",
    "    x, theta, d, xi = prepare_data(batch)\n",
    "    grads = jax.grad(loss_fn)(params, prng_key, x, theta, d, xi)\n",
    "    grads_d = jax.grad(loss_fn, argnums=5)(params, prng_key, x, theta, d, xi)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, grads_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:     0; Validation loss: 9.621\n",
      "STEP:    10; Validation loss: 9.436\n",
      "STEP:    20; Validation loss: 9.180\n",
      "STEP:    30; Validation loss: 9.156\n",
      "STEP:    40; Validation loss: 9.083\n",
      "STEP:    50; Validation loss: 8.976\n",
      "STEP:    60; Validation loss: 8.843\n",
      "STEP:    70; Validation loss: 8.801\n",
      "STEP:    80; Validation loss: 8.849\n",
      "STEP:    90; Validation loss: 8.805\n",
      "STEP:   100; Validation loss: 8.771\n",
      "STEP:   110; Validation loss: 8.834\n",
      "STEP:   120; Validation loss: 8.704\n",
      "STEP:   130; Validation loss: 8.718\n",
      "STEP:   140; Validation loss: 8.763\n",
      "STEP:   150; Validation loss: 8.792\n"
     ]
    }
   ],
   "source": [
    "# TODO: Put this in hydra config file\n",
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "# d = jnp.array([-10.0, 0.0, 5.0, 10.0])\n",
    "# d = jnp.array([1., 2.])\n",
    "# d = jnp.array([1.])\n",
    "d_obs = jnp.array([0.])\n",
    "# d_obs = jnp.array([])\n",
    "# d_prop = jrandom.uniform(key, shape=(1,), minval=-10.0, maxval=10.0)\n",
    "d_prop = jnp.array([10.])\n",
    "# d_prop = jnp.array([])\n",
    "d_sim = jnp.concatenate((d_obs, d_prop), axis=0)\n",
    "len_x = len(d_sim)\n",
    "len_d = len(d_obs)\n",
    "len_xi = len(d_prop)\n",
    "num_samples = 100\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "d_shape = (len(d_obs),)\n",
    "xi_shape = (len_xi,)\n",
    "EVENT_SHAPE = (len(d_sim),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = (theta_shape[0], len_d, len_xi)\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 5 #3 # 10\n",
    "mlp_num_layers = 4 # 3 # 4\n",
    "hidden_size = 128 # 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 100\n",
    "early_stopping_memory = 10\n",
    "early_stopping_threshold = 5e-2\n",
    "\n",
    "training_steps = 500\n",
    "eval_frequency = 10\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 10000\n",
    "# TODO: put this function in training since d will be changing.\n",
    "X = sim_data(d_sim, num_samples, key)\n",
    "\n",
    "shift = X.mean(axis=0)\n",
    "scale = X.std(axis=0) + 1e-14\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *d_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Can change the length of the deque for more/less leniency in measuring the loss\n",
    "loss_deque = deque(maxlen=early_stopping_memory)\n",
    "for step in range(training_steps):\n",
    "    params, opt_state, grads_d = update(\n",
    "        params, next(prng_seq), opt_state, next(train_ds)\n",
    "    )\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "    \n",
    "        loss_deque.append(val_loss)\n",
    "        avg_abs_diff = jnp.mean(abs(jnp.array(loss_deque) - sum(loss_deque)/len(loss_deque)))\n",
    "        if step > warmup_steps and avg_abs_diff < early_stopping_threshold:\n",
    "            break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and checking outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.expand_dims(X[:, X.shape[1] // 2], -1) == jnp.expand_dims(X[:, 2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[5.],\n",
       "             [5.],\n",
       "             [5.],\n",
       "             ...,\n",
       "             [5.],\n",
       "             [5.],\n",
       "             [5.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, -len_xi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABW3UlEQVR4nO3deZxVdeH/8de5d+be2fd9GBh2VJAdxA1M3LVQM1JTI7MyKf3y65uSJi0mLWaUqZh9TSNN00rLTFMSRcUNGEV2gYFh9oXZl7ud3x/nzsgo4DDcO2fm3vfz8Zhwzty5930xhzef7RimaZqIiIiIRAiH3QFEREREQknlRkRERCKKyo2IiIhEFJUbERERiSgqNyIiIhJRVG5EREQkoqjciIiISESJsTvAQAsEAlRUVJCcnIxhGHbHERERkT4wTZOWlhYKCgpwOI48NhN15aaiooKioiK7Y4iIiEg/lJWVMWzYsCM+JurKTXJyMmD95qSkpNicRkRERPqiubmZoqKinj/HjyTqyk33VFRKSorKjYiIyBDTlyUlWlAsIiIiEUXlRkRERCKKyo2IiIhElKhbcyMiIjJY+f1+vF6v3TFs43K5PnWbd1+o3IiIiNjMNE2qqqpobGy0O4qtHA4HI0eOxOVyHdPzqNyIiIjYrLvY5OTkkJCQEJWHzHYfsltZWcnw4cOP6fdA5UZERMRGfr+/p9hkZmbaHcdW2dnZVFRU4PP5iI2N7ffzaEGxiIiIjbrX2CQkJNicxH7d01F+v/+YnkflRkREZBCIxqmojwvV74HKjYiIiEQUlRsRERGJKCo3IiIiElFUbkREjoJpmnR4jm2xo0g0qKys5IorrmDcuHE4HA5uuummAXttlRsRkaNw53NbmfzD//DfbdV2RxEZ1Lq6usjOzua2225j8uTJA/raKjciIn3U4fHz2Fv78PgDfPep96lv7bI7kkQo0zRp9/hs+TBNs08Za2trycvL48477+y59sYbb+ByuVi9ejXFxcX8+te/5uqrryY1NTVcv1WHpEP8RET66MWt1bQFp6TqWj187++bWPml6drCKyHX4fVz/O0v2PLaW350DgmuT68H2dnZPPTQQyxYsICzzz6b8ePHc9VVV7F48WLOPPPMAUh6eBq5ERHpo2c2lgNw1vG5xDgMXthczd+D10Si0fnnn891113HlVdeyTe+8Q0SExNZvny53bE0ciMi0hcNbR5e2VELwM3njmfysFTu+s8Olv1jMyeNyqQgLd7mhBJJ4mOdbPnROba99tG46667mDhxIk8++STr16/H7XaHKVnfqdyIiPTBvzZV4guYnFCQwpicZL4xN5GXttZQUtbI/z71Hqu+MhuHQ9NTEhqGYfRpamgw2LVrFxUVFQQCAUpLS5k0aZLdkTQtJSLSF91TUgumFAIQ43Rw9xcmExfr4PUP6/njulIb04nYw+Px8KUvfYmFCxfy4x//mK9+9avU1NTYHUvlRkTk05Q1tPPu3gMYBnx2SkHP9VHZSSw97zgAfvvyLrviidjm1ltvpampid/85jfcfPPNjBs3jq985Ss9Xy8pKaGkpITW1lZqa2spKSlhy5YtYc+lciMi8imeKbFGbU4ZlU7uhhXw+/lQtxOAS6cPA6CutYvmTq9dEUUG3Jo1a1ixYgWrVq0iJSUFh8PBqlWrWLt2Lffffz8AU6dOZerUqaxfv57HHnuMqVOncv7554c929CY0BMRsYlpmjxdUkEsPn7Cb2HNv6wvvPA9uPJJktwxpCfEcqDdy/6GDo4viLU3sMgAmTdvHl5v70JfXFxMU1NTz+d9PTMn1DRyIyJyBJsrmqmsqeVh1y8YUf4vcMSA4YSd/4HS1wEoykgAoOxAu51RRSRI5UZE5Aheeud9nnD9mFMcmyA2ES5/AqZfY31x9Q/BNClKD5abBpUbkcFA01IiIofhbzvAZSVfodBRQ5c7E/fVT0HhNMg9AUr+DGVvwY7nGZZRDMD+Ax32BhYRQCM3IiKHVbb2jxRSQwVZcO1/rGIDkJIPs79u/fPqHzE8zTq0TCM3IoODyo2IyGE4t1uLh9/MugR3zpjeXzz1JohLhZotTGt+CdCaG5HBQuVGRORQOg6Qf+BdAHxjD7F1NT4dTrkJgDEf/IZYfJQ1dNi2O0REPqJyIyJyCOaO/xCDn+2BYYyeMPnQD5r9DUjKI7aljCucq+nw+qlv8wxsUBH5BJUbEZFD6Nj0DwBWmzM4oSD10A9yJcDc7wLwjVhrCkvrbkTsp3IjIvJx3k5ce1YDsCtjHnFHukvy5C8CBvnUkUUTZdoxJWI7lRsRkY/bvYYYfwcVZgbJo2Ye+bGuRMgcDcBxjr3s16JiEQD+9re/cdZZZ5GdnU1KSgpz5szhhRdeGJDXVrkREfm4bc8C8B//DKaOSP/0x+dNAuB4Yy9lDRq5EQF49dVXOeuss3juuedYv349Z5xxBhdddBEbN24M+2ur3IiIHCzgx9z+bwD+E5jBtOF9KDe5EwGN3Eh0qa2tJS8vjzvvvLPn2htvvIHL5WL16tWsWLGC7373u8ycOZOxY8dy5513MnbsWP75z3+GPZtOKBYROVjZ2xjtdTSaieyKP5Fh6fGf/j15JwLWyM0KLSiWUDBN8Nr0/6XYBDCMT31YdnY2Dz30EAsWLODss89m/PjxXHXVVSxevJgzzzzzE48PBAK0tLSQkZERjtS9qNyIiBwsOCW1OjCVE0dkY/Thhzx51sjNKKOSusYm/AETp6MP3ydyON52uLPAntf+XoW1lqwPzj//fK677jquvPJKZsyYQWJiIsuXLz/kY++66y5aW1v5whe+EMq0h6RpKRGRbqbZe73N8LS+fV9yPmZCJjFGgJGBMqqbO8OXUWSQueuuu/D5fDz55JM8+uijuN3uTzzmscce44c//CF/+ctfyMnJCXsmjdyIiHSr2QIHSunCxauBE/lyUR/W2wAYBkbuRNjzCsc59lLW0E5BWh+ms0QOJzbBGkGx67WPwq5du6ioqCAQCFBaWsqkSZN6ff3xxx/nq1/9Kk8++STz588PZdLDUrkREem2zTqI71X/RLqMOE4cdpjD+w4lbxLsecXaMXWgg9lhiihRwjD6PDVkJ4/Hw5e+9CUWLlzI+PHj+epXv8qmTZt6Rmf+/Oc/85WvfIXHH3+cCy64YMByqdyIiHTb8Txg7ZKakJdCovsofkQGt4Mf59jHG1pULFHi1ltvpampid/85jckJSXx3HPP8ZWvfIVnn32Wxx57jGuuuYZf//rXzJ49m6qqKgDi4+NJTT2Kvzj0g9bciIgABPxQvRmAdwLj+77eplt3uTH2UtbQFuJwIoPPmjVrWLFiBatWrSIlJQWHw8GqVatYu3Yt999/P7/73e/w+XzccMMN5Ofn93zceOONYc+mkRsREYADpeDrxIOLfWYui/tyvs3Bssbhd8SSEuigq7YUmBqGkCKDx7x58/B6vb2uFRcX09TUBMD1119vRyxAIzciIpbabQB8aBYQwHH0IzfOWLrSxgGQ3LglxOFE5Gio3IiIANRsBWB7oJDU+FhGZR39Yk4j35qayuv8EI8vENJ4ItJ3KjciItAzcrMzMIypw9P6dnjfx8QNmwxYJxVXNOoeUyJ2UbkREQGoscrNDnMYU/t6vs3HGD2LivdRpntMidhG5UZEJOCHuh2AVW6mHO16m27B2zAUOWqpqq4OUTiJFqZp2h3BdqH6PVC5ERFp2AP+LjpMF2VmNhPykvv3PPHpNMbmAuCp2BTCgBLJYmNjAWhv12ifx+MBwOl0HtPzaCu4iEittZj4Q7OApDgXOcmfvDdOXzWmTiCtrhpX7QehSicRzul0kpaWRk1NDQAJCQn9WvM11AUCAWpra0lISCAm5tjqicqNiMhB623G5iQd0x8svuwToO4V0lp2hCqdRIG8vDyAnoITrRwOB8OHDz/mcqdyIyISHLnZGRjG2Jx+TkkFuQonw1YY1vlhKJJJlDAMg/z8fHJycj5xMF40cblcOBzHvmJG5UZE5KCRm5Nzk47pqdJHTQNglFlGW3sHiQm6O7j0ndPpPOb1JqIFxSIS7fw+qN8JWOVmdM6xlZvkvDG0Eo/b8FJTqnU3InZQuRGR6NawG/we2k035WYWY4+x3OBwsNc5EoC2vSXHnk9EjprKjYhEt+71NmYh8a5YClKPfRqpNnEMAIFq3WNKxA4qNyIS3YLrbXaawxiTk4TDcexbcL0pwwFwNpcd83OJyNFTuRGR6FZjja7sCBQy5linpIKM9BEAJLTvD8nzicjRUbkRkehWe/AZN8e2DbybO2sUAOmeypA8n4gcHZUbEYlePg/UW+fR7AwMC9nITXK+teYmLdAInraQPKeI9J3KjYhEr4ZdEPDRZsZRTgh2SgXl5ubSZCYA4G8oDclzikjfDYpyc++991JcXExcXByzZ8/m7bff7tP3Pf744xiGwYIFC8IbUEQiU81HO6VcMU6KMhJC8rQ5yXGUmTkANFfqpGKRgWZ7uXniiSdYsmQJy5YtY8OGDUyePJlzzjnnU++vUVpayne+8x1OO+20AUoqIhGne71NYBijs5NwhmCnFIDTYVAbY90rqK1qV0ieU0T6zvZyc/fdd3PdddexaNEijj/+eFauXElCQgIPPfTQYb/H7/dz5ZVX8sMf/pBRo0YNYFoRiSjBkZvtwRtmhlKzuwAAb31pSJ9XRD6dreXG4/Gwfv165s+f33PN4XAwf/581q1bd9jv+9GPfkROTg7XXnvtp75GV1cXzc3NvT5ERICekZudYSg3HclFADga94b0eUXk09laburq6vD7/eTm5va6npubS1VV1SG/57XXXuP//u//ePDBB/v0GsuXLyc1NbXno6io6Jhzi0gE8HVBvTVltCOEO6W6manWWTdxbTrIT2Sg2T4tdTRaWlq46qqrePDBB8nKyurT9yxdupSmpqaej7Iy/aAREawt4KafFjOeKjIYe4x3A/+42Czr/lKpnRVgmiF9bhE5shg7XzwrKwun00l1dXWv69XV1eTl5X3i8bt27aK0tJSLLrqo51ogEAAgJiaG7du3M3r06F7f43a7cbvdYUgvIkPaQYf3xTgcjMhMDOnTJ+da6wHjzA5ob4DEzJA+v4gcnq0jNy6Xi+nTp7N69eqea4FAgNWrVzNnzpxPPH7ChAls2rSJkpKSno/PfvaznHHGGZSUlGjKSUT6rmE3AHvMfEZmJRLrDO2Pw5yMVKrMdOuTA6UhfW4ROTJbR24AlixZwjXXXMOMGTOYNWsWK1asoK2tjUWLFgFw9dVXU1hYyPLly4mLi2PixIm9vj8tLQ3gE9dFRI4oeLje3kBOyKekAArS4tlnZpNnHMDfsAfnsOkhfw0ROTTby83ChQupra3l9ttvp6qqiilTpvD888/3LDLet28fDseQWhokIkNBcDRln5nDmBDdU+pgWUlu1pk5zGQHbdW7SQn5K4jI4dhebgAWL17M4sWLD/m1NWvWHPF7H3744dAHEpHIFyw3ZWYOZ4Z4pxRYB/kdcOWDH7pqd4f8+UXk8DQkIiLRx9eF2VwOwD4zN+Rn3HRrSwyuA9RZNyIDSuVGRKJP4z4MTNpMNw1GCiOzQrtTqpsvxSo3rhYdQSEykFRuRCT6HLTeZkRmEnGxzrC8TEymddZNUmclBPxheQ0R+SSVGxGJPgettxmdHZ4pKYCk7CI8phOn6YPmirC9joj0pnIjItHnoJGbUdnhmZICyEtLpMLM6vWaIhJ+KjciEn0a9gCw18wN23obgPzUePaZOdYnWlQsMmBUbkQk+hw0LRXWcpMWx34zG4BA8NBAEQk/lRsRiS6miXnQtFQ4y01Woptywxq56azdE7bXEZHeVG5EJLq01WF42wiYBvWxeeQkh+/Gug6HQUtcIQCBBpUbkYGiciMi0eWAVTIqyWBYVhqGYYT15TzBs25imveF9XVE5CMqNyISXQZovU03I60YgLjOWvB2hP31RETlRkSiTfd6m8DAlJuUzFxazTjrk0adVCwyEFRuRCS6BMtNuLeBd8tPjacsuGNKZ92IDAyVGxGJLsGFvQM1LZWfFs9+nXUjMqBUbkQkqgSCC4rDvQ28W8HBB/lp5EZkQKjciEj08HbiaKkEoDmugLQEV9hfMi81rmdaKnBAIzciA0HlRkSiR6O1HbvFjCctK39AXjIz0UWlkQuAr15n3YgMBJUbEYkeB28Dz04ekJd0OAw6k4ZZ/6w1NyIDQuVGRKJHr/U2CQP2soHUEQDEeFugo3HAXlckWqnciEj06HVPqaQBe9nM9DQazODrNemsG5FwU7kRkegxQDfM/Li81HjKzSzrEx3kJxJ2KjciEjX89R9NSxUP4LRUQVoc5d0H+WnkRiTsVG5EJDqYJjSWAtCRWESCK2bAXjq/18iNbqApEm4qNyISHVprcPo68JsG7qziAX3p/NS4j8qNRm5Ewk7lRkSiQ3C9TSWZDM9JG9CXPrjcBLTmRiTsVG5EJDocfDfwzIFbTAyQkeii0WUd5Bc4oHIjEm4qNyISHQb4nlIHMwyDhOxiAGI6asHbOaCvLxJtVG5EJCqYB5eb7IEtNwB5eYW0m27rk6b9A/76ItFE5UZEooK3zio3+408itIHbht4t/F5yQctKtaOKZFwUrkRkejQUApAV9IwXDED/6NvXG6yDvITGSAqNyIS+bwduDqqAYjNGmVLhLEHlRvfAY3ciISTyo2IRL7gGpdWM47snHxbImQluWiItXZMtVTvsSWDSLRQuRGRyHdgLwBlZjYjswfuhpkHMwyDQGoRAL4GjdyIhJPKjYhEvuBtF/ab2QO+DfxgCcGTkWNbtVtKJJxUbkQk4gWCa1z2m9kUD/ABfgfLKLDW+yR11UDAb1sOkUinciMiEa+zdjcAFUYOBWnxtuUoHD4Kr+kkBj+0VNqWQyTSqdyISMTz15cC0Jk4DKfDsC3HuPw0qswMK0tdqW05RCKdyo2IRLzYFutcGSN9hK05MhJd1DiyAagu+9DWLCKRTOVGRCJbVwtx3kYA4nNG25sFaIsvAKC5arfNSUQil8qNiES2Rmsx8QEzifycbJvDgBncDu6t13ZwkXBRuRGRyBY842a/mcUIG3dKdXNnWVNjMS3aDi4SLio3IhLRzMbuA/xyGJE58DfM/Lj0AmtqLLlLu6VEwkXlRkQiWkeNtbZlv5nNMBvuBv5xBcPHApAbqKWlw2NzGpHIpHIjIhGtq9a6j1NrfIEtdwP/uOTckQAkGF3sLtPUlEg42P9fuohIGBlN1sJdf4q928B7xMbR6EgHoHrfDpvDiEQmlRsRiVymSXybNToSE7yv02DQFmfdmbyxUtvBRcJB5UZEIlfHAdz+NgBS8kbaHOYj/pRhAHjq99qcRCQyqdyISOQK7pSqNVMZlpNlc5iPuIOjSM5mrbkRCQeVGxGJXMED/MrM7EGxDbxbar51d/B0bzVNHV6b04hEHpUbEYlY3dvAy8wchmcMnnITFxy5KTRq2VndYm8YkQikciMiEautehcAB2LzSHDF2JzmIMFbMBQY9eyobrU5jEjkUbkRkYjla7DW3HQlFdmc5GNSrQXFmUYLu8urbQ4jEnlUbkQkYrmaywAw0gfJGTfd4tPwxiQBUFO+y+YwIpFH5UZEIpNpktRZAUB8zuDZBt4tEJyaaq/Zg88fsDmNSGRRuRGRyNRWi8vsImAapBeMsjvNJ7gyhgOQE6hlV22bzWlEIovKjYhEpgPWeptKMhiRnW5zmE8y0qxyU2jU8v7+RnvDiEQYlRsRiUidtR/dDXz4IDrjpkd6MQAjjBo2lTfZm0UkwqjciEhEaqq0FurWOHJJjY+1Oc0hZI0DYLRRzvv7VW5EQknlRkQikqfOGrlpSyi0OclhZI0FYJRRxfbKRrxaVCwSMio3IhKRjOCtF3ypw21OchhpwzGdbtyGlyx/NTt1mJ9IyAyKcnPvvfdSXFxMXFwcs2fP5u233z7sY//2t78xY8YM0tLSSExMZMqUKaxatWoA04rIUBDfZt2U0pVZbG+Qw3E4MYKjN2OMCjaVN9qbRySC2F5unnjiCZYsWcKyZcvYsGEDkydP5pxzzqGmpuaQj8/IyODWW29l3bp1vP/++yxatIhFixbxwgsvDHByERm0An5SPdbJv0m5o20OcwTBcjPaqNC6G5EQsr3c3H333Vx33XUsWrSI448/npUrV5KQkMBDDz10yMfPmzePiy++mOOOO47Ro0dz4403cuKJJ/Laa68NcHIRGbRaKonBh8d0kjts8B3g1yNrPGCVmw+0Y0okZGwtNx6Ph/Xr1zN//vyeaw6Hg/nz57Nu3bpP/X7TNFm9ejXbt2/n9NNPP+Rjurq6aG5u7vUhIpHNU7cHgAozixHZKTanOYLuaSlHOVsrW/D4tKhYJBRsLTd1dXX4/X5yc3N7Xc/NzaWqquqw39fU1ERSUhIul4sLLriAe+65h7POOuuQj12+fDmpqak9H0VFg+wGeiIScgfKPwSg0sgmM9Flc5ojCG4HH+OoxOMPsKO6xeZAIpHB9mmp/khOTqakpIR33nmHn/zkJyxZsoQ1a9Yc8rFLly6lqamp56OsrGxgw4rIgGurts64aXYXYBiGzWmOIHMMYJBOCxk06zA/kRCJsfPFs7KycDqdVFdX97peXV1NXl7eYb/P4XAwZswYAKZMmcLWrVtZvnw58+bN+8Rj3W43brc7pLlFZHDzNVjbwDuThtmc5FO4EiCtCBr39SwqvnyW3aFEhj5bR25cLhfTp09n9erVPdcCgQCrV69mzpw5fX6eQCBAV1dXOCKKyBAU02KN0DrSB+kZNwfrPqnYoe3gIqFi68gNwJIlS7jmmmuYMWMGs2bNYsWKFbS1tbFo0SIArr76agoLC1m+fDlgraGZMWMGo0ePpquri+eee45Vq1Zx//332/k2RGQQSeqoACAxd/DdDfwTssbDhy8xxijnr1UtdPn8uGOcdqcSGdJsLzcLFy6ktraW22+/naqqKqZMmcLzzz/fs8h43759OBwfDTC1tbXxzW9+k/379xMfH8+ECRP405/+xMKFC+16CyIymAT8ZPhrAcgsHGNzmD4I7piaEFOFt9Nke1ULJw5LszeTyBBnmKZp2h1iIDU3N5OamkpTUxMpKYN4i6iI9Etn/V7i7jkRr+mk6f/tJytlEN4R/GClr8PD51PjzGNW293csWAiXzpphN2pRAado/nze0julhIROZyafdY28Coji8zkeJvT9EG2dZBftr8aNx426aRikWOmciMiEeVAhVVuGmLzBvc28G4JmRCfjoHJKKOS97UdXOSYqdyISETprC21fk0osDdIXxlGr9sw7KxuodPrtzmUyNCmciMikaXJOuMmkDoEtoF3Cy4qnuSuwhewFhWLSP+p3IhIRIlvKwcgLqvY3iBHI3jWzYlxNQBsq9I98ESOhcqNiESUNE8lAKn5o21OchSCi4pHYRWzrZUauRE5Fio3IhIxmtq7yDXrAMgZPtbmNEchOC2V1VWGgwBbKjVyI3IsVG5EJGLs31eK2/Dhw0FS1hBac5M2ApxunIEuCow6tlU2E2VHkImElMqNiESM+u5t4M4scNp+AHvfOZzBO4TDeEcFzZ0+Kpo6bQ4lMnSp3IhIxGit2m39Gpdvc5J+CE5NzU6uB2CbpqZE+k3lRkQihr9hLwDe5CKbk/RDcMfURHc1AFtVbkT6TeVGRCJGTMt+AJwZQ/DeTMEdUyOx3sNWnXUj0m8qNyISEUzTJLmzAoDknJE2p+mH4LRUZqc1+qSRG5H+U7kRkYhQ29pFnlkLQHrhGJvT9EOmVW5cXQfIoJnSujY6PLoNg0h/qNyISEQorW1jmGGVG1dmsb1h+sOV0LPuZn7CTgIm7KjW1JRIf6jciEhEqKjYS5zhJYABKYV2x+mfMfMBOD9uM6CpKZH+6le5efnll0OdQ0TkmDRWWtvAW2KzIcZlc5p+GnMmANO86wGTbVpULNIv/So35557LqNHj+aOO+6grKws1JlERI5aV20pAJ2JBfYGORYjToWYeFK8tUwwynQbBpF+6le5KS8vZ/HixTz11FOMGjWKc845h7/85S94PJ5Q5xMR6RNH0z7rH9KG0G0XPi42DkaeBsA8R4luwyDST/0qN1lZWfzP//wPJSUlvPXWW4wbN45vfvObFBQU8O1vf5v33nsv1DlFRA7LHzBJaLfuBh6XPQS3gR9szFkAnOF8T7dhEOmnY15QPG3aNJYuXcrixYtpbW3loYceYvr06Zx22mls3rw5FBlFRI6oorGDfGoASM4dZXOaYzTWWlQ83bGDJNp1GwaRfuh3ufF6vTz11FOcf/75jBgxghdeeIHf/va3VFdX8+GHHzJixAguu+yyUGYVETmkPXVtFBp1ADjSh/C0FEDGKMgYTQx+TnF8oB1TIv3Qr9vmfutb3+LPf/4zpmly1VVX8fOf/5yJEyf2fD0xMZG77rqLgoIhvLBPRIaMPbWtTA+ecUPqEC83AGPPgrd2Mc/xHq9Vfs7uNCJDTr/KzZYtW7jnnnu45JJLcLvdh3xMVlaWtoyLyICorq4g0eiyPkkdZm+YUBhzFry1knnO93iwssnuNCJDTr+mpZYtW8Zll132iWLj8/l49dVXAYiJiWHu3LnHnlBE5FO01uwBoMOdbe04GuqKT8GMiSPfaMDVsF23YRA5Sv0qN2eccQYNDQ2fuN7U1MQZZ5xxzKFERI5GoMG62aQvOQJGbQBi4zGKTwXgdKNEt2EQOUr9KjemaWIYxieu19fXk5iYeMyhRET6qsvnJ769HIDYzBE2pwmh4JbweY73tKhY5Cgd1ZqbSy65BADDMPjyl7/ca1rK7/fz/vvvc/LJJ4c2oYjIEeytb6cAa6eUO6vY3jChNPYseP5mZji2s6a8CoiAhdIiA+Soyk1qaipgjdwkJycTHx/f8zWXy8VJJ53EddddF9qEIiJHsLu2tedu4MZQPp344zJH05pYRFJbGebuV4BZdicSGTKOqtz84Q9/AKC4uJjvfOc7moISEdvtqm3jM8EzbkiLoGkpwDH2LCh5iJGNb9DQ5iEjcYjeEFRkgPV7t5SKjYgMBruqWyjsPuMmrcjeMCGWcML5AJzp2MDrO2tsTiMydPR55GbatGmsXr2a9PR0pk6desgFxd02bNgQknAiIp+mqraaFKPD+iQ1ssoNI0+ny5FAbqCR0vdfgylftDuRyJDQ53Lzuc99rmcB8YIFC8KVR0Skz0zTxFe3BwzwxWcT40qwO1JoxbhpLjyd7LLnSdn7H0xz4RH/Yikilj6Xm2XLlh3yn0VE7FLb2kWmtxJc4MiIrPU23VKnLoCy5znJ+xa7atsYk5NkdySRQa9fa27KysrYv39/z+dvv/02N910E7/73e9CFkxE5NPsqmmjyLDWojjSi+0NEyauCefgx8F4x37ee09T/iJ90a9yc8UVV/TcN6qqqor58+fz9ttvc+utt/KjH/0opAFFRA5nd10rRd2LidMjc+SGhAyq0qYB4NvyL5vDiAwN/So3H3zwAbNmWWcu/OUvf2HSpEm88cYbPProozz88MOhzCcicljWyE13uSm2NUs4OY+7EIBRDa/g8QVsTiMy+PWr3Hi93p7FxS+99BKf/exnAZgwYQKVlZWhSycicgS7alt7pqUi7Yybg+XMvBiAaWzj/R27bE4jMvj1q9yccMIJrFy5krVr1/Liiy9y7rnnAlBRUUFmZmZIA4qIHM6e2uae04kjdloKcGQUU+4ejdMwqV3/D7vjiAx6/So3P/vZz3jggQeYN28el19+OZMnTwbgH//4R890lYhIOHV6/XgaK3EbPkzDCSkRckfww2gacTYAaWUv2pxEZPA7qtsvdJs3bx51dXU0NzeTnp7ec/1rX/saCQkRds6EiAxKpfVtDCM4JZVaCM5+/TgbMnJnXgI77mdy1wYONDaRnpZqdySRQatfIzcATqezV7EB655TOTk5xxxKROTTHLyY2IjgxcTdMsfMpMbIIsHo4sO3tGtK5Ej6VW6qq6u56qqrKCgoICYmBqfT2etDRCTcrMXE3feUitz1Nj0Mg71ZcwEwt6nciBxJv8Zxv/zlL7Nv3z6+//3vk5+fr+PARWTA7a5t5ZTunVIRvJj4YO6JF8HLf2X0gdcwA34Mh/4yKXIo/So3r732GmvXrmXKlCkhjiMi0je7atu43NG9DbzY1iwDZeysc2n5bzyZRiNlm9+gaNJpdkcSGZT6NS1VVFSEaZqhziIi0iemabK7tjUqtoEfLD4+np0J1u7U0pKXbU4jMnj1q9ysWLGCW265hdLS0hDHERH5dNXNXXg8XeTTYF2IggXF3WKKZgDgL3vX5iQig1e/pqUWLlxIe3s7o0ePJiEhgdjY2F5fb2hoCEk4EZFD2V3bSoFRh8MwITYBErPtjjRgiifPhR2/ZWTnVkrr2ijOSrQ7ksig069ys2LFihDHEBHpu947pYZDFG1qSBllHZQ6wlHD/63fwrXnzLQ5kcjg069yc80114Q6h4hIn+2qbWN4FNxT6pDi02hOLCalrZR9m9aCyo3IJ/T7EL9du3Zx2223cfnll1NTY/2Q+fe//83mzZtDFk5E5FB63TAzShYTH8xdbI3eZDRuorSuzeY0IoNPv8rNK6+8wqRJk3jrrbf429/+RmtrKwDvvfcey5YtC2lAEZGP213bFl0H+H2Me4RVbqYYH/KvTZU2pxEZfPpVbm655RbuuOMOXnzxRVwuV8/1z3zmM7z55pshCyci8nEdHj/ljR0M6xm5KbY1jy0KpwMw2bGL596vsDmMyODTr3KzadMmLr744k9cz8nJoa6u7phDiYgczu46a6R4hCO6zrjpJXciptNNmtFGW9UOTU2JfEy/yk1aWhqVlZ8cCt24cSOFhYXHHEpE5HB217aRSAfptFgXonBaihgXRr51mN8UY5empkQ+pl/l5otf/CI333wzVVVVGIZBIBDg9ddf5zvf+Q5XX311qDOKiPTotQ08Ph3iUuwNZJeDp6ZUbkR66Ve5ufPOO5kwYQJFRUW0trZy/PHHc9ppp3HyySdz2223hTqjiEiPXbVtH+2UisZRm27DrJOKpzo+ZHNFs6amRA7Sr3Ljcrl48MEH2b17N88++yx/+tOf2L59O6tWrcLp1F1qRSR8dh88chONi4m7BUduTnDsw4VXU1MiB+nzIX5Lliw54tcP3iV199139z+RiMhhBAImu2vb+HwUn3HTI70YEjKJba/nOGMvL2zO4oYzxtidSmRQ6HO52bhxY6/PN2zYgM/nY/z48QDs2LEDp9PJ9OnTQ5tQRCSosrmTDq+f4a7oPeOmh2FYozc7/8MUxy5WlY+hqd1LakLsp3+vSITrc7l5+eWXe/757rvvJjk5mUceeYT09HQADhw4wKJFizjttNNCn1JEBNhVY20DHxVTBwGie+QGoHAG7PwPp8bv5ZFWeGtPPWefkGd3KhHb9WvNzS9/+UuWL1/eU2wA0tPTueOOO/jlL3951M937733UlxcTFxcHLNnz+btt98+7GMffPBBTjvtNNLT00lPT2f+/PlHfLyIRI5dta2ASYHZvaC42M449guuu5nq/BCAN3bV25lGZNDoV7lpbm6mtrb2E9dra2tpaWk5qud64oknWLJkCcuWLWPDhg1MnjyZc845p+d+VR+3Zs0aLr/8cl5++WXWrVtHUVERZ599NuXl5f15KyIyhOyqbSWTZtxmJ2BAWpHdkexVOA2ArK79pNLKG7t0iKoI9LPcXHzxxSxatIi//e1v7N+/n/379/PXv/6Va6+9lksuueSonuvuu+/muuuuY9GiRRx//PGsXLmShIQEHnrooUM+/tFHH+Wb3/wmU6ZMYcKECfz+978nEAiwevXqQz6+q6uL5ubmXh8iMjTtqjnonlIpBRDjtjeQ3RIyIGM0YJ13s6O6ldqWLptDidivX+Vm5cqVnHfeeVxxxRWMGDGCESNGcMUVV3Duuedy33339fl5PB4P69evZ/78+R8FcjiYP38+69at69NztLe34/V6ycjIOOTXly9fTmpqas9HUVGU/01PZAjrdTfwaF5MfLDg1NRZKfsBWLdbU1Mi/So3CQkJ3HfffdTX17Nx40Y2btxIQ0MD9913H4mJiX1+nrq6Ovx+P7m5ub2u5+bmUlVV1afnuPnmmykoKOhVkA62dOlSmpqaej7Kysr6nE9EBo+WTi81LV0flZtoX0zcLXiY30nuPQCs09SUSN93Sx1KYmIiJ554YqiyHLWf/vSnPP7446xZs4a4uLhDPsbtduN2R/nQtUgE2F1rncA7zlUPJhq56RYcuRnRuQ0wtahYhH6O3IRKVlYWTqeT6urqXterq6vJyzvydsa77rqLn/70p/znP/+xtWCJyMCwdkrBhNjgqG7WWBvTDCK5E8ERg6urgWGOBvbWt7P/QLvdqURsZWu5cblcTJ8+vddi4O7FwXPmzDns9/385z/nxz/+Mc8//zwzZswYiKgiYrPuclPkD+6MVLmxxMZBzvEAXJRl3YJhnUZvJMrZWm7Auq3Dgw8+yCOPPMLWrVu5/vrraWtrY9GiRQBcffXVLF26tOfxP/vZz/j+97/PQw89RHFxMVVVVVRVVdHa2mrXWxCRAbCrpo00Wkj0N1oXMnWrgR4FUwGYmxRcVKxyI1HumNbchMLChQupra3l9ttvp6qqiilTpvD888/3LDLet28fDsdHHez+++/H4/Hw+c9/vtfzLFu2jB/84AcDGV1EBtCu2lZGGcGbQ6YMA1ffNy9EvMJpsOERJgR2AdZhfqZpYhiGzcFE7GF7uQFYvHgxixcvPuTX1qxZ0+vz0tLS8AcSkUHF5w9QWt/GAkeFdUFTUr0FR25SGz/A5TSoau5kT10bo7KTbA4mYg/bp6VERD5N2YEOvH6T8c7gyE3WOHsDDTY5x4PTjdHZxHmFHQC8rqkpiWIqNyIy6HXfMPMEd/CMG43c9OaMhbxJAJyX3r2oWOfdSPRSuRGRQa97p9QoNC11WMGpqakxuwFrUXEgYNqZSMQ2KjciMujtrm0jFh/Z3u5yo2mpTwiWm+yWrSS4nBxo97Kt6uhuZCwSKVRuRGTQ21XbynCjGgd+cCVBcr7dkQaf4B3CHVXvM3NEKgDvlDbYmUjENio3IjLo7aptZbQRHLXJHAPa4vxJWeMgNgE8rZyR2QjApvImezOJ2ETlRkQGtYY2DwfavYw2tFPqiBxOyJ8MwAzXXgA27Ve5keikciMig1r3YuJJ7uA96FRuDq/Ampoa5dkBwM6aFto9PjsTidhC5UZEBrXubeDjYrpvmKnbLhxWcFFxQt375CS7CZiwtbLZ5lAiA0/lRkQGNWvkxqTQb903SSM3RxBcVEzVJqYWWreneF9TUxKFVG5EZFDbVdtGFs3E+1sAAzJG2x1p8EofCe5U8HVyepp1iJ/W3Ug0UrkRkUGt106ptOEQG2dvoMHM4YACa1HxtNhSAN7XjimJQio3IjJodfn8lDW0M8qhw/v6LLiouLjLWlS8q7aV1i4tKpboonIjIoPWnro2AiYc17OYWOXmUwUXFcfXvkd+ahymCZs1eiNRRuVGRAatLRXWTp+JumFm3wXLDdVbmJofD+gwP4k+KjciMmhtDpabYsqtCyo3ny5tOCRkQsDLvHSrFKrcSLRRuRGRQWtLRTNuPKR7dDpxnxlGz7qbaWwHtGNKoo/KjYgMSqZpsqWymWKjCgMT4lIhMdvuWEND8akADG/eAMDuujaaO712JhIZUCo3IjIoVTR10tThZZwjOGqTOVY3zOyrkacB4Nq/jqJUFwAfaGpKoojKjYgMSt07fGYk11sXNCXVd3mTrcP8upo4P7sW0NSURBeVGxEZlLZUdu+U6r5hphYT95kzBopPAWCuayugRcUSXVRuRGRQ6t4GPjzQvVNKIzdHpdiamprQsRFQuZHoonIjIoOStQ3cJKNjr3VBIzdHZ+TpAKTXrScWH3vr22lq16JiiQ4qNyIy6DS1eylv7CCPBpy+NjCc1k0hpe9yjoeETAxvO2elWqNfGr2RaKFyIyKDTvd6m1NTguttMsdAjMvGREOQw9EzNXVeknWfqffLG20MJDJwVG5EZNDpLjenJAZvmJl/oo1phrDglvBp/k2AdkxJ9FC5EZFBZ3OF9YfwCc591oW8STamGcJGzgUgv/l93Hh4X+VGooTKjYgMOt07pYZ17rQuqNz0T+YYSM7HEfAwzbGT8sYOGts9dqcSCTuVGxEZVLp8fj6saSWRDhJagzul8jQt1S+G8dG6mwRr3U33zUhFIpnKjYgMKjurW/EFTGbEBdfbJBdAYpa9oYay4JbwU2I2Ax9N+YlEMpUbERlUuqekzkgN3lNKU1LHJlhuiru2k0gHH5Rr5EYin8qNiAwq3TulJseWWRe0U+rYpI+AtOE4TT8zHds1ciNRQeVGRAaV7pGbYu9u64JGbo5dcPRmjmMzu+vaaOvy2RxIJLxUbkRk0AgETLZUNhODj9QW7ZQKmeCW8LkxWzBN2FalqSmJbCo3IjJolB1op7XLx4SYKhwBD7hTIK3Y7lhDX7DcTGAPORzQuhuJeCo3IjJodE9JfSYteNuF3InWbQTk2CTnQuEMAOY7N2jdjUQ8/dQQkUGj+wyWme7gYmJNSYXOhPMBONvxrs66kYinciMig0b3iMKYwB7rgnZKhc74CwBrUXF5dQ0eX8DmQCLho3IjIoOCaZqUlDUCJtlt1mm6GrkJoezxmBmjcRs+TjZL2FHdYncikbBRuRGRQWFvfTsH2r2McB4gxtMEjhjInmB3rMhhGBjdU1POd7XuRiKayo2IDArWqA2cmxVcTJx9HMS47QsUiSZcCMBnHCVsLW+wOYxI+KjciMigsHHfAQDmJATvKaUpqdAbNpMuVwYpRjuUvmZ3GpGwUbkRkUGhe+RmPKXWBZWb0HM46Rx9DgBjGl7FHzBtDiQSHio3ImK7Tq+/555S2a3brYvaKRUWSSd+FoAzjHfZU9tqcxqR8FC5ERHbba5oxus3GZnoIaZlv3Uxd6K9oSKUc8wZdOKm0Khn/9Y37Y4jEhYqNyJiu+71Nudn11sX0oZDfJp9gSJZbDy7U2cD4Nzxb5vDiISHyo2I2K57vc2cxHLrQp6mpMKppdhad1NU+7LNSUTCQ+VGRGy3cV8jAOPM4MnEKjdhlXLihfhNg2LvbsyGPXbHEQk5lRsRsVVNSyfljR0YBmQ2brIuFkyxNVOkGz1iOO+a1gGJjRufsTmNSOip3IiIrUqCozYzsgI4Gz60Lg6baV+gKOCKcbAx6TTrky0qNxJ5VG5ExFbd623OTw/uksoaBwkZ9gWKEg3DrXU36fUboLnC5jQioaVyIyK26l5vMztmp3WhaLZ9YaLIhHETeDcwzvpk6z/tDSMSYio3ImIbf8Dk/f2NABR3fGBdVLkZEHNGZ/Kc3/q99m36m81pREJL5UZEbLOzpoU2j59Ul0l87XvWRZWbAZGfGs+mlNMBcO5/C1qqbE4kEjoqNyJim+7FxBfl1mP4OiE+HTLH2BsqioweM4ENgTEYmJqakoiiciMituleb3NGQvCslWGzwKEfSwNlzuhM/hWcmmLz07ZmEQkl/RQREdtsLLNuu3BCYJt1oWiWjWmiz5xRmTzvt37Pzb2vQ0u1zYlEQkPlRkRs0dLpZWdNK2CSfaDEuqj1NgMqJyUOd3YxJYHR1tTUNk1NSWRQuRERW7y/vwnThGmpbThbK8FwQuF0u2NFnTmjNDUlkUflRkRs8W6pNSX12Ywy60L+ieBKsDFRdJozOpN/B4LTgXtfh9ZaewOJhIDt5ebee++luLiYuLg4Zs+ezdtvv33Yx27evJlLL72U4uJiDMNgxYoVAxdURELq3b0NAMyODd5yQVNStjhpVCb7zRzeC4wCM6CpKYkItpabJ554giVLlrBs2TI2bNjA5MmTOeecc6ipqTnk49vb2xk1ahQ//elPycvLG+C0IhIq/oDZsw28uL378D4tJrZDVpKbcblJPQf6aWpKIoGt5ebuu+/muuuuY9GiRRx//PGsXLmShIQEHnrooUM+fubMmfziF7/gi1/8Im63e4DTikio7KhuoaXLR7bLS1z9FuuiRm5sM2dUJs91T02VroW2OnsDiRwj28qNx+Nh/fr1zJ8//6MwDgfz589n3bp1IXudrq4umpube32IiL3e3Wutt1mQU41h+iGlEFKH2Zwqes0ZnUmZmcsOx+jg1NSzdkcSOSa2lZu6ujr8fj+5ubm9rufm5lJVFbpjwJcvX05qamrPR1FRUcieW0T6Z0Ow3MzrPrxPoza2mj0yE8OAv3fNtC5oakqGONsXFIfb0qVLaWpq6vkoKyuzO5JI1OteTHycb6t1QeXGVumJLibkpXw0NbXnVWhvsDeUyDGwrdxkZWXhdDqpru59ImZ1dXVIFwu73W5SUlJ6fYiIfWqaOylr6MBpBEhvKLEuajGx7eaMymSvmUdF3Fgw/ZqakiHNtnLjcrmYPn06q1ev7rkWCARYvXo1c+bMsSuWiIRZ93qb+dnNGJ2NEBMPeZPsDSXMGZ0J8NGZN5qakiHM1mmpJUuW8OCDD/LII4+wdetWrr/+etra2li0aBEAV199NUuXLu15vMfjoaSkhJKSEjweD+Xl5ZSUlPDhhx/a9RZE5CitD5abi5J3WheGzQBnrI2JBGDWyAycDoM/tUyzLux5RVNTMmTF2PniCxcupLa2lttvv52qqiqmTJnC888/37PIeN++fTgOukNwRUUFU6dO7fn8rrvu4q677mLu3LmsWbNmoOOLSD90j9xM95dYF0Z/xr4w0iM1PpaTR2eydqdJXeJYstp2wvbnYOqX7I4mctRsLTcAixcvZvHixYf82scLS3FxMaZpDkAqEQmHDo+fzeVNxOAjt/4d6+LoM+wNJT0umJTP2p11/Ns/i6vYCVueUbmRISnid0uJyODx3v5GfAGTzyTtw+FthfgMyJtsdywJOvuEPJwOg4ebplgXdr0MHY12RhLpF5UbERkw3ettFqRsty6MPgMc+jE0WGQkujh5dCa7zELqE0ZDwGtNTYkMMfqpIiIDprvczPCVWBdGaUpqsLnwxHwAngsEzx7a8oyNaUT6R+VGRAZEIGCyfu8BUmglu2WzdVHrbQads4+3pqb+2DM19V/obLI1k8jRUrkRkQGxq7aVpg4v82K3YpgByBqn+0kNQumJLk4Zk8VOcxgNCSPB74Ht/7Y7lshRUbkRkQHx0fk23etttAV8sLpwUnBqyh+cmtKBfjLEqNyIyICwzrcxmenfaF1QuRm0zj4hlxiHwSPNwQP9dq3WrikZUlRuRCTsTNPkzd31jDCqSeuqBEcsjDjF7lhyGGkJH01N1SeM0tSUDDkqNyISdjuqW9l/oIN5MR9YF4pmgzvJ3lByRBf07Jo6ybqw+e82phE5Oio3IhJ2L22tBuCzSQedbyOD2tnHB6emDt411XHA1kwifaVyIyJh99LWapz4meR9z7qgcjPopSW4OHVsFh+aw6jrPtBv27/sjiXSJyo3IhJWda1dlJQ1MtnYhcvXCvHpkD/F7ljSBxedWADAM95Z1gVNTckQoXIjImH13201mCZ8Pi04JTVyLjic9oaSPjlvUh6JLiePtk63LuxeA+0NtmYS6QuVGxEJq5e2WOttznBusi5oC/iQkeCK4fxJ+ew2C6iIGwMBH2x71u5YIp9K5UZEwqbT62ftzjqGGTXkt34AGDD2bLtjyVG4dLp1ivSTHTOsC5qakiFA5UZEwmbd7no6vH6uSHjHujDyNEjJtzeUHJVZxRkUZcTzd89M68LuV6Ct3t5QIp9C5UZEwmZ1cAv4xbFvWhcmXWZjGukPh8Pg0mnDKDXzKY0dA6Yftv7D7lgiR6RyIyJhYZomq7fWMM4oI79zl3Uq8XEX2R1L+uHSadbU1BOampIhQuVGRMJic0UzlU2dXBK7zrow9mxrG7gMOUUZCcwemcG/um+kWboWmivtDSVyBCo3IhIWq7fWACaXurqnpC61NY8cm89PH8Y+M5dNjuPADMA7D9odSeSwVG5EJCxWb6tmmrGTbF8VxCbCuPPsjiTH4PxJ+SS4nPy28xzrwrsPgafN3lAih6FyIyIhV93cyfv7m7jIGZySOu5CcCXYG0qOSaI7hvMm5vNiYAb1rgLrPlPv/dnuWCKHpHIjIiH37PuVOPFzcexb1oWJn7c3kITEpdMLCeDgd13B0Zt190EgYG8okUNQuRGRkPIHTB55o5Q5ji2kmY0Qn6EbZUaIk0ZmUpQRz6qu0/DEJEPDLtjxvN2xRD5B5UZEQmr11mr2NbRzmSs4JXXCxeCMtTeUhITDYXDNnGLaiePvjrOsi+vutTeUyCGo3IhISD30+h7ceDjXGTyVeJKmpCLJF2YWkehy8qvmMwgYMbD3NajYaHcskV5UbkQkZDZXNPHm7gbOi3kXt78NUoZB0Ul2x5IQSomL5Qszi6gik7cS5loXNXojg4zKjYiEzB9eL8VBgFsSgsfzT78GHPoxE2m+fHIxhgF3NATv8L7579C0395QIgfRTx0RCYnali7+UVLB5xyvk+fZZ51GPPsbdseSMBiRmcj843LZbI5kd9JUCPjgjd/aHUukh8qNiITEo2/tJeD3cHNc8L5Dp9wEcSm2ZpLw+copIwG4s+lc68LbD8C+t2xMJPIRlRsROWZdPj9/enMvn3e+Sl6gChKzYdZ1dseSMDppVAbH5afwkncS2/MutG7J8PevQ1er3dFEVG5E5Nj9871KmlvbuCn2aevCaf8PXIm2ZpLwMgyDr5xSDMANDQsxUwrhwB548fv2BhNB5UZEjlEgYPJ/r+3hi87/kkcdJBfA9EV2x5IB8NkpBWQlufiw2cmbJ95hXXz3Idj5or3BJOqp3IjIMfnn+xXsqazlWzHPWBdO/w7ExtkbSgaEO8bJl04aAcCtJRn4Zn7d+sIzi6G9wcZkEu1UbkSk37p8fn7xwnaucr5IttEIacNh6lV2x5IBtOiUkeQku9ld18Y9xhWQNQ5aq+C579gdTaKYyo2I9Nuf3tyH0VjKt7vX2sy9BWJctmaSgZUaH8uPF0wE4LevVbDr1F+C4YQP/golumu42EPlRkT6panDywOrN3N/7K9Jph2GzYQTF9odS2xwzgl5XDApH3/A5NuvGvhPv9n6wr+WQO0Oe8NJVFK5EZF+WfnKLm7y/p6JjlLMhEy47GFwxtgdS2zyg8+eQGp8LJsrmnmQi2Hk6eBthye/DN4Ou+NJlFG5EZGjVtHYQcPrD3NFzMuYGBiXPAipw+yOJTbKTnbz/QuPB+BXq3exd+4K67yjms3w/FJ7w0nUUbkRkaP253/+mx84/s/6ZO7NMOZMewPJoHDptEJOG5tFly/A/75QQ2DB7wAD1v/BWoMjMkBUbkTkqGwt3c8lO5cSb3hoLjwdY+7NdkeSQcIwDO68eBIJLidv72lg6XtZmKf+P+uL/7gR6nfZG1CihsqNiPRZzYFG2v54OSMdVRyIySbliod112/ppSgjgZ9deiIOA554t4xbGy/ALJoDnhb4yzW6PYMMCP1UEpE+ae3oZNd9C5kReJ924jC++CgkZtodSwahiyYX8KuFU3AY8Ni7lfw08TuYidlQvQme/gYEAnZHlAinciMin8rr87Hxt19ijvdNuoilZcEfSRsz2+5YMoh9bkohd3/BKjgPlHSxMu9HmE4XbP0nrFludzyJcCo3InJEZiDAm/d9ndPaXsRnOqiYfx+5U86xO5YMAQumFvLLL0zGYcDPNqfydGHw1OJXf64FxhJWKjciclimabLuof/ltIanANg+ezkjT/2CzalkKLl46jB++YXJGAb8z46JbBpxtfWFp78J5RvsDScRS+VGRA7J5/Oz9v4bOHn/7wF497hbOOH8b9icSoaii6cO43vnHQfAgh1nU5c/F3yd8PgVOsFYwkLlRkQ+obm9k1d+dRWn1zwKwFtj/4cZC3UQm/TfV08byeWzivCbDs4r/zKd6eOgpRJ+Nw/ee9zueBJhVG5EpJey2kY2/OrznNn2LwKmwebpdzD7yh/YHUuGOMMw+NHnJnLKmExqPW4ubbsZT9Ep4G2Dv38dnr4BPG12x5QIoXIjIj3e31PJ3vsuZp53LV5i2D//Xk646Ft2x5IIEet0cN+V0xmdncjm5ngua7sZz2k3AwaU/Ake/AxUb7Y7pkQAlRsRAeCDjeuIe/gsTjU30Imblov/yPDTrrQ7lkSY1PhYHvryTDISXbxX0crX9s3Hd9UzkJQLtdtg5anwzGJo2m93VBnCVG5Eop1psudfv2Ls0xcxziij0ZFO4Kq/kzH5AruTSYQakZnIg1fPIC7WwZrttfzvu6kEvrYWjrsIzABsXAW/mQYv3Apt9XbHlSFI5UYkmrXVUf/7ixn5zg9wG15K4mbh/tabJIw+xe5kEuGmj0jn/iun43QY/H1jOXe+Wo/5hVVw7Usw4lTwd8G638KvJ8Pau8HbaXdkGUJUbkSiUSAAGx/F85tZZJa/TJcZy5/Sb2DCkn8Tn55ndzqJEmdMyOEXnz8RgN+/tocHXt0NRTPhy8/Cl/4KeSda96Ra/UP47UzY9BSYps2pZShQuRGJNvvXY/5+PjzzTVxd9WwPDOOnw+7lCzfcQZwrxu50EmUumTaM2y6wzsD56b+3sWpdKSbAmPnwtVfg4gcguQCa9sFfr4X/Owv2vWlrZhn8DNOMrhrc3NxMamoqTU1NpKSk2B1HZOA07YeX74QS6+yaVjOOX/suoXHSV7jzsunEOvV3HbHPT/+9jZWv7ALg9HHZ/GTBRIoyEqwvetqtKarXfgXeduva8JPhlBth7Nm6M32UOJo/v1VuRCKZacLe1+GtB2Dbv8D0A/CU/3R+ZV7ODRedyuWzijAMw+agEu1M0+S+Nbv49Us78fgDxMU6+J/547j21JHEdBfv5kpYcyeU/BkCXuta9gQ4+Vtw/AJwJ9mWX8JP5eYIVG4k4nnaoWYrlK+H9Q9DzUfnhrwZOI6feb9IU+YUfnvFNI4v0H8DMrjsrm3le3/fxJu7GwA4Lj+FG84YzdnH5+GK6S45FfDm/fDuH6w1OQBOFwyfA2PPgjFnQfZ4UGmPKCo3R6ByIxGjswnqdwU/PrTOCKn+wPqcj/6z9hhx/NV3Cg/7zmK7OZwFUwq44+JJJLm1vkYGJ9M0eWr9fn7y3FYa260RmoxEF5+fPowvzixiVHZwhKazySrw7z4EB0p7P0lKIYw42So8I06GrPGavhriVG6OQOVGhqym/bB7jfVR+pp1X57D6HRlsttZzN9aJvAX31yaSWLWyAy+OW80c8dlaxpKhoT61i4efqOUv7xbRnVzV8/1yUVpnDYmi1PGZDFtRBpup8Mq+DtfhA9fhNLXra3kB4vPgJzjIWMkZIyyPlIKwHCCARgO68Ppsj5i4qwPVwLExg/sG5dDGnLl5t577+UXv/gFVVVVTJ48mXvuuYdZs2Yd9vFPPvkk3//+9yktLWXs2LH87Gc/4/zzz+/Ta6ncSMiYJnQ1Q8cB6GgETytggMNp/cB0OCA2EeLTrY8YV9+f29dlHUNfWQIVG2HvG9YP749LysWXNoq6uCJ2+PJY3ZDNc7VZ1JqpPQ+Zf1wO188bzfQRGcf6jkVs4fMHWLO9lj+/vY+Xt9cQOOhPrbhYBzOLMxiZlUh+ajz5qXEUJJoUd2wmq349jrJ1UPYO+Dr6H8CVDEk5kJxn/ZpSCGnDe3+4k4/9jcoRDaly88QTT3D11VezcuVKZs+ezYoVK3jyySfZvn07OTk5n3j8G2+8wemnn87y5cu58MILeeyxx/jZz37Ghg0bmDhx4qe+nsqN9FlnszXUfaAUGvdZIydNZdavzeXQXm+dptpHXUY8bY4kOhwJdBgJdDgS6DQSwOEkweEl3vARZ3iID7SR2roHh+nt9f2m4aAxfRL70mazNW4ar7UVsLHaT3njJ39oTyxM4YzxOVx4YgHj8/RDVyJHdXMnr+yo5fUP63j9w3rqWrsO+9hYp8Gw9ARGpscyK76Csc4qCgKVZHnKSW4vw9VZB5gYZgAwwQxg+D3g84Cv86NFy30RlwopwyC1EFKHQWIOxKWAO+WjX93J4EqyFj67kqzPHc5j/j2JFkOq3MyePZuZM2fy29/+FoBAIEBRURHf+ta3uOWWWz7x+IULF9LW1sazzz7bc+2kk05iypQprFy58lNfL1zlpqm5lZJt24lxGDgcBk7DwOmwPmKcBjGGA6fTINZp4MDAMAwMAxzB2YGQzRKYJj3rLczgf7Sm3/qPNuAD04/h94K/C8PfheH3WB/eDgzfQR+edhyeFgxvKw5PK4anNfi1zp4PADM2HjM2ATMmATM2gUBcGv74DAJxGQTiMwm4U4Nfj//oV4cz+IYN61fThIAPI+C1Mvq9GN42HJ4WHF3NGJ5WHJ5mjK4WHJ5mK1dXi/VDCPOg92xYrxGbQCA2ETM2kYArycrmSiTQndM0MQN+zIAPAn5MTxtGex2O9jqcHXXEdNThbi3H5TnQp99yj+GimSRazDgCARMnARwEcBgmiXSSShsO4+j/MztgJrEpMJL3zVG8FxjNm4HjaSHhkI8dkZnAxMJU5o7LZt64bHJS4o769USGGtM02VHdyrt7Gyg/0EFVUyeVTZ1UNnVQ0diJx9/3v3x0czkduGKsjzgnZMR6GeFuYbirlYKYZvIcTWQHakj3VpHaWUliRwWxnsZ+vwdfbBI+Vyre2GS8san4XUn4YlMIuJJ7PgxXPMQm4HAl4IiNx4iJBQwMhwPDcGI4DOtnvBmw/sJlBjAMA7N7qs1wgsOJaTjBGYvpiMF0xAZHmR2A46DHfuwPI9P86LkJPn/AjxHwHvSrz/p6wG/9GWMGcCakkT7h9H7/vhzK0fz5beuKQo/Hw/r161m6dGnPNYfDwfz581m3bt0hv2fdunUsWbKk17VzzjmHp59++pCP7+rqoqvro2bf3Nx87MEPoXLHW8x97pKwPLfYp85MoczMYb+ZRbmZTbmZSYWZSaWZSa2ZSjOJdNF7uiku1kFBajz5aXFkJrpJi3OS5eok29lOuqODOH8bMf42Yn2tuHxt+P0+WvwxNHudNHudNHhj+NAsZLc3k5YuPy2dXkwgJ9HFcYluMpNcZCS6GJOTxPH5KRxXkEJKXKw9v0EiNjIMg/F5yYccnfQHTKqaO9lb38a++nb2NrRT1dRJVVMn1c2dVDV30u7xf+L7PP6AVYqCf2xUAB+QBCQBhz69O5EO8o16Co168o16Cow6Mmgh2eggmXaSjXaS6SDJ6CCRThLpwGVYrx3jbSXG20qk/XVkW+xxpN9q32GLtpaburo6/H4/ubm5va7n5uaybdu2Q35PVVXVIR9fVVV1yMcvX76cH/7wh6EJfARxsTGf+EPu48yD/pcQjZeZGIf83Hp6AxMDnzWOgC84puAlBg8xeIili1g8xNJpuujATRcuOnHRRhytxNNmxlu/Ek9n8GtdZmzPe42ni3iji3i6SAiOUqQbzWTQQjotpBitxOMhji7i8RBPF06sIWAHJkYwqZcYfDjxEoMXJ+3B1281E6xfSaAl+HkL1rUuXJg9z2StCYynkwQ6SaCLRDpINLo/7yQxmBPAb3SPrzjpMtw0OVJpcqTT7Eyj1ZlGfUwOdTH5eGMSrRE4w/job3MxDiY4HcxOiCU72U1OchzZyW6yk9wUpMWRGh+rBbsiNnM6DArT4ilMi+fk0Z/8ummaeP0m/oCJLxDAH7A+9/oDeHwBunzWr20eH43tXg60e2hosz6aO7w0d3pp7vDR3OmltSuBdl86WwKjed8fwOc3e37E9/wkMCDW6cDpMIh1GMQ5fKQaHaQZbaQ6OkillRTaSAi0EWe2ER/o/mgn1vTgNjtxmV24zS4cBKyf7qaJQQAHJv6eqx/9PDQI4AxejcGPkwAx+Igh0PN59/dbH4ce6Qp89IoEMPDjxI8DL87gP1ufd38EcFAbM5wJof/X2mcRvxd06dKlvUZ6mpubKSoqCvnrFE+eC5NrQ/68IiISeoZh4Irprh5a9xJpbC03WVlZOJ1Oqqure12vrq4mL+/Qw395eXlH9Xi3243b7Q5NYBERERn0bD3RyOVyMX36dFavXt1zLRAIsHr1aubMmXPI75kzZ06vxwO8+OKLh328iIiIRBfbp6WWLFnCNddcw4wZM5g1axYrVqygra2NRYsWAXD11VdTWFjI8uXLAbjxxhuZO3cuv/zlL7ngggt4/PHHeffdd/nd735n59sQERGRQcL2crNw4UJqa2u5/fbbqaqqYsqUKTz//PM9i4b37duH46Ajs08++WQee+wxbrvtNr73ve8xduxYnn766T6dcSMiIiKRz/ZzbgaaDvETEREZeo7mz2/dRUxEREQiisqNiIiIRBSVGxEREYkoKjciIiISUVRuREREJKKo3IiIiEhEUbkRERGRiKJyIyIiIhFF5UZEREQiiu23Xxho3QcyNzc325xERERE+qr7z+2+3Fgh6spNS0sLAEVFRTYnERERkaPV0tJCamrqER8TdfeWCgQCVFRUkJycjGEYIX/+5uZmioqKKCsri7p7V0Xre4/W9w1673rveu/RxO73bpomLS0tFBQU9Lqh9qFE3ciNw+Fg2LBhYX+dlJSUqPs/frdofe/R+r5B713vPfrovdvz3j9txKabFhSLiIhIRFG5ERERkYiichNibrebZcuW4Xa77Y4y4KL1vUfr+wa9d713vfdoMpTee9QtKBYREZHIppEbERERiSgqNyIiIhJRVG5EREQkoqjciIiISERRuQmjHTt28LnPfY6srCxSUlI49dRTefnll+2ONWD+9a9/MXv2bOLj40lPT2fBggV2RxpQXV1dTJkyBcMwKCkpsTtO2JWWlnLttdcycuRI4uPjGT16NMuWLcPj8dgdLSzuvfdeiouLiYuLY/bs2bz99tt2Rwq75cuXM3PmTJKTk8nJyWHBggVs377d7lgD7qc//SmGYXDTTTfZHWVAlJeX86UvfYnMzEzi4+OZNGkS7777rt2xjkjlJowuvPBCfD4f//3vf1m/fj2TJ0/mwgsvpKqqyu5oYffXv/6Vq666ikWLFvHee+/x+uuvc8UVV9gda0B997vfpaCgwO4YA2bbtm0EAgEeeOABNm/ezK9+9StWrlzJ9773PbujhdwTTzzBkiVLWLZsGRs2bGDy5Mmcc8451NTU2B0trF555RVuuOEG3nzzTV588UW8Xi9nn302bW1tdkcbMO+88w4PPPAAJ554ot1RBsSBAwc45ZRTiI2N5d///jdbtmzhl7/8Jenp6XZHOzJTwqK2ttYEzFdffbXnWnNzswmYL774oo3Jws/r9ZqFhYXm73//e7uj2Oa5554zJ0yYYG7evNkEzI0bN9odyRY///nPzZEjR9odI+RmzZpl3nDDDT2f+/1+s6CgwFy+fLmNqQZeTU2NCZivvPKK3VEGREtLizl27FjzxRdfNOfOnWveeOONdkcKu5tvvtk89dRT7Y5x1DRyEyaZmZmMHz+eP/7xj7S1teHz+XjggQfIyclh+vTpdscLqw0bNlBeXo7D4WDq1Knk5+dz3nnn8cEHH9gdbUBUV1dz3XXXsWrVKhISEuyOY6umpiYyMjLsjhFSHo+H9evXM3/+/J5rDoeD+fPns27dOhuTDbympiaAiPt3fDg33HADF1xwQa9/95HuH//4BzNmzOCyyy4jJyeHqVOn8uCDD9od61Op3ISJYRi89NJLbNy4keTkZOLi4rj77rt5/vnnB/9w3jHavXs3AD/4wQ+47bbbePbZZ0lPT2fevHk0NDTYnC68TNPky1/+Mt/4xjeYMWOG3XFs9eGHH3LPPffw9a9/3e4oIVVXV4ff7yc3N7fX9dzc3KiYcu4WCAS46aabOOWUU5g4caLdccLu8ccfZ8OGDSxfvtzuKANq9+7d3H///YwdO5YXXniB66+/nm9/+9s88sgjdkc7IpWbo3TLLbdgGMYRP7Zt24Zpmtxwww3k5OSwdu1a3n77bRYsWMBFF11EZWWl3W+jX/r63gOBAAC33norl156KdOnT+cPf/gDhmHw5JNP2vwu+qev7/2ee+6hpaWFpUuX2h05ZPr63g9WXl7Oueeey2WXXcZ1111nU3IJpxtuuIEPPviAxx9/3O4oYVdWVsaNN97Io48+SlxcnN1xBlQgEGDatGnceeedTJ06la997Wtcd911rFy50u5oR6TbLxyl2tpa6uvrj/iYUaNGsXbtWs4++2wOHDjQ69bwY8eO5dprr+WWW24Jd9SQ6+t7f/311/nMZz7D2rVrOfXUU3u+Nnv2bObPn89PfvKTcEcNub6+9y984Qv885//xDCMnut+vx+n08mVV1456P+2cyh9fe8ulwuAiooK5s2bx0knncTDDz+MwxFZf4fyeDwkJCTw1FNP9doBeM0119DY2MgzzzxjX7gBsnjxYp555hleffVVRo4caXecsHv66ae5+OKLcTqdPdf8fj+GYeBwOOjq6ur1tUgyYsQIzjrrLH7/+9/3XLv//vu54447KC8vtzHZkcXYHWCoyc7OJjs7+1Mf197eDvCJH+wOh6NnZGOo6et7nz59Om63m+3bt/eUG6/XS2lpKSNGjAh3zLDo63v/zW9+wx133NHzeUVFBeeccw5PPPEEs2fPDmfEsOnrewdrxOaMM87oGa2LtGID4HK5mD59OqtXr+4pN4FAgNWrV7N48WJ7w4WZaZp861vf4u9//ztr1qyJimIDcOaZZ7Jp06Ze1xYtWsSECRO4+eabI7bYAJxyyimf2O6/Y8eOwf+z3NblzBGstrbWzMzMNC+55BKzpKTE3L59u/md73zHjI2NNUtKSuyOF3Y33nijWVhYaL7wwgvmtm3bzGuvvdbMyckxGxoa7I42oPbs2RM1u6X2799vjhkzxjzzzDPN/fv3m5WVlT0fkebxxx833W63+fDDD5tbtmwxv/a1r5lpaWlmVVWV3dHC6vrrrzdTU1PNNWvW9Pr3297ebne0ARctu6XefvttMyYmxvzJT35i7ty503z00UfNhIQE809/+pPd0Y5I5SaM3nnnHfPss882MzIyzOTkZPOkk04yn3vuObtjDQiPx2P+v//3/8ycnBwzOTnZnD9/vvnBBx/YHWvARVO5+cMf/mACh/yIRPfcc485fPhw0+VymbNmzTLffPNNuyOF3eH+/f7hD3+wO9qAi5ZyY5qm+c9//tOcOHGi6Xa7zQkTJpi/+93v7I70qbTmRkRERCJK5E2Ii4iISFRTuREREZGIonIjIiIiEUXlRkRERCKKyo2IiIhEFJUbERERiSgqNyIiIhJRVG5EREQkoqjciIiISERRuREREZGIonIjIiIiEUXlRkSGvNraWvLy8rjzzjt7rr3xxhu4XC5Wr15tYzIRsYNunCkiEeG5555jwYIFvPHGG4wfP54pU6bwuc99jrvvvtvuaCIywFRuRCRi3HDDDbz00kvMmDGDTZs28c477+B2u+2OJSIDTOVGRCJGR0cHEydOpKysjPXr1zNp0iS7I4mIDbTmRkQixq5du6ioqCAQCFBaWmp3HBGxiUZuRCQieDweZs2axZQpUxg/fjwrVqxg06ZN5OTk2B1NRAaYyo2IRIT//d//5amnnuK9994jKSmJuXPnkpqayrPPPmt3NBEZYJqWEpEhb82aNaxYsYJVq1aRkpKCw+Fg1apVrF27lvvvv9/ueCIywDRyIyIiIhFFIzciIiISUVRuREREJKKo3IiIiEhEUbkRERGRiKJyIyIiIhFF5UZEREQiisqNiIiIRBSVGxEREYkoKjciIiISUVRuREREJKKo3IiIiEhE+f+gUbiT1o4xOQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# TODO: Make code that automatically slices these up.\n",
    "theta_test = jnp.expand_dims(X[:, X.shape[1] // 2], -1)\n",
    "# d_test = jnp.expand_dims(X[:, 4], -1)\n",
    "d_test = X[:, -len_x:-len_xi]\n",
    "xi_test = X[:, -len_xi:]\n",
    "# xi_test = jnp.expand_dims(X[:, 2], -1)\n",
    "# xi_test = jnp.ones((10000, 1)) * 3\n",
    "# xi_test = jnp.expand_dims(X[:, 1], -1)\n",
    "\n",
    "\n",
    "samples = model_sample.apply(params, \n",
    "                    next(prng_seq),\n",
    "                    num_samples=len(theta_test),\n",
    "                    theta=theta_test,\n",
    "                    d=d_test,\n",
    "                    # d=d_obs,\n",
    "                    xi=xi_test)\n",
    "\n",
    "\n",
    "density_1 = gaussian_kde(samples[:, 0])\n",
    "density_2 = gaussian_kde(samples[:, 1])\n",
    "\n",
    "\n",
    "# Plot the density\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(jnp.min(samples), jnp.max(samples), 100)\n",
    "ax.plot(x, density_1(x), label='x1')\n",
    "ax.plot(x, density_2(x), label='x2')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular ACE\n",
    "Really just copy/paste of AF's implementation in `pyro`. Since the current `pyro` release doesn't have some of AF's code from SGBOED, have to copy/paste some of his custom functions here.\n",
    "\n",
    "Going to compare the lower/upper bounds of SGBOED on a simple linear regression task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "0\n",
      "eig tensor(2.2699)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "1\n",
      "eig tensor(1.3538)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "2\n",
      "eig tensor(1.0857)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "3\n",
      "eig tensor(-0.7965)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "4\n",
      "eig tensor(1.3518)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "5\n",
      "eig tensor(-0.1831)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "6\n",
      "eig tensor(0.8764)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "7\n",
      "eig tensor(1.7372)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "8\n",
      "eig tensor(2.0288)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "9\n",
      "eig tensor(2.0209)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "10\n",
      "eig tensor(1.4414)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "11\n",
      "eig tensor(1.3914)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "12\n",
      "eig tensor(-0.4504)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "13\n",
      "eig tensor(0.1036)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "14\n",
      "eig tensor(0.9313)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "15\n",
      "eig tensor(0.6978)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "16\n",
      "eig tensor(2.3113)\n",
      "tensor([-0.0066], grad_fn=<SelectBackward0>)\n",
      "17\n",
      "eig tensor(1.8593)\n",
      "tensor([-0.0061], grad_fn=<SelectBackward0>)\n",
      "18\n",
      "eig tensor(1.4697)\n",
      "tensor([-0.0055], grad_fn=<SelectBackward0>)\n",
      "19\n",
      "eig tensor(0.6198)\n",
      "tensor([-0.0049], grad_fn=<SelectBackward0>)\n",
      "20\n",
      "eig tensor(0.2874)\n",
      "tensor([-0.0044], grad_fn=<SelectBackward0>)\n",
      "21\n",
      "eig tensor(2.4385)\n",
      "tensor([-0.0039], grad_fn=<SelectBackward0>)\n",
      "22\n",
      "eig tensor(1.2628)\n",
      "tensor([-0.0034], grad_fn=<SelectBackward0>)\n",
      "23\n",
      "eig tensor(2.2122)\n",
      "tensor([-0.0029], grad_fn=<SelectBackward0>)\n",
      "24\n",
      "eig tensor(1.9329)\n",
      "tensor([-0.0025], grad_fn=<SelectBackward0>)\n",
      "25\n",
      "eig tensor(1.9115)\n",
      "tensor([-0.0021], grad_fn=<SelectBackward0>)\n",
      "26\n",
      "eig tensor(1.9007)\n",
      "tensor([-0.0015], grad_fn=<SelectBackward0>)\n",
      "27\n",
      "eig tensor(2.1512)\n",
      "tensor([-0.0009], grad_fn=<SelectBackward0>)\n",
      "28\n",
      "eig tensor(2.7386)\n",
      "tensor([-0.0004], grad_fn=<SelectBackward0>)\n",
      "29\n",
      "eig tensor(1.9571)\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>)\n",
      "30\n",
      "eig tensor(2.6530)\n",
      "tensor([0.0008], grad_fn=<SelectBackward0>)\n",
      "31\n",
      "eig tensor(2.4668)\n",
      "tensor([0.0014], grad_fn=<SelectBackward0>)\n",
      "32\n",
      "eig tensor(1.8890)\n",
      "tensor([0.0019], grad_fn=<SelectBackward0>)\n",
      "33\n",
      "eig tensor(2.7383)\n",
      "tensor([0.0022], grad_fn=<SelectBackward0>)\n",
      "34\n",
      "eig tensor(2.2506)\n",
      "tensor([0.0024], grad_fn=<SelectBackward0>)\n",
      "35\n",
      "eig tensor(2.1409)\n",
      "tensor([0.0026], grad_fn=<SelectBackward0>)\n",
      "36\n",
      "eig tensor(3.0385)\n",
      "tensor([0.0028], grad_fn=<SelectBackward0>)\n",
      "37\n",
      "eig tensor(2.5979)\n",
      "tensor([0.0030], grad_fn=<SelectBackward0>)\n",
      "38\n",
      "eig tensor(2.4799)\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "39\n",
      "eig tensor(2.6393)\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "40\n",
      "eig tensor(2.9287)\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "41\n",
      "eig tensor(2.4400)\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "42\n",
      "eig tensor(2.3881)\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "43\n",
      "eig tensor(2.4209)\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "44\n",
      "eig tensor(2.5852)\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "45\n",
      "eig tensor(2.5280)\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "46\n",
      "eig tensor(1.9943)\n",
      "tensor([0.0030], grad_fn=<SelectBackward0>)\n",
      "47\n",
      "eig tensor(2.1834)\n",
      "tensor([0.0030], grad_fn=<SelectBackward0>)\n",
      "48\n",
      "eig tensor(2.5161)\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "49\n",
      "eig tensor(1.8794)\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "50\n",
      "eig tensor(2.6235)\n",
      "tensor([0.0035], grad_fn=<SelectBackward0>)\n",
      "51\n",
      "eig tensor(2.4932)\n",
      "tensor([0.0038], grad_fn=<SelectBackward0>)\n",
      "52\n",
      "eig tensor(2.6778)\n",
      "tensor([0.0040], grad_fn=<SelectBackward0>)\n",
      "53\n",
      "eig tensor(2.1564)\n",
      "tensor([0.0042], grad_fn=<SelectBackward0>)\n",
      "54\n",
      "eig tensor(2.7448)\n",
      "tensor([0.0044], grad_fn=<SelectBackward0>)\n",
      "55\n",
      "eig tensor(2.4631)\n",
      "tensor([0.0046], grad_fn=<SelectBackward0>)\n",
      "56\n",
      "eig tensor(2.8219)\n",
      "tensor([0.0046], grad_fn=<SelectBackward0>)\n",
      "57\n",
      "eig tensor(2.1745)\n",
      "tensor([0.0045], grad_fn=<SelectBackward0>)\n",
      "58\n",
      "eig tensor(2.9364)\n",
      "tensor([0.0044], grad_fn=<SelectBackward0>)\n",
      "59\n",
      "eig tensor(2.5219)\n",
      "tensor([0.0043], grad_fn=<SelectBackward0>)\n",
      "60\n",
      "eig tensor(3.0210)\n",
      "tensor([0.0042], grad_fn=<SelectBackward0>)\n",
      "61\n",
      "eig tensor(2.9381)\n",
      "tensor([0.0041], grad_fn=<SelectBackward0>)\n",
      "62\n",
      "eig tensor(2.4062)\n",
      "tensor([0.0040], grad_fn=<SelectBackward0>)\n",
      "63\n",
      "eig tensor(2.7316)\n",
      "tensor([0.0040], grad_fn=<SelectBackward0>)\n",
      "64\n",
      "eig tensor(2.2108)\n",
      "tensor([0.0039], grad_fn=<SelectBackward0>)\n",
      "65\n",
      "eig tensor(2.6323)\n",
      "tensor([0.0038], grad_fn=<SelectBackward0>)\n",
      "66\n",
      "eig tensor(2.3714)\n",
      "tensor([0.0038], grad_fn=<SelectBackward0>)\n",
      "67\n",
      "eig tensor(2.7313)\n",
      "tensor([0.0037], grad_fn=<SelectBackward0>)\n",
      "68\n",
      "eig tensor(2.8404)\n",
      "tensor([0.0037], grad_fn=<SelectBackward0>)\n",
      "69\n",
      "eig tensor(2.2692)\n",
      "tensor([0.0036], grad_fn=<SelectBackward0>)\n",
      "70\n",
      "eig tensor(2.3601)\n",
      "tensor([0.0036], grad_fn=<SelectBackward0>)\n",
      "71\n",
      "eig tensor(2.1339)\n",
      "tensor([0.0036], grad_fn=<SelectBackward0>)\n",
      "72\n",
      "eig tensor(2.3693)\n",
      "tensor([0.0036], grad_fn=<SelectBackward0>)\n",
      "73\n",
      "eig tensor(2.5218)\n",
      "tensor([0.0035], grad_fn=<SelectBackward0>)\n",
      "74\n",
      "eig tensor(2.4095)\n",
      "tensor([0.0035], grad_fn=<SelectBackward0>)\n",
      "75\n",
      "eig tensor(2.6550)\n",
      "tensor([0.0035], grad_fn=<SelectBackward0>)\n",
      "76\n",
      "eig tensor(2.5843)\n",
      "tensor([0.0034], grad_fn=<SelectBackward0>)\n",
      "77\n",
      "eig tensor(3.0500)\n",
      "tensor([0.0034], grad_fn=<SelectBackward0>)\n",
      "78\n",
      "eig tensor(2.8967)\n",
      "tensor([0.0030], grad_fn=<SelectBackward0>)\n",
      "79\n",
      "eig tensor(2.8753)\n",
      "tensor([0.0028], grad_fn=<SelectBackward0>)\n",
      "80\n",
      "eig tensor(2.5199)\n",
      "tensor([0.0022], grad_fn=<SelectBackward0>)\n",
      "81\n",
      "eig tensor(2.9723)\n",
      "tensor([0.0018], grad_fn=<SelectBackward0>)\n",
      "82\n",
      "eig tensor(2.3973)\n",
      "tensor([0.0013], grad_fn=<SelectBackward0>)\n",
      "83\n",
      "eig tensor(3.5052)\n",
      "tensor([0.0009], grad_fn=<SelectBackward0>)\n",
      "84\n",
      "eig tensor(3.0829)\n",
      "tensor([0.0005], grad_fn=<SelectBackward0>)\n",
      "85\n",
      "eig tensor(3.1051)\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>)\n",
      "86\n",
      "eig tensor(3.7133)\n",
      "tensor([-7.4018e-05], grad_fn=<SelectBackward0>)\n",
      "87\n",
      "eig tensor(2.9560)\n",
      "tensor([-0.0010], grad_fn=<SelectBackward0>)\n",
      "88\n",
      "eig tensor(2.8322)\n",
      "tensor([-0.0019], grad_fn=<SelectBackward0>)\n",
      "89\n",
      "eig tensor(2.3829)\n",
      "tensor([-0.0026], grad_fn=<SelectBackward0>)\n",
      "90\n",
      "eig tensor(3.5666)\n",
      "tensor([-0.0034], grad_fn=<SelectBackward0>)\n",
      "91\n",
      "eig tensor(2.9882)\n",
      "tensor([-0.0040], grad_fn=<SelectBackward0>)\n",
      "92\n",
      "eig tensor(2.8684)\n",
      "tensor([-0.0046], grad_fn=<SelectBackward0>)\n",
      "93\n",
      "eig tensor(2.7576)\n",
      "tensor([-0.0051], grad_fn=<SelectBackward0>)\n",
      "94\n",
      "eig tensor(2.9008)\n",
      "tensor([-0.0056], grad_fn=<SelectBackward0>)\n",
      "95\n",
      "eig tensor(2.7886)\n",
      "tensor([-0.0060], grad_fn=<SelectBackward0>)\n",
      "96\n",
      "eig tensor(2.7624)\n",
      "tensor([-0.0064], grad_fn=<SelectBackward0>)\n",
      "97\n",
      "eig tensor(3.0106)\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "98\n",
      "eig tensor(2.9636)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "99\n",
      "eig tensor(3.1000)\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "100\n",
      "eig tensor(1.8559)\n",
      "tensor([-0.0076], grad_fn=<SelectBackward0>)\n",
      "101\n",
      "eig tensor(2.8431)\n",
      "tensor([-0.0079], grad_fn=<SelectBackward0>)\n",
      "102\n",
      "eig tensor(2.7093)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "103\n",
      "eig tensor(2.9526)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "104\n",
      "eig tensor(2.9919)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "105\n",
      "eig tensor(3.2389)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "106\n",
      "eig tensor(2.5965)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "107\n",
      "eig tensor(3.0923)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "108\n",
      "eig tensor(2.3435)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "109\n",
      "eig tensor(2.5613)\n",
      "tensor([-0.0093], grad_fn=<SelectBackward0>)\n",
      "110\n",
      "eig tensor(3.2890)\n",
      "tensor([-0.0094], grad_fn=<SelectBackward0>)\n",
      "111\n",
      "eig tensor(2.0018)\n",
      "tensor([-0.0094], grad_fn=<SelectBackward0>)\n",
      "112\n",
      "eig tensor(2.6557)\n",
      "tensor([-0.0095], grad_fn=<SelectBackward0>)\n",
      "113\n",
      "eig tensor(3.2228)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "114\n",
      "eig tensor(2.5649)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "115\n",
      "eig tensor(2.9954)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "116\n",
      "eig tensor(2.8821)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "117\n",
      "eig tensor(2.5248)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "118\n",
      "eig tensor(2.7606)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "119\n",
      "eig tensor(3.0253)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "120\n",
      "eig tensor(3.0845)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "121\n",
      "eig tensor(1.9567)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "122\n",
      "eig tensor(3.5613)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "123\n",
      "eig tensor(3.7269)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "124\n",
      "eig tensor(2.3912)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "125\n",
      "eig tensor(2.9403)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "126\n",
      "eig tensor(3.1286)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "127\n",
      "eig tensor(2.9060)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "128\n",
      "eig tensor(2.1788)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "129\n",
      "eig tensor(3.3944)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "130\n",
      "eig tensor(3.3249)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "131\n",
      "eig tensor(3.0462)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "132\n",
      "eig tensor(2.0952)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "133\n",
      "eig tensor(3.0286)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "134\n",
      "eig tensor(3.4656)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "135\n",
      "eig tensor(2.8746)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "136\n",
      "eig tensor(2.5940)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "137\n",
      "eig tensor(3.1724)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "138\n",
      "eig tensor(3.7248)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "139\n",
      "eig tensor(2.7237)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "140\n",
      "eig tensor(2.9168)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "141\n",
      "eig tensor(2.5784)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "142\n",
      "eig tensor(3.3708)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "143\n",
      "eig tensor(3.4063)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "144\n",
      "eig tensor(2.7827)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "145\n",
      "eig tensor(2.9726)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "146\n",
      "eig tensor(2.3603)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "147\n",
      "eig tensor(3.0436)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "148\n",
      "eig tensor(3.3989)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "149\n",
      "eig tensor(2.6643)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "150\n",
      "eig tensor(2.7248)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "151\n",
      "eig tensor(3.0270)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "152\n",
      "eig tensor(2.6241)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "153\n",
      "eig tensor(3.6486)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "154\n",
      "eig tensor(3.0882)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "155\n",
      "eig tensor(3.5226)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "156\n",
      "eig tensor(2.6946)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "157\n",
      "eig tensor(3.2243)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "158\n",
      "eig tensor(3.5253)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "159\n",
      "eig tensor(3.8104)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "160\n",
      "eig tensor(3.1051)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "161\n",
      "eig tensor(2.9290)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "162\n",
      "eig tensor(3.1150)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "163\n",
      "eig tensor(3.5002)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "164\n",
      "eig tensor(3.1210)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "165\n",
      "eig tensor(3.1390)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "166\n",
      "eig tensor(3.1953)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "167\n",
      "eig tensor(3.6158)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "168\n",
      "eig tensor(2.4745)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "169\n",
      "eig tensor(3.0942)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "170\n",
      "eig tensor(2.4319)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "171\n",
      "eig tensor(3.4400)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "172\n",
      "eig tensor(3.2619)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "173\n",
      "eig tensor(1.5573)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "174\n",
      "eig tensor(2.7736)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "175\n",
      "eig tensor(2.9254)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "176\n",
      "eig tensor(3.0353)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "177\n",
      "eig tensor(2.3331)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "178\n",
      "eig tensor(2.5136)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "179\n",
      "eig tensor(2.4429)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "180\n",
      "eig tensor(3.0579)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "181\n",
      "eig tensor(2.8077)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "182\n",
      "eig tensor(2.1670)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "183\n",
      "eig tensor(2.4760)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "184\n",
      "eig tensor(2.4025)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "185\n",
      "eig tensor(3.1577)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "186\n",
      "eig tensor(3.4835)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "187\n",
      "eig tensor(2.8135)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "188\n",
      "eig tensor(3.0285)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "189\n",
      "eig tensor(2.2568)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "190\n",
      "eig tensor(3.1784)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "191\n",
      "eig tensor(3.3238)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "192\n",
      "eig tensor(3.4659)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "193\n",
      "eig tensor(3.3157)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "194\n",
      "eig tensor(2.6289)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "195\n",
      "eig tensor(3.2662)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "196\n",
      "eig tensor(2.8978)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "197\n",
      "eig tensor(2.4889)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "198\n",
      "eig tensor(3.4292)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "199\n",
      "eig tensor(2.2367)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "200\n",
      "eig tensor(3.2262)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "201\n",
      "eig tensor(2.5075)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "202\n",
      "eig tensor(2.3691)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "203\n",
      "eig tensor(2.7909)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "204\n",
      "eig tensor(2.6000)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "205\n",
      "eig tensor(2.8905)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "206\n",
      "eig tensor(3.0457)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "207\n",
      "eig tensor(3.4596)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "208\n",
      "eig tensor(3.3457)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "209\n",
      "eig tensor(3.3955)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "210\n",
      "eig tensor(2.2233)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "211\n",
      "eig tensor(3.0667)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "212\n",
      "eig tensor(3.4976)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "213\n",
      "eig tensor(3.0361)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "214\n",
      "eig tensor(2.6534)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "215\n",
      "eig tensor(2.3739)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "216\n",
      "eig tensor(2.7176)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "217\n",
      "eig tensor(2.7229)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "218\n",
      "eig tensor(3.1144)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "219\n",
      "eig tensor(3.0150)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "220\n",
      "eig tensor(3.2073)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "221\n",
      "eig tensor(2.5819)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "222\n",
      "eig tensor(2.7952)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "223\n",
      "eig tensor(2.5344)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "224\n",
      "eig tensor(3.2632)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "225\n",
      "eig tensor(3.0228)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "226\n",
      "eig tensor(3.2953)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "227\n",
      "eig tensor(2.6617)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "228\n",
      "eig tensor(2.7308)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "229\n",
      "eig tensor(2.5702)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "230\n",
      "eig tensor(3.0610)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "231\n",
      "eig tensor(3.3880)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "232\n",
      "eig tensor(3.7232)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "233\n",
      "eig tensor(3.3736)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "234\n",
      "eig tensor(3.2378)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "235\n",
      "eig tensor(3.1074)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "236\n",
      "eig tensor(3.8308)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "237\n",
      "eig tensor(2.9874)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "238\n",
      "eig tensor(2.4320)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "239\n",
      "eig tensor(3.2754)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "240\n",
      "eig tensor(2.8340)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "241\n",
      "eig tensor(1.7470)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "242\n",
      "eig tensor(3.1408)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "243\n",
      "eig tensor(2.7962)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "244\n",
      "eig tensor(3.8382)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "245\n",
      "eig tensor(3.0400)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "246\n",
      "eig tensor(2.3341)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "247\n",
      "eig tensor(2.8338)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "248\n",
      "eig tensor(2.7190)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "249\n",
      "eig tensor(2.4389)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "250\n",
      "eig tensor(2.4230)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "251\n",
      "eig tensor(2.3990)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "252\n",
      "eig tensor(3.1386)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "253\n",
      "eig tensor(2.9328)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "254\n",
      "eig tensor(2.5369)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "255\n",
      "eig tensor(2.5145)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "256\n",
      "eig tensor(3.1486)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "257\n",
      "eig tensor(3.1843)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "258\n",
      "eig tensor(2.5411)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "259\n",
      "eig tensor(2.7255)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "260\n",
      "eig tensor(3.2658)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "261\n",
      "eig tensor(2.9862)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "262\n",
      "eig tensor(2.5359)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "263\n",
      "eig tensor(2.8373)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "264\n",
      "eig tensor(2.9977)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "265\n",
      "eig tensor(2.2297)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "266\n",
      "eig tensor(2.7986)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "267\n",
      "eig tensor(2.8051)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "268\n",
      "eig tensor(3.4839)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "269\n",
      "eig tensor(2.4259)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "270\n",
      "eig tensor(2.5514)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "271\n",
      "eig tensor(2.6932)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "272\n",
      "eig tensor(3.1509)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "273\n",
      "eig tensor(3.5254)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "274\n",
      "eig tensor(2.9069)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "275\n",
      "eig tensor(3.2540)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "276\n",
      "eig tensor(3.5092)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "277\n",
      "eig tensor(2.9604)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "278\n",
      "eig tensor(3.5012)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "279\n",
      "eig tensor(3.2869)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "280\n",
      "eig tensor(3.0086)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "281\n",
      "eig tensor(2.6494)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "282\n",
      "eig tensor(3.0796)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "283\n",
      "eig tensor(2.5979)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "284\n",
      "eig tensor(2.8208)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "285\n",
      "eig tensor(3.4609)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "286\n",
      "eig tensor(3.0304)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "287\n",
      "eig tensor(3.0941)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "288\n",
      "eig tensor(3.4125)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "289\n",
      "eig tensor(0.2991)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "290\n",
      "eig tensor(3.2893)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "291\n",
      "eig tensor(3.0174)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "292\n",
      "eig tensor(2.7554)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "293\n",
      "eig tensor(3.0508)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "294\n",
      "eig tensor(3.0325)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "295\n",
      "eig tensor(2.9140)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "296\n",
      "eig tensor(3.0885)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "297\n",
      "eig tensor(2.7682)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "298\n",
      "eig tensor(2.3047)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "299\n",
      "eig tensor(3.4065)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "300\n",
      "eig tensor(2.3274)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "301\n",
      "eig tensor(3.2476)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "302\n",
      "eig tensor(3.3563)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "303\n",
      "eig tensor(3.5553)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "304\n",
      "eig tensor(2.8158)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "305\n",
      "eig tensor(3.1501)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "306\n",
      "eig tensor(3.4243)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "307\n",
      "eig tensor(3.8464)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "308\n",
      "eig tensor(3.1700)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "309\n",
      "eig tensor(2.5694)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "310\n",
      "eig tensor(2.9627)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "311\n",
      "eig tensor(3.3022)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "312\n",
      "eig tensor(2.6088)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "313\n",
      "eig tensor(2.8127)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "314\n",
      "eig tensor(3.2851)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "315\n",
      "eig tensor(2.7302)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "316\n",
      "eig tensor(2.9214)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "317\n",
      "eig tensor(2.9872)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "318\n",
      "eig tensor(2.2663)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "319\n",
      "eig tensor(3.2645)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "320\n",
      "eig tensor(3.7101)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "321\n",
      "eig tensor(3.2214)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "322\n",
      "eig tensor(2.4896)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "323\n",
      "eig tensor(3.5265)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "324\n",
      "eig tensor(2.6397)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "325\n",
      "eig tensor(3.2275)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "326\n",
      "eig tensor(3.4085)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "327\n",
      "eig tensor(3.1053)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "328\n",
      "eig tensor(3.2713)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "329\n",
      "eig tensor(3.2852)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "330\n",
      "eig tensor(2.4933)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "331\n",
      "eig tensor(4.2791)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "332\n",
      "eig tensor(2.6207)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "333\n",
      "eig tensor(2.5591)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "334\n",
      "eig tensor(3.4023)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "335\n",
      "eig tensor(2.7203)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "336\n",
      "eig tensor(3.1471)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "337\n",
      "eig tensor(2.8317)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "338\n",
      "eig tensor(2.6453)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "339\n",
      "eig tensor(3.3394)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "340\n",
      "eig tensor(2.9882)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "341\n",
      "eig tensor(3.0156)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "342\n",
      "eig tensor(3.6048)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "343\n",
      "eig tensor(3.2268)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "344\n",
      "eig tensor(2.8865)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "345\n",
      "eig tensor(3.0832)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "346\n",
      "eig tensor(3.0688)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "347\n",
      "eig tensor(2.8071)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "348\n",
      "eig tensor(3.3329)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "349\n",
      "eig tensor(3.0409)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "350\n",
      "eig tensor(3.5115)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "351\n",
      "eig tensor(3.5755)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "352\n",
      "eig tensor(3.2306)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "353\n",
      "eig tensor(2.2760)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "354\n",
      "eig tensor(3.4480)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "355\n",
      "eig tensor(3.2288)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "356\n",
      "eig tensor(2.4012)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "357\n",
      "eig tensor(3.6703)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "358\n",
      "eig tensor(3.1157)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "359\n",
      "eig tensor(3.5245)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "360\n",
      "eig tensor(3.4470)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "361\n",
      "eig tensor(2.9641)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "362\n",
      "eig tensor(2.7436)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "363\n",
      "eig tensor(3.1585)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "364\n",
      "eig tensor(3.4895)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "365\n",
      "eig tensor(3.6017)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "366\n",
      "eig tensor(3.0740)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "367\n",
      "eig tensor(2.9702)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "368\n",
      "eig tensor(4.1875)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "369\n",
      "eig tensor(3.5456)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "370\n",
      "eig tensor(3.1050)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "371\n",
      "eig tensor(3.3820)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "372\n",
      "eig tensor(3.0477)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "373\n",
      "eig tensor(3.3235)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "374\n",
      "eig tensor(2.6490)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "375\n",
      "eig tensor(3.4626)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "376\n",
      "eig tensor(2.8772)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "377\n",
      "eig tensor(3.4614)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "378\n",
      "eig tensor(3.2150)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "379\n",
      "eig tensor(2.7923)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "380\n",
      "eig tensor(3.1647)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "381\n",
      "eig tensor(3.0879)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "382\n",
      "eig tensor(3.2479)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "383\n",
      "eig tensor(3.0882)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "384\n",
      "eig tensor(3.5846)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "385\n",
      "eig tensor(2.3705)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "386\n",
      "eig tensor(2.5317)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "387\n",
      "eig tensor(2.9823)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "388\n",
      "eig tensor(2.2536)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "389\n",
      "eig tensor(2.9078)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "390\n",
      "eig tensor(3.3033)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "391\n",
      "eig tensor(2.9372)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "392\n",
      "eig tensor(3.3918)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "393\n",
      "eig tensor(3.2989)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "394\n",
      "eig tensor(2.7040)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "395\n",
      "eig tensor(3.1074)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "396\n",
      "eig tensor(3.7815)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "397\n",
      "eig tensor(3.2110)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "398\n",
      "eig tensor(3.5901)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "399\n",
      "eig tensor(3.3937)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "400\n",
      "eig tensor(2.6304)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "401\n",
      "eig tensor(2.7162)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "402\n",
      "eig tensor(3.4560)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "403\n",
      "eig tensor(2.3362)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "404\n",
      "eig tensor(3.1448)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "405\n",
      "eig tensor(2.9088)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "406\n",
      "eig tensor(2.9998)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "407\n",
      "eig tensor(2.8744)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "408\n",
      "eig tensor(3.7476)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "409\n",
      "eig tensor(2.7990)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "410\n",
      "eig tensor(2.7298)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "411\n",
      "eig tensor(2.9339)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "412\n",
      "eig tensor(3.0255)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "413\n",
      "eig tensor(3.7927)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "414\n",
      "eig tensor(2.8306)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "415\n",
      "eig tensor(2.1328)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "416\n",
      "eig tensor(2.2904)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "417\n",
      "eig tensor(2.6848)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "418\n",
      "eig tensor(2.5805)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "419\n",
      "eig tensor(3.3696)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "420\n",
      "eig tensor(3.8529)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "421\n",
      "eig tensor(3.0389)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "422\n",
      "eig tensor(2.6473)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "423\n",
      "eig tensor(3.0817)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "424\n",
      "eig tensor(3.0331)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "425\n",
      "eig tensor(2.6988)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "426\n",
      "eig tensor(2.9361)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "427\n",
      "eig tensor(2.1412)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "428\n",
      "eig tensor(2.9052)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "429\n",
      "eig tensor(3.0046)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "430\n",
      "eig tensor(3.0123)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "431\n",
      "eig tensor(2.8833)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "432\n",
      "eig tensor(3.0698)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "433\n",
      "eig tensor(3.0493)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "434\n",
      "eig tensor(3.3484)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "435\n",
      "eig tensor(2.9379)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "436\n",
      "eig tensor(2.9268)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "437\n",
      "eig tensor(3.5120)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "438\n",
      "eig tensor(3.6652)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "439\n",
      "eig tensor(2.6689)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "440\n",
      "eig tensor(3.2792)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "441\n",
      "eig tensor(3.4409)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "442\n",
      "eig tensor(2.5440)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "443\n",
      "eig tensor(3.3234)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "444\n",
      "eig tensor(3.4040)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "445\n",
      "eig tensor(3.4671)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "446\n",
      "eig tensor(2.6742)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "447\n",
      "eig tensor(2.9144)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "448\n",
      "eig tensor(3.2102)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "449\n",
      "eig tensor(3.9352)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "450\n",
      "eig tensor(2.7262)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "451\n",
      "eig tensor(4.2161)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "452\n",
      "eig tensor(3.0158)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "453\n",
      "eig tensor(3.2537)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "454\n",
      "eig tensor(3.0026)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "455\n",
      "eig tensor(3.2717)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "456\n",
      "eig tensor(3.4830)\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "457\n",
      "eig tensor(3.3779)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "458\n",
      "eig tensor(3.7795)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "459\n",
      "eig tensor(2.9728)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "460\n",
      "eig tensor(3.1504)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "461\n",
      "eig tensor(2.0891)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "462\n",
      "eig tensor(3.4430)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "463\n",
      "eig tensor(3.2713)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "464\n",
      "eig tensor(3.3716)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "465\n",
      "eig tensor(2.8340)\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "466\n",
      "eig tensor(2.5966)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "467\n",
      "eig tensor(3.3914)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "468\n",
      "eig tensor(2.1609)\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "469\n",
      "eig tensor(2.8333)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "470\n",
      "eig tensor(3.1170)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "471\n",
      "eig tensor(3.6621)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "472\n",
      "eig tensor(2.0271)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "473\n",
      "eig tensor(3.1353)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "474\n",
      "eig tensor(2.6406)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "475\n",
      "eig tensor(3.1349)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "476\n",
      "eig tensor(2.5601)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "477\n",
      "eig tensor(3.4620)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "478\n",
      "eig tensor(2.8390)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "479\n",
      "eig tensor(2.6622)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "480\n",
      "eig tensor(2.8731)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "481\n",
      "eig tensor(2.9159)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "482\n",
      "eig tensor(1.8688)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "483\n",
      "eig tensor(2.8804)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "484\n",
      "eig tensor(3.2156)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "485\n",
      "eig tensor(2.9708)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "486\n",
      "eig tensor(2.7639)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "487\n",
      "eig tensor(2.3138)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "488\n",
      "eig tensor(3.9425)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "489\n",
      "eig tensor(2.7083)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "490\n",
      "eig tensor(3.0697)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "491\n",
      "eig tensor(3.2353)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "492\n",
      "eig tensor(3.0102)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "493\n",
      "eig tensor(3.2316)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "494\n",
      "eig tensor(3.1690)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "495\n",
      "eig tensor(2.9993)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "496\n",
      "eig tensor(2.9455)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "497\n",
      "eig tensor(3.2077)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "498\n",
      "eig tensor(3.4835)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "499\n",
      "eig tensor(2.2651)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "500\n",
      "eig tensor(2.9256)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "501\n",
      "eig tensor(3.1221)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "502\n",
      "eig tensor(3.5242)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "503\n",
      "eig tensor(2.5765)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "504\n",
      "eig tensor(3.4605)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "505\n",
      "eig tensor(3.7290)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "506\n",
      "eig tensor(2.9409)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "507\n",
      "eig tensor(3.0723)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "508\n",
      "eig tensor(2.8246)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "509\n",
      "eig tensor(2.9855)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "510\n",
      "eig tensor(2.9758)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "511\n",
      "eig tensor(3.1246)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "512\n",
      "eig tensor(3.1470)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "513\n",
      "eig tensor(2.8126)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "514\n",
      "eig tensor(4.0287)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "515\n",
      "eig tensor(3.1545)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "516\n",
      "eig tensor(3.1697)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "517\n",
      "eig tensor(2.3026)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "518\n",
      "eig tensor(2.3789)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "519\n",
      "eig tensor(2.2478)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "520\n",
      "eig tensor(3.0920)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "521\n",
      "eig tensor(2.9269)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "522\n",
      "eig tensor(3.4784)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "523\n",
      "eig tensor(3.0167)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "524\n",
      "eig tensor(3.8078)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "525\n",
      "eig tensor(2.6574)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "526\n",
      "eig tensor(2.3241)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "527\n",
      "eig tensor(4.0467)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "528\n",
      "eig tensor(2.9625)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "529\n",
      "eig tensor(3.5625)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "530\n",
      "eig tensor(3.4892)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "531\n",
      "eig tensor(3.5206)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "532\n",
      "eig tensor(3.2916)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "533\n",
      "eig tensor(3.1711)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "534\n",
      "eig tensor(3.0473)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "535\n",
      "eig tensor(3.4273)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "536\n",
      "eig tensor(3.6071)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "537\n",
      "eig tensor(3.0261)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "538\n",
      "eig tensor(3.6213)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "539\n",
      "eig tensor(3.1021)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "540\n",
      "eig tensor(3.0933)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "541\n",
      "eig tensor(2.9587)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "542\n",
      "eig tensor(3.0107)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "543\n",
      "eig tensor(2.6809)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "544\n",
      "eig tensor(3.3512)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "545\n",
      "eig tensor(3.4099)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "546\n",
      "eig tensor(3.2120)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "547\n",
      "eig tensor(3.3626)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "548\n",
      "eig tensor(3.0642)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "549\n",
      "eig tensor(3.1561)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "550\n",
      "eig tensor(2.3519)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "551\n",
      "eig tensor(3.1689)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "552\n",
      "eig tensor(3.8160)\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "553\n",
      "eig tensor(3.0937)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "554\n",
      "eig tensor(2.8551)\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "555\n",
      "eig tensor(2.6295)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "556\n",
      "eig tensor(3.6243)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "557\n",
      "eig tensor(3.1619)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "558\n",
      "eig tensor(3.1012)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "559\n",
      "eig tensor(3.7324)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "560\n",
      "eig tensor(2.7794)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "561\n",
      "eig tensor(2.5178)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "562\n",
      "eig tensor(3.0444)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "563\n",
      "eig tensor(2.8029)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "564\n",
      "eig tensor(2.2300)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "565\n",
      "eig tensor(2.6873)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "566\n",
      "eig tensor(2.5658)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "567\n",
      "eig tensor(3.1091)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "568\n",
      "eig tensor(2.4130)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "569\n",
      "eig tensor(3.3545)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "570\n",
      "eig tensor(3.8414)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "571\n",
      "eig tensor(3.4641)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "572\n",
      "eig tensor(2.8125)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "573\n",
      "eig tensor(3.1334)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "574\n",
      "eig tensor(2.8729)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "575\n",
      "eig tensor(3.3873)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "576\n",
      "eig tensor(3.6245)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "577\n",
      "eig tensor(3.2245)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "578\n",
      "eig tensor(2.7211)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "579\n",
      "eig tensor(2.9104)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "580\n",
      "eig tensor(3.0676)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "581\n",
      "eig tensor(3.7484)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "582\n",
      "eig tensor(3.7140)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "583\n",
      "eig tensor(2.3728)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "584\n",
      "eig tensor(2.5060)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "585\n",
      "eig tensor(3.5006)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "586\n",
      "eig tensor(3.0304)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "587\n",
      "eig tensor(3.0506)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "588\n",
      "eig tensor(2.7936)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "589\n",
      "eig tensor(3.7144)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "590\n",
      "eig tensor(2.5934)\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "591\n",
      "eig tensor(3.2522)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "592\n",
      "eig tensor(2.5241)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "593\n",
      "eig tensor(3.2677)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "594\n",
      "eig tensor(3.0261)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "595\n",
      "eig tensor(3.3527)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "596\n",
      "eig tensor(3.1532)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "597\n",
      "eig tensor(3.4335)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "598\n",
      "eig tensor(2.2639)\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "599\n",
      "eig tensor(3.3975)\n",
      "tensor([-0.0095], grad_fn=<SelectBackward0>)\n",
      "600\n",
      "eig tensor(3.1775)\n",
      "tensor([-0.0095], grad_fn=<SelectBackward0>)\n",
      "601\n",
      "eig tensor(2.7149)\n",
      "tensor([-0.0094], grad_fn=<SelectBackward0>)\n",
      "602\n",
      "eig tensor(3.2185)\n",
      "tensor([-0.0093], grad_fn=<SelectBackward0>)\n",
      "603\n",
      "eig tensor(2.4254)\n",
      "tensor([-0.0093], grad_fn=<SelectBackward0>)\n",
      "604\n",
      "eig tensor(3.0692)\n",
      "tensor([-0.0092], grad_fn=<SelectBackward0>)\n",
      "605\n",
      "eig tensor(3.2080)\n",
      "tensor([-0.0092], grad_fn=<SelectBackward0>)\n",
      "606\n",
      "eig tensor(3.0630)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "607\n",
      "eig tensor(3.3462)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "608\n",
      "eig tensor(3.4373)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "609\n",
      "eig tensor(3.1997)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "610\n",
      "eig tensor(2.7663)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "611\n",
      "eig tensor(2.5085)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "612\n",
      "eig tensor(2.7979)\n",
      "tensor([-0.0088], grad_fn=<SelectBackward0>)\n",
      "613\n",
      "eig tensor(3.4695)\n",
      "tensor([-0.0088], grad_fn=<SelectBackward0>)\n",
      "614\n",
      "eig tensor(2.4679)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "615\n",
      "eig tensor(3.3630)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "616\n",
      "eig tensor(2.8820)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "617\n",
      "eig tensor(2.9347)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "618\n",
      "eig tensor(3.7929)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "619\n",
      "eig tensor(3.1195)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "620\n",
      "eig tensor(3.2871)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "621\n",
      "eig tensor(3.0480)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "622\n",
      "eig tensor(3.4456)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "623\n",
      "eig tensor(3.8010)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "624\n",
      "eig tensor(3.4061)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "625\n",
      "eig tensor(3.0897)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "626\n",
      "eig tensor(3.8129)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "627\n",
      "eig tensor(2.5748)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "628\n",
      "eig tensor(3.0095)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "629\n",
      "eig tensor(3.1006)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "630\n",
      "eig tensor(3.1598)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "631\n",
      "eig tensor(2.9519)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "632\n",
      "eig tensor(3.7715)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "633\n",
      "eig tensor(2.9128)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "634\n",
      "eig tensor(2.8506)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "635\n",
      "eig tensor(3.6128)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "636\n",
      "eig tensor(3.6080)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "637\n",
      "eig tensor(3.5493)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "638\n",
      "eig tensor(2.1438)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "639\n",
      "eig tensor(1.8785)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "640\n",
      "eig tensor(3.6318)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "641\n",
      "eig tensor(2.8873)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "642\n",
      "eig tensor(3.1848)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "643\n",
      "eig tensor(2.4414)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "644\n",
      "eig tensor(3.4906)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "645\n",
      "eig tensor(3.2076)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "646\n",
      "eig tensor(3.6758)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "647\n",
      "eig tensor(2.9202)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "648\n",
      "eig tensor(3.5989)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "649\n",
      "eig tensor(2.8909)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "650\n",
      "eig tensor(3.1645)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "651\n",
      "eig tensor(2.6621)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "652\n",
      "eig tensor(2.2108)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "653\n",
      "eig tensor(3.4172)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "654\n",
      "eig tensor(3.8852)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "655\n",
      "eig tensor(2.8947)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "656\n",
      "eig tensor(2.8851)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "657\n",
      "eig tensor(4.1301)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "658\n",
      "eig tensor(2.6340)\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "659\n",
      "eig tensor(3.2642)\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "660\n",
      "eig tensor(2.7039)\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "661\n",
      "eig tensor(3.7953)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "662\n",
      "eig tensor(3.5466)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "663\n",
      "eig tensor(2.8900)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "664\n",
      "eig tensor(2.7866)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "665\n",
      "eig tensor(3.0210)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "666\n",
      "eig tensor(2.6716)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "667\n",
      "eig tensor(2.6865)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "668\n",
      "eig tensor(3.2755)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "669\n",
      "eig tensor(3.3654)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "670\n",
      "eig tensor(2.7957)\n",
      "tensor([-0.0080], grad_fn=<SelectBackward0>)\n",
      "671\n",
      "eig tensor(3.2213)\n",
      "tensor([-0.0080], grad_fn=<SelectBackward0>)\n",
      "672\n",
      "eig tensor(3.0930)\n",
      "tensor([-0.0080], grad_fn=<SelectBackward0>)\n",
      "673\n",
      "eig tensor(3.6033)\n",
      "tensor([-0.0079], grad_fn=<SelectBackward0>)\n",
      "674\n",
      "eig tensor(3.7182)\n",
      "tensor([-0.0079], grad_fn=<SelectBackward0>)\n",
      "675\n",
      "eig tensor(3.2706)\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "676\n",
      "eig tensor(3.0226)\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "677\n",
      "eig tensor(2.1437)\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "678\n",
      "eig tensor(2.9555)\n",
      "tensor([-0.0077], grad_fn=<SelectBackward0>)\n",
      "679\n",
      "eig tensor(2.9827)\n",
      "tensor([-0.0076], grad_fn=<SelectBackward0>)\n",
      "680\n",
      "eig tensor(3.6264)\n",
      "tensor([-0.0075], grad_fn=<SelectBackward0>)\n",
      "681\n",
      "eig tensor(3.0834)\n",
      "tensor([-0.0075], grad_fn=<SelectBackward0>)\n",
      "682\n",
      "eig tensor(3.4779)\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "683\n",
      "eig tensor(3.1047)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "684\n",
      "eig tensor(3.1231)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "685\n",
      "eig tensor(3.5111)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "686\n",
      "eig tensor(3.1398)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "687\n",
      "eig tensor(3.3688)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "688\n",
      "eig tensor(2.7355)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "689\n",
      "eig tensor(3.5222)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "690\n",
      "eig tensor(2.5654)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "691\n",
      "eig tensor(3.1494)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "692\n",
      "eig tensor(2.8794)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "693\n",
      "eig tensor(3.3878)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "694\n",
      "eig tensor(3.0052)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "695\n",
      "eig tensor(3.0114)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "696\n",
      "eig tensor(2.9536)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "697\n",
      "eig tensor(3.3313)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "698\n",
      "eig tensor(3.4267)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "699\n",
      "eig tensor(3.3868)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "700\n",
      "eig tensor(3.7615)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "701\n",
      "eig tensor(2.1693)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "702\n",
      "eig tensor(3.6598)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "703\n",
      "eig tensor(3.1912)\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "704\n",
      "eig tensor(2.9876)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "705\n",
      "eig tensor(3.7997)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "706\n",
      "eig tensor(3.3977)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "707\n",
      "eig tensor(3.5488)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "708\n",
      "eig tensor(3.6049)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "709\n",
      "eig tensor(2.6132)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "710\n",
      "eig tensor(3.3605)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "711\n",
      "eig tensor(2.6615)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "712\n",
      "eig tensor(3.5296)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "713\n",
      "eig tensor(3.4837)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "714\n",
      "eig tensor(3.2913)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "715\n",
      "eig tensor(2.3549)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "716\n",
      "eig tensor(3.4103)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "717\n",
      "eig tensor(2.9043)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "718\n",
      "eig tensor(3.2749)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "719\n",
      "eig tensor(2.8592)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "720\n",
      "eig tensor(2.5026)\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "721\n",
      "eig tensor(2.8116)\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "722\n",
      "eig tensor(2.8585)\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "723\n",
      "eig tensor(3.4520)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "724\n",
      "eig tensor(3.6465)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "725\n",
      "eig tensor(3.5349)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "726\n",
      "eig tensor(2.5333)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "727\n",
      "eig tensor(2.9840)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "728\n",
      "eig tensor(3.1399)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "729\n",
      "eig tensor(2.5983)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "730\n",
      "eig tensor(3.4086)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "731\n",
      "eig tensor(3.0973)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "732\n",
      "eig tensor(3.1531)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "733\n",
      "eig tensor(3.0641)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "734\n",
      "eig tensor(4.1803)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "735\n",
      "eig tensor(3.2774)\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "736\n",
      "eig tensor(3.3785)\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "737\n",
      "eig tensor(4.4984)\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "738\n",
      "eig tensor(2.9326)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "739\n",
      "eig tensor(3.9417)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "740\n",
      "eig tensor(3.2976)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "741\n",
      "eig tensor(3.2616)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "742\n",
      "eig tensor(2.3731)\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "743\n",
      "eig tensor(2.8114)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "744\n",
      "eig tensor(2.8255)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "745\n",
      "eig tensor(3.3598)\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "746\n",
      "eig tensor(3.6499)\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "747\n",
      "eig tensor(3.3268)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "748\n",
      "eig tensor(3.4393)\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "749\n",
      "eig tensor(3.9487)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "750\n",
      "eig tensor(3.2839)\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "751\n",
      "eig tensor(3.7262)\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "752\n",
      "eig tensor(3.8975)\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "753\n",
      "eig tensor(3.6470)\n",
      "tensor([-0.0075], grad_fn=<SelectBackward0>)\n",
      "754\n",
      "eig tensor(3.6933)\n",
      "tensor([-0.0075], grad_fn=<SelectBackward0>)\n",
      "755\n",
      "eig tensor(3.6585)\n",
      "tensor([-0.0077], grad_fn=<SelectBackward0>)\n",
      "756\n",
      "eig tensor(3.5274)\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "757\n",
      "eig tensor(2.1687)\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "758\n",
      "eig tensor(3.2211)\n",
      "tensor([-0.0079], grad_fn=<SelectBackward0>)\n",
      "759\n",
      "eig tensor(3.5213)\n",
      "tensor([-0.0080], grad_fn=<SelectBackward0>)\n",
      "760\n",
      "eig tensor(3.2159)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "761\n",
      "eig tensor(3.3818)\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "762\n",
      "eig tensor(2.3412)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "763\n",
      "eig tensor(3.6496)\n",
      "tensor([-0.0082], grad_fn=<SelectBackward0>)\n",
      "764\n",
      "eig tensor(2.8455)\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "765\n",
      "eig tensor(2.8729)\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "766\n",
      "eig tensor(3.5327)\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "767\n",
      "eig tensor(3.5750)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "768\n",
      "eig tensor(2.9121)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "769\n",
      "eig tensor(4.1926)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "770\n",
      "eig tensor(3.5458)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "771\n",
      "eig tensor(3.1372)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "772\n",
      "eig tensor(3.6197)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "773\n",
      "eig tensor(2.1039)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "774\n",
      "eig tensor(3.0387)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "775\n",
      "eig tensor(3.8620)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "776\n",
      "eig tensor(1.4693)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "777\n",
      "eig tensor(3.6795)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "778\n",
      "eig tensor(3.6424)\n",
      "tensor([-0.0088], grad_fn=<SelectBackward0>)\n",
      "779\n",
      "eig tensor(3.3392)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "780\n",
      "eig tensor(2.7919)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "781\n",
      "eig tensor(3.1069)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "782\n",
      "eig tensor(2.6310)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "783\n",
      "eig tensor(3.1959)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "784\n",
      "eig tensor(3.0489)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "785\n",
      "eig tensor(3.1848)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "786\n",
      "eig tensor(2.1107)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "787\n",
      "eig tensor(2.6076)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "788\n",
      "eig tensor(3.8463)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "789\n",
      "eig tensor(3.3356)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "790\n",
      "eig tensor(3.1051)\n",
      "tensor([-0.0092], grad_fn=<SelectBackward0>)\n",
      "791\n",
      "eig tensor(3.5367)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "792\n",
      "eig tensor(2.7336)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "793\n",
      "eig tensor(3.3793)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "794\n",
      "eig tensor(3.1960)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "795\n",
      "eig tensor(3.2117)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "796\n",
      "eig tensor(2.9142)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "797\n",
      "eig tensor(2.9629)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "798\n",
      "eig tensor(3.6937)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "799\n",
      "eig tensor(4.3074)\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "800\n",
      "eig tensor(3.0156)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "801\n",
      "eig tensor(2.7498)\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "802\n",
      "eig tensor(3.2983)\n",
      "tensor([-0.0088], grad_fn=<SelectBackward0>)\n",
      "803\n",
      "eig tensor(3.0658)\n",
      "tensor([-0.0088], grad_fn=<SelectBackward0>)\n",
      "804\n",
      "eig tensor(3.1637)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "805\n",
      "eig tensor(3.0208)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "806\n",
      "eig tensor(3.3236)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "807\n",
      "eig tensor(3.6910)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "808\n",
      "eig tensor(3.1559)\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "809\n",
      "eig tensor(3.7246)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "810\n",
      "eig tensor(3.2113)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "811\n",
      "eig tensor(3.3835)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "812\n",
      "eig tensor(3.0831)\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "813\n",
      "eig tensor(3.3143)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "814\n",
      "eig tensor(2.8731)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "815\n",
      "eig tensor(3.5016)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "816\n",
      "eig tensor(3.8013)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "817\n",
      "eig tensor(3.4056)\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "818\n",
      "eig tensor(1.7257)\n",
      "tensor([-0.0087], grad_fn=<SelectBackward0>)\n",
      "819\n",
      "eig tensor(3.6071)\n",
      "tensor([-0.0091], grad_fn=<SelectBackward0>)\n",
      "820\n",
      "eig tensor(3.2422)\n",
      "tensor([-0.0094], grad_fn=<SelectBackward0>)\n",
      "821\n",
      "eig tensor(3.3598)\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "822\n",
      "eig tensor(4.1470)\n",
      "tensor([-0.0099], grad_fn=<SelectBackward0>)\n",
      "823\n",
      "eig tensor(2.8379)\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "824\n",
      "eig tensor(3.2869)\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "825\n",
      "eig tensor(2.5519)\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "826\n",
      "eig tensor(3.6834)\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "827\n",
      "eig tensor(2.8224)\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "828\n",
      "eig tensor(3.2942)\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "829\n",
      "eig tensor(3.4578)\n",
      "tensor([-0.0111], grad_fn=<SelectBackward0>)\n",
      "830\n",
      "eig tensor(2.8061)\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "831\n",
      "eig tensor(3.2035)\n",
      "tensor([-0.0113], grad_fn=<SelectBackward0>)\n",
      "832\n",
      "eig tensor(2.8439)\n",
      "tensor([-0.0114], grad_fn=<SelectBackward0>)\n",
      "833\n",
      "eig tensor(3.0277)\n",
      "tensor([-0.0115], grad_fn=<SelectBackward0>)\n",
      "834\n",
      "eig tensor(3.6638)\n",
      "tensor([-0.0116], grad_fn=<SelectBackward0>)\n",
      "835\n",
      "eig tensor(3.6511)\n",
      "tensor([-0.0116], grad_fn=<SelectBackward0>)\n",
      "836\n",
      "eig tensor(3.3671)\n",
      "tensor([-0.0117], grad_fn=<SelectBackward0>)\n",
      "837\n",
      "eig tensor(3.2326)\n",
      "tensor([-0.0118], grad_fn=<SelectBackward0>)\n",
      "838\n",
      "eig tensor(4.0987)\n",
      "tensor([-0.0119], grad_fn=<SelectBackward0>)\n",
      "839\n",
      "eig tensor(3.4869)\n",
      "tensor([-0.0120], grad_fn=<SelectBackward0>)\n",
      "840\n",
      "eig tensor(3.4267)\n",
      "tensor([-0.0121], grad_fn=<SelectBackward0>)\n",
      "841\n",
      "eig tensor(3.4793)\n",
      "tensor([-0.0122], grad_fn=<SelectBackward0>)\n",
      "842\n",
      "eig tensor(2.5709)\n",
      "tensor([-0.0122], grad_fn=<SelectBackward0>)\n",
      "843\n",
      "eig tensor(2.9152)\n",
      "tensor([-0.0123], grad_fn=<SelectBackward0>)\n",
      "844\n",
      "eig tensor(2.9830)\n",
      "tensor([-0.0124], grad_fn=<SelectBackward0>)\n",
      "845\n",
      "eig tensor(3.1019)\n",
      "tensor([-0.0125], grad_fn=<SelectBackward0>)\n",
      "846\n",
      "eig tensor(3.2156)\n",
      "tensor([-0.0125], grad_fn=<SelectBackward0>)\n",
      "847\n",
      "eig tensor(2.1980)\n",
      "tensor([-0.0126], grad_fn=<SelectBackward0>)\n",
      "848\n",
      "eig tensor(2.5889)\n",
      "tensor([-0.0127], grad_fn=<SelectBackward0>)\n",
      "849\n",
      "eig tensor(3.9051)\n",
      "tensor([-0.0128], grad_fn=<SelectBackward0>)\n",
      "850\n",
      "eig tensor(3.3960)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "851\n",
      "eig tensor(2.8532)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "852\n",
      "eig tensor(3.0878)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "853\n",
      "eig tensor(3.1398)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "854\n",
      "eig tensor(3.5157)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "855\n",
      "eig tensor(3.6568)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "856\n",
      "eig tensor(3.7624)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "857\n",
      "eig tensor(3.2696)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "858\n",
      "eig tensor(3.0377)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "859\n",
      "eig tensor(2.5776)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "860\n",
      "eig tensor(3.0105)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "861\n",
      "eig tensor(3.5547)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "862\n",
      "eig tensor(2.6745)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "863\n",
      "eig tensor(3.3154)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "864\n",
      "eig tensor(3.5091)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "865\n",
      "eig tensor(2.9537)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "866\n",
      "eig tensor(3.9864)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "867\n",
      "eig tensor(3.8490)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "868\n",
      "eig tensor(2.5604)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "869\n",
      "eig tensor(3.2654)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "870\n",
      "eig tensor(3.7234)\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "871\n",
      "eig tensor(3.3573)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "872\n",
      "eig tensor(3.6723)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "873\n",
      "eig tensor(-0.6096)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "874\n",
      "eig tensor(3.0953)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "875\n",
      "eig tensor(3.5757)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "876\n",
      "eig tensor(3.5197)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "877\n",
      "eig tensor(2.8950)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "878\n",
      "eig tensor(3.5935)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "879\n",
      "eig tensor(3.5813)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "880\n",
      "eig tensor(2.9305)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "881\n",
      "eig tensor(3.1363)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "882\n",
      "eig tensor(3.2385)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "883\n",
      "eig tensor(3.5988)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "884\n",
      "eig tensor(3.0072)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "885\n",
      "eig tensor(3.5284)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "886\n",
      "eig tensor(3.4030)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "887\n",
      "eig tensor(3.0974)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "888\n",
      "eig tensor(3.8754)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "889\n",
      "eig tensor(2.7972)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "890\n",
      "eig tensor(2.7556)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "891\n",
      "eig tensor(3.2557)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "892\n",
      "eig tensor(3.8634)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "893\n",
      "eig tensor(3.4404)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "894\n",
      "eig tensor(2.7764)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "895\n",
      "eig tensor(4.2035)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "896\n",
      "eig tensor(2.3184)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "897\n",
      "eig tensor(2.5927)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "898\n",
      "eig tensor(3.1038)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "899\n",
      "eig tensor(4.1943)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "900\n",
      "eig tensor(3.4634)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "901\n",
      "eig tensor(3.9285)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "902\n",
      "eig tensor(3.9666)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "903\n",
      "eig tensor(3.0169)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "904\n",
      "eig tensor(2.7511)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "905\n",
      "eig tensor(3.1263)\n",
      "tensor([-0.0128], grad_fn=<SelectBackward0>)\n",
      "906\n",
      "eig tensor(2.7172)\n",
      "tensor([-0.0128], grad_fn=<SelectBackward0>)\n",
      "907\n",
      "eig tensor(2.7922)\n",
      "tensor([-0.0128], grad_fn=<SelectBackward0>)\n",
      "908\n",
      "eig tensor(3.4618)\n",
      "tensor([-0.0128], grad_fn=<SelectBackward0>)\n",
      "909\n",
      "eig tensor(3.0864)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "910\n",
      "eig tensor(2.3928)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "911\n",
      "eig tensor(2.0236)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "912\n",
      "eig tensor(2.7418)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "913\n",
      "eig tensor(3.2857)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "914\n",
      "eig tensor(2.0690)\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "915\n",
      "eig tensor(2.6772)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "916\n",
      "eig tensor(3.8055)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "917\n",
      "eig tensor(2.3823)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "918\n",
      "eig tensor(3.3410)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "919\n",
      "eig tensor(2.7262)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "920\n",
      "eig tensor(1.9796)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "921\n",
      "eig tensor(2.8703)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "922\n",
      "eig tensor(3.2766)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "923\n",
      "eig tensor(2.8688)\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "924\n",
      "eig tensor(3.0346)\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "925\n",
      "eig tensor(3.3677)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "926\n",
      "eig tensor(3.4089)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "927\n",
      "eig tensor(2.4139)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "928\n",
      "eig tensor(3.8964)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "929\n",
      "eig tensor(3.7827)\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "930\n",
      "eig tensor(3.3464)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "931\n",
      "eig tensor(3.1823)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "932\n",
      "eig tensor(2.4557)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "933\n",
      "eig tensor(3.2374)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "934\n",
      "eig tensor(3.4547)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "935\n",
      "eig tensor(3.8796)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "936\n",
      "eig tensor(2.9815)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "937\n",
      "eig tensor(2.5217)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "938\n",
      "eig tensor(3.3398)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "939\n",
      "eig tensor(2.9389)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "940\n",
      "eig tensor(3.4331)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "941\n",
      "eig tensor(3.3589)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "942\n",
      "eig tensor(3.3563)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "943\n",
      "eig tensor(2.2698)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "944\n",
      "eig tensor(2.9772)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "945\n",
      "eig tensor(2.3923)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "946\n",
      "eig tensor(3.1401)\n",
      "tensor([-0.0130], grad_fn=<SelectBackward0>)\n",
      "947\n",
      "eig tensor(3.4986)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "948\n",
      "eig tensor(2.9835)\n",
      "tensor([-0.0131], grad_fn=<SelectBackward0>)\n",
      "949\n",
      "eig tensor(3.2348)\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "950\n",
      "eig tensor(3.2439)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "951\n",
      "eig tensor(3.2285)\n",
      "tensor([-0.0133], grad_fn=<SelectBackward0>)\n",
      "952\n",
      "eig tensor(3.0385)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "953\n",
      "eig tensor(4.1308)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "954\n",
      "eig tensor(-0.6163)\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "955\n",
      "eig tensor(3.7382)\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "956\n",
      "eig tensor(2.9399)\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "957\n",
      "eig tensor(3.7171)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "958\n",
      "eig tensor(3.4776)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "959\n",
      "eig tensor(2.7739)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "960\n",
      "eig tensor(3.4671)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "961\n",
      "eig tensor(2.6502)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "962\n",
      "eig tensor(2.8837)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "963\n",
      "eig tensor(2.6771)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "964\n",
      "eig tensor(3.4861)\n",
      "tensor([-0.0137], grad_fn=<SelectBackward0>)\n",
      "965\n",
      "eig tensor(3.2180)\n",
      "tensor([-0.0136], grad_fn=<SelectBackward0>)\n",
      "966\n",
      "eig tensor(2.7374)\n",
      "tensor([-0.0138], grad_fn=<SelectBackward0>)\n",
      "967\n",
      "eig tensor(2.7606)\n",
      "tensor([-0.0138], grad_fn=<SelectBackward0>)\n",
      "968\n",
      "eig tensor(3.4119)\n",
      "tensor([-0.0139], grad_fn=<SelectBackward0>)\n",
      "969\n",
      "eig tensor(2.2185)\n",
      "tensor([-0.0140], grad_fn=<SelectBackward0>)\n",
      "970\n",
      "eig tensor(3.4805)\n",
      "tensor([-0.0141], grad_fn=<SelectBackward0>)\n",
      "971\n",
      "eig tensor(2.6491)\n",
      "tensor([-0.0142], grad_fn=<SelectBackward0>)\n",
      "972\n",
      "eig tensor(3.3576)\n",
      "tensor([-0.0142], grad_fn=<SelectBackward0>)\n",
      "973\n",
      "eig tensor(3.1149)\n",
      "tensor([-0.0143], grad_fn=<SelectBackward0>)\n",
      "974\n",
      "eig tensor(2.4953)\n",
      "tensor([-0.0143], grad_fn=<SelectBackward0>)\n",
      "975\n",
      "eig tensor(2.9388)\n",
      "tensor([-0.0143], grad_fn=<SelectBackward0>)\n",
      "976\n",
      "eig tensor(2.9799)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "977\n",
      "eig tensor(3.4531)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "978\n",
      "eig tensor(2.3984)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "979\n",
      "eig tensor(3.3540)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "980\n",
      "eig tensor(2.9796)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "981\n",
      "eig tensor(3.3712)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "982\n",
      "eig tensor(1.3842)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "983\n",
      "eig tensor(2.8387)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "984\n",
      "eig tensor(3.0243)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "985\n",
      "eig tensor(2.8448)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "986\n",
      "eig tensor(3.3795)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "987\n",
      "eig tensor(3.1466)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "988\n",
      "eig tensor(2.7256)\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "989\n",
      "eig tensor(3.6202)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "990\n",
      "eig tensor(3.6903)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "991\n",
      "eig tensor(2.5201)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "992\n",
      "eig tensor(2.4579)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "993\n",
      "eig tensor(3.2914)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "994\n",
      "eig tensor(2.4246)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "995\n",
      "eig tensor(2.7213)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "996\n",
      "eig tensor(1.6986)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "997\n",
      "eig tensor(2.8520)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "998\n",
      "eig tensor(2.3008)\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "999\n",
      "eig tensor(2.8630)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numbers\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro import poutine\n",
    "from pyro.contrib.util import rmv, lexpand\n",
    "\n",
    "def is_bad(a):\n",
    "    return torch_isnan(a) or torch_isinf(a)\n",
    "    \n",
    "\n",
    "def torch_isnan(x):\n",
    "    \"\"\"\n",
    "    A convenient function to check if a Tensor contains any nan; also works with numbers\n",
    "    \"\"\"\n",
    "    if isinstance(x, numbers.Number):\n",
    "        return x != x\n",
    "    return torch.isnan(x).any()\n",
    "\n",
    "\n",
    "def torch_isinf(x):\n",
    "    \"\"\"\n",
    "    A convenient function to check if a Tensor contains any +inf; also works with numbers\n",
    "    \"\"\"\n",
    "    if isinstance(x, numbers.Number):\n",
    "        return x == float('inf') or x == -float('inf')\n",
    "    return (x == float('inf')).any() or (x == -float('inf')).any()\n",
    "\n",
    "\n",
    "def _safe_mean_terms(terms):\n",
    "    mask = torch.isnan(terms) | (terms == float('-inf')) | (terms == float('inf'))\n",
    "    if terms.dtype is torch.float32:\n",
    "        nonnan = (~mask).sum(0).float()\n",
    "    elif terms.dtype is torch.float64:\n",
    "        nonnan = (~mask).sum(0).double()\n",
    "    terms[mask] = 0.\n",
    "    loss = terms.sum(0) / nonnan\n",
    "    agg_loss = loss.sum()\n",
    "    return agg_loss, loss\n",
    "\n",
    "\n",
    "def make_regression_model(w_loc, w_scale, sigma_scale, xi_init, observation_label=\"y\"):\n",
    "    def regression_model(design_prototype):\n",
    "        design = pyro.param(\"xi\", xi_init)\n",
    "        design = (design / design.norm(dim=-1, p=1, keepdim=True)).expand(design_prototype.shape)\n",
    "        if is_bad(design):\n",
    "            raise ArithmeticError(\"bad design, contains nan or inf\")\n",
    "        batch_shape = design.shape[:-2]\n",
    "        with pyro.plate_stack(\"plate_stack\", batch_shape):\n",
    "            # `w` is shape p, the prior on each component is independent\n",
    "            w = pyro.sample(\"w\", dist.Laplace(w_loc, w_scale).to_event(1))\n",
    "            # `sigma` is scalar\n",
    "            sigma = 1e-6 + pyro.sample(\"sigma\", dist.Exponential(sigma_scale)).unsqueeze(-1)\n",
    "            mean = rmv(design, w)\n",
    "            sd = sigma\n",
    "            y = pyro.sample(observation_label, dist.Normal(mean, sd).to_event(1))\n",
    "            return y\n",
    "\n",
    "    return regression_model\n",
    "\n",
    "\n",
    "class TensorLinear(nn.Module):\n",
    "    __constants__ = ['bias']\n",
    "\n",
    "    def __init__(self, *shape, bias=True):\n",
    "        super(TensorLinear, self).__init__()\n",
    "        self.in_features = shape[-2]\n",
    "        self.out_features = shape[-1]\n",
    "        self.batch_dims = shape[:-2]\n",
    "        self.weight = nn.Parameter(torch.Tensor(*self.batch_dims, self.out_features, self.in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(*self.batch_dims, self.out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return rmv(self.weight, input) + self.bias\n",
    "\n",
    "\n",
    "class PosteriorGuide(nn.Module):\n",
    "    def __init__(self, y_dim, w_dim, batching):\n",
    "        super(PosteriorGuide, self).__init__()\n",
    "        n_hidden = 64\n",
    "        self.linear1 = TensorLinear(*batching, y_dim, n_hidden)\n",
    "        self.linear2 = TensorLinear(*batching, n_hidden, n_hidden)\n",
    "        self.output_layer = TensorLinear(*batching, n_hidden, w_dim + 3)\n",
    "        self.covariance_shape = batching + (w_dim, w_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, y_dict, design_prototype, observation_labels, target_labels):\n",
    "        y = y_dict[\"y\"] - .5\n",
    "        x = self.relu(self.linear1(y))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        final = self.output_layer(x)\n",
    "\n",
    "        posterior_mean = final[..., :-3]\n",
    "        gamma_concentration = 1e-6 + self.softplus(final[..., -3])\n",
    "        gamma_rate = 1. + self.softplus(final[..., -2])\n",
    "        scale_tril_multiplier = 1e-6 + self.softplus(final[..., -1])\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "\n",
    "        posterior_scale_tril = pyro.param(\n",
    "            \"posterior_scale_tril\",\n",
    "            torch.eye(posterior_mean.shape[-1], device=posterior_mean.device).expand(self.covariance_shape),\n",
    "            constraint=constraints.lower_cholesky\n",
    "        )\n",
    "        posterior_scale_tril = posterior_scale_tril * scale_tril_multiplier.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        batch_shape = design_prototype.shape[:-2]\n",
    "        with pyro.plate_stack(\"guide_plate_stack\", batch_shape):\n",
    "            pyro.sample(\"sigma\", dist.Gamma(gamma_concentration, gamma_rate))\n",
    "            pyro.sample(\"w\", dist.MultivariateNormal(posterior_mean, scale_tril=posterior_scale_tril))\n",
    "\n",
    "\n",
    "def _ace_eig_loss(model, guide, M, observation_labels, target_labels):\n",
    "    def loss_fn(design, num_particles, evaluation=False, **kwargs):\n",
    "        N = num_particles\n",
    "        expanded_design = lexpand(design, N)\n",
    "        \n",
    "        # Sample from p(y, theta | d)\n",
    "        trace = poutine.trace(model).get_trace(expanded_design)\n",
    "        y_dict_exp = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "        y_dict = {l: trace.nodes[l][\"value\"] for l in observation_labels}\n",
    "        theta_dict = {l: trace.nodes[l][\"value\"] for l in target_labels}\n",
    "\n",
    "        trace.compute_log_prob()\n",
    "        marginal_terms_cross = sum(trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "        marginal_terms_cross += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "        reguide_trace = poutine.trace(\n",
    "            pyro.condition(guide, data=theta_dict)).get_trace(\n",
    "            y_dict, expanded_design, observation_labels, target_labels)\n",
    "        # Here's a spot where you could update each model's parameters based on log_prob\n",
    "        reguide_trace.compute_log_prob()\n",
    "        marginal_terms_cross -= sum(reguide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "\n",
    "        # Sample M times from q(theta | y, d) for each y\n",
    "        reexpanded_design = lexpand(expanded_design, M)\n",
    "        guide_trace = poutine.trace(guide).get_trace(\n",
    "            y_dict, reexpanded_design, observation_labels, target_labels\n",
    "        )\n",
    "        theta_y_dict = {l: guide_trace.nodes[l][\"value\"] for l in target_labels}\n",
    "        theta_y_dict.update(y_dict_exp)\n",
    "        guide_trace.compute_log_prob()\n",
    "\n",
    "        # Re-run that through the model to compute the joint\n",
    "        model_trace = poutine.trace(\n",
    "            pyro.condition(model, data=theta_y_dict)).get_trace(reexpanded_design)\n",
    "        model_trace.compute_log_prob()\n",
    "\n",
    "        marginal_terms_proposal = -sum(guide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "        marginal_terms_proposal += sum(model_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "        marginal_terms_proposal += sum(model_trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "        marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "        terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "        # At eval time, add p(y | theta, d) terms\n",
    "        if evaluation:\n",
    "            terms += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "        return _safe_mean_terms(terms)\n",
    "\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "def neg_loss(loss):\n",
    "    def new_loss(*args, **kwargs):\n",
    "        return (-a for a in loss(*args, **kwargs))\n",
    "    return new_loss\n",
    "\n",
    "\n",
    "def opt_eig_loss_w_history(design, loss_fn, num_samples, num_steps, optim, time_budget):\n",
    "    params = None\n",
    "    est_loss_history = []\n",
    "    xi_history = []\n",
    "    baseline = 0.\n",
    "    t = time.time()\n",
    "    wall_times = []\n",
    "    for step in range(num_steps):\n",
    "        if params is not None:\n",
    "            pyro.infer.util.zero_grads(params)\n",
    "        with poutine.trace(param_only=True) as param_capture:\n",
    "            agg_loss, loss = loss_fn(design, num_samples, evaluation=True, control_variate=baseline)\n",
    "        baseline = -loss.detach()\n",
    "        params = set(site[\"value\"].unconstrained()\n",
    "                     for site in param_capture.trace.nodes.values())\n",
    "        if torch.isnan(agg_loss):\n",
    "            raise ArithmeticError(\"Encountered NaN loss in opt_eig_ape_loss\")\n",
    "        agg_loss.backward(retain_graph=True)\n",
    "        est_loss_history.append(loss.detach())\n",
    "        wall_times.append(time.time() - t)\n",
    "        optim(params)\n",
    "        optim.step()\n",
    "        print(pyro.param(\"xi\")[0, 0, ...])\n",
    "        print(step)\n",
    "        print('eig', baseline.squeeze())\n",
    "        if time_budget and time.time() - t > time_budget:\n",
    "            break\n",
    "\n",
    "    xi_history.append(pyro.param('xi').detach().clone())\n",
    "\n",
    "    est_loss_history = torch.stack(est_loss_history)\n",
    "    xi_history = torch.stack(xi_history)\n",
    "    wall_times = torch.tensor(wall_times)\n",
    "\n",
    "    return xi_history, est_loss_history, wall_times\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Start Pyro code implementation\n",
    "num_steps = 1000\n",
    "num_samples = 10\n",
    "time_budget = 1000\n",
    "seed = 420\n",
    "num_parallel = 1\n",
    "start_lr = 0.001\n",
    "end_lr = 0.001\n",
    "device = 'cpu'\n",
    "n = 20\n",
    "p = 1\n",
    "scale = 1.\n",
    "\n",
    "pyro.clear_param_store()\n",
    "if seed >= 0:\n",
    "    pyro.set_rng_seed(seed)\n",
    "else:\n",
    "    seed = int(torch.rand(tuple()) * 2 ** 30)\n",
    "    pyro.set_rng_seed(seed)\n",
    "\n",
    "xi_init = torch.randn((num_parallel, n, p), device=device)\n",
    "# Change the prior distribution here\n",
    "# prior params\n",
    "w_prior_loc = torch.zeros(p, device=device)\n",
    "w_prior_scale = scale * torch.ones(p, device=device)\n",
    "sigma_prior_scale = scale * torch.tensor(1., device=device)\n",
    "\n",
    "model_learn_xi = make_regression_model(\n",
    "    w_prior_loc, w_prior_scale, sigma_prior_scale, xi_init)\n",
    "\n",
    "contrastive_samples = num_samples\n",
    "\n",
    "# Fix correct loss\n",
    "targets = [\"w\", \"sigma\"]\n",
    "\n",
    "guide = PosteriorGuide(n, p, (num_parallel,)).to(device)\n",
    "eig_loss = _ace_eig_loss(model_learn_xi, guide, contrastive_samples, [\"y\"], targets)\n",
    "loss = neg_loss(eig_loss)\n",
    "\n",
    "gamma = (end_lr / start_lr) ** (1 / num_steps)\n",
    "scheduler = pyro.optim.ExponentialLR({'optimizer': torch.optim.Adam, 'optim_args': {'lr': start_lr},\n",
    "                                        'gamma': gamma})\n",
    "\n",
    "design_prototype = torch.zeros(num_parallel, n, p, device=device)  # this is annoying, code needs refactor\n",
    "\n",
    "xi_history, est_loss_history, wall_times = opt_eig_loss_w_history(\n",
    "    design_prototype, loss, num_samples=num_samples, num_steps=num_steps, optim=scheduler,\n",
    "    time_budget=time_budget)\n",
    "\n",
    "est_eig_history = -est_loss_history\n",
    "\n",
    "results = {'seed': seed, 'xi_history': xi_history.cpu(), 'est_eig_history': est_eig_history.cpu(),\n",
    "            'wall_times': wall_times.cpu()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['seed', 'xi_history', 'est_eig_history', 'wall_times'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFI-ACE\n",
    "Manually trained and stepped-through LFI-ACE model.\n",
    "\n",
    "1. Approximate likelihood using normalizing flow. Use a bunch of samples and their corresponding $\\theta$ values. Also, since the `pyro` version only uses one noise element, get rid of the other one that Kleinegesse used.\n",
    "2. Use approximated likelihood in ACE computation.\n",
    "3. Approximate the likelihood using LFI-ACE.\n",
    "\n",
    "This is code where i'm experimenting with the `update`, `loss`, and `params` stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "def sim_data(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack(\n",
    "        (y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d))))\n",
    "    )\n",
    "\n",
    "def lfi_ace_loss_fn(\n",
    "    params: hk.Params, prng_key: PRNGKey, x: Array, theta: Array, d: Array, xi: Array, \n",
    "    M: int, prior_dists, \n",
    ") -> Array:\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, d, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sim_linear_prior(num_samples: int, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Simulate prior samples and return their log_prob.\n",
    "    \"\"\"\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    samples, log_prob = base_distribution.sample_and_log_prob(seed=key, sample_shape=[num_samples])\n",
    "\n",
    "    return samples, log_prob\n",
    "\n",
    "\n",
    "def lfi_pce_loss_fn(\n",
    "    params: hk.Params, prng_key: PRNGKey, x: Array, theta: Array, d: Array, num_samples: int, # xi: Array, \n",
    "    M: int #, prior_dist: Distribution\n",
    ") -> Array:\n",
    "    keys = jrandom.split(prng_key, 2)\n",
    "    xi = params['xi']\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "    # theta_0 = prior_dist.sample((num_samples, 1))\n",
    "    # simulate the outcomes before finding their log_probs\n",
    "    d_sim = jnp.concatenate((d, xi), axis=0)\n",
    "    X = sim_data(d_sim, num_samples, keys[0])  # Do I need to split up the prng_key?\n",
    "\n",
    "    # I'm implicitly returning the prior here, that's a little annoying...\n",
    "    x, theta_0, d, xi = prepare_data(X)  # TODO: Maybe refactor this?\n",
    "\n",
    "    conditional_lp = log_prob.apply(flow_params, x, theta_0, d, xi)\n",
    "    # theta_L = prior_dist.sample((num_samples, M-1))\n",
    "    # Need to make an array of the new theta values\n",
    "    \n",
    "    contrastive_lps = []\n",
    "    for _ in range(M):\n",
    "        theta, _ = sim_linear_prior(num_samples, keys[1])\n",
    "        contrastive_lp = log_prob.apply(flow_params, x, theta, d, xi)\n",
    "        contrastive_lps.append(contrastive_lp)\n",
    "\n",
    "    marginal_log_prbs = jnp.concatenate((conditional_lp, jnp.array(contrastive_lps)))\n",
    "\n",
    "    marginal_lp = jax.nn.logsumexp(marginal_log_prbs, -1) - math.log(num_samples)\n",
    "\n",
    "    return _safe_mean_terms(conditional_lp - marginal_lp)\n",
    "\n",
    "\n",
    "    \n",
    "    theta_L = jnp.concatenate((theta_0, theta_L), axis=-1)\n",
    "    flow_loss = -jnp.mean(likelihood_0)\n",
    "    likelihood_L = log_prob.apply(flow_params, x, theta_L, d, xi)\n",
    "\n",
    "    marginal = jax.nn.logsumexp(likelihoods, -1) - math.log(num_samples)\n",
    "    eig_estimate = (likelihoods.exp() * (likelihoods - marginal)).sum(-1).mean(0)\n",
    "    surrogate_loss = eig_estimate.sum() + flow_loss\n",
    "    return surrogate_loss, eig_estimate\n",
    "\n",
    "    # Take N samples of the model\n",
    "    expanded_design = lexpand(design, N)  # N copies of the model\n",
    "    trace = poutine.trace(model).get_trace(expanded_design)\n",
    "    trace.compute_log_prob()\n",
    "    conditional_lp = sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "    y_dict = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "    # Resample M values of theta and compute conditional probabilities\n",
    "    conditional_model = pyro.condition(model, data=y_dict)\n",
    "    # Using (M, 1) instead of (M, N) - acceptable to re-use thetas between ys because\n",
    "    # theta comes before y in graphical model\n",
    "    reexpanded_design = lexpand(design, M, 1)  # sample M theta\n",
    "    retrace = poutine.trace(conditional_model).get_trace(reexpanded_design)\n",
    "    retrace.compute_log_prob()\n",
    "    marginal_log_probs = torch.cat([lexpand(conditional_lp, 1),\n",
    "                                    sum(retrace.nodes[l][\"log_prob\"] for l in observation_labels)])\n",
    "    marginal_lp = marginal_log_probs.logsumexp(0) - math.log(M+1)\n",
    "\n",
    "    return _safe_mean_terms(conditional_lp - marginal_lp)\n",
    "\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, d, xi))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    x, theta, d, xi = prepare_data(batch)\n",
    "    grads = jax.grad(loss_fn)(params, prng_key, x, theta, d, xi)\n",
    "    # grads_d = jax.grad(loss_fn, argnums=5)(params, prng_key, x, theta, d, xi)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing it completely manual\n",
    "Just to work out the bugs ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def sim_linear_prior(num_samples: int, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Simulate prior samples and return their log_prob.\n",
    "    \"\"\"\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    samples, log_prob = base_distribution.sample_and_log_prob(seed=key, sample_shape=[num_samples])\n",
    "\n",
    "    return samples, log_prob\n",
    "\n",
    "def jax_lexpand(A, *dimensions):\n",
    "    \"\"\"Expand tensor, adding new dimensions on left.\"\"\"\n",
    "    if jnp.isscalar(A):\n",
    "        A = A * jnp.ones(dimensions)\n",
    "        return A\n",
    "    shape = tuple(dimensions) + A.shape\n",
    "    A = A[jnp.newaxis, ...]\n",
    "    A = jnp.broadcast_to(A, shape)\n",
    "    return A\n",
    "\n",
    "\n",
    "def lfi_pce_eig(params: hk.Params, prng_key: PRNGKey, N: int=100, M: int=10, **kwargs):\n",
    "    keys = jrandom.split(prng_key, 3 + M)\n",
    "    xi = params['xi']\n",
    "    flow_params = {k: v for k, v in params.items() if k != 'xi'}\n",
    "\n",
    "    # simulate the outcomes before finding their log_probs\n",
    "    X = sim_data(d_sim, num_samples, keys[0])  # Do I need to split up the prng_key?\n",
    "\n",
    "    # I'm implicitly returning the prior here, that's a little annoying...\n",
    "    x, theta_0, d, xi = prepare_data(X)  # TODO: Maybe refactor this?\n",
    "\n",
    "    conditional_lp = log_prob.apply(flow_params, x, theta_0, d, xi)\n",
    "\n",
    "    contrastive_lps = []\n",
    "    thetas = []\n",
    "    # TODO: can this be parallelized to speed up the computation?\n",
    "    for i in range(M):\n",
    "        theta, _ = sim_linear_prior(num_samples, keys[i + 1])\n",
    "        thetas.append(theta)\n",
    "        contrastive_lp = log_prob.apply(flow_params, x, theta, d, xi)\n",
    "        contrastive_lps.append(contrastive_lp)\n",
    "\n",
    "    marginal_log_prbs = jnp.concatenate((jax_lexpand(conditional_lp, 1), jnp.array(contrastive_lps)))\n",
    "\n",
    "    marginal_lp = jax.nn.logsumexp(marginal_log_prbs, 0) - math.log(M + 1)\n",
    "\n",
    "    return sum(conditional_lp - marginal_lp) - jnp.mean(contrastive_lp)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_pce(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, N: int, M: int\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    grads = jax.grad(lfi_pce_eig)(params, prng_key, N=num_samples, M=inner_samples)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n",
    "\n",
    "# Boilerplate setup\n",
    "seed = 1231\n",
    "M = 10\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "d = jnp.array([])\n",
    "xi = jnp.array([0.])\n",
    "d_sim = jnp.concatenate((d, xi), axis=0)\n",
    "\n",
    "# Params and hyperparams\n",
    "len_x = len(d_sim)\n",
    "len_d = len(d)\n",
    "len_xi = len(xi)\n",
    "\n",
    "theta_shape = (2,)\n",
    "d_shape = (len(d),)\n",
    "xi_shape = (len_xi,)\n",
    "EVENT_SHAPE = (len(d_sim),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "\n",
    "num_samples = 10\n",
    "inner_samples = 10 # AKA M or L in BOED parlance\n",
    "batch_size = 128\n",
    "flow_num_layers = 5 #3 # 10\n",
    "mlp_num_layers = 4 # 3 # 4\n",
    "hidden_size = 128 # 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 100\n",
    "early_stopping_memory = 10\n",
    "early_stopping_threshold = 5e-2\n",
    "\n",
    "# Initialzie the params\n",
    "prng_seq = hk.PRNGSequence(42)  # TODO: Put one of \"keys\" here?\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *d_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "params['xi'] = xi\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "loss_deque = deque(maxlen=early_stopping_memory)\n",
    "for step in range(training_steps):\n",
    "    params, opt_state, grads_d = update_pce(\n",
    "        params, next(prng_seq), opt_state, N=num_samples, M=M\n",
    "    )\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "    \n",
    "        loss_deque.append(val_loss)\n",
    "        avg_abs_diff = jnp.mean(abs(jnp.array(loss_deque) - sum(loss_deque)/len(loss_deque)))\n",
    "        if step > warmup_steps and avg_abs_diff < early_stopping_threshold:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_fn(params: hk.Params, batch: Batch) -> Array:\n",
    "    x, theta, d, xi = prepare_data(batch)\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, theta, d, xi))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_grads = list(itertools.chain.from_iterable(grads.values())) + [grads_xi]\n",
    "\n",
    "# grads_xi_dict = hk.Params({\"xi\": grads_xi})\n",
    "\n",
    "# def concatenate_grads(grads1, grads2):\n",
    "#     return jax.tree_util.tree_map(jnp.concatenate, grads1, grads2)\n",
    "\n",
    "# all_grads = concatenate_grads(grads, grads_xi_dict)\n",
    "grads[\"xi\"] = grads_xi\n",
    "updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "new_params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conditioner_module/linear': {'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'w': DeviceArray([[-0.01242231,  0.1120088 ,  0.08578228, ..., -0.10011856,\n",
       "                -0.17064607, -0.09720737],\n",
       "               [-0.13173585, -0.02001649,  0.07170735, ...,  0.0293232 ,\n",
       "                 0.04132429,  0.05849221],\n",
       "               [-0.08464306,  0.09088153, -0.0486125 , ..., -0.02859271,\n",
       "                 0.07857817,  0.08087411],\n",
       "               ...,\n",
       "               [-0.08180398,  0.10232883, -0.07808912, ...,  0.09093236,\n",
       "                 0.01447338, -0.04487672],\n",
       "               [-0.03613302, -0.01441263, -0.09435159, ...,  0.00482197,\n",
       "                -0.16004968, -0.01607105],\n",
       "               [ 0.01921565, -0.09442761,  0.15707242, ...,  0.08147731,\n",
       "                 0.09811798,  0.08951801]], dtype=float32)},\n",
       " 'conditioner_module/linear_1': {'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'w': DeviceArray([[-0.02034451,  0.123349  ,  0.05846733, ..., -0.16802481,\n",
       "                 0.08035374, -0.04638239],\n",
       "               [ 0.05927319, -0.01684132, -0.02363183, ..., -0.17040513,\n",
       "                -0.0330485 ,  0.08511825],\n",
       "               [-0.00772113, -0.02820051,  0.07359286, ..., -0.05096417,\n",
       "                 0.08660227,  0.07430685],\n",
       "               ...,\n",
       "               [ 0.06259712,  0.05462907,  0.06284006, ..., -0.04855134,\n",
       "                -0.02922278,  0.12071123],\n",
       "               [-0.05934619, -0.11144834, -0.01344574, ..., -0.03980796,\n",
       "                 0.03746247,  0.02662061],\n",
       "               [ 0.01435024, -0.10825513,  0.0433238 , ..., -0.00268012,\n",
       "                -0.01134736,  0.1207571 ]], dtype=float32)},\n",
       " 'conditioner_module/linear_2': {'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'w': DeviceArray([[ 0.11704958, -0.00679462, -0.02611776, ...,  0.01910821,\n",
       "                 0.04144697, -0.14837135],\n",
       "               [-0.08871965,  0.03400696,  0.00537359, ..., -0.0946667 ,\n",
       "                -0.08239766,  0.00729494],\n",
       "               [-0.03551254, -0.0701015 , -0.05134199, ..., -0.06530296,\n",
       "                 0.00289406, -0.03862466],\n",
       "               ...,\n",
       "               [-0.00543919, -0.05729227,  0.12322035, ..., -0.03560666,\n",
       "                 0.09187058, -0.08146285],\n",
       "               [ 0.05871596,  0.06050637,  0.01374201, ..., -0.09838425,\n",
       "                -0.0707456 , -0.11911879],\n",
       "               [ 0.04733174,  0.02616994,  0.03164458, ..., -0.03422334,\n",
       "                -0.06521995,  0.0040209 ]], dtype=float32)},\n",
       " 'conditioner_module/linear_3': {'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'w': DeviceArray([[ 0.02284653,  0.02760232,  0.03274241, ...,  0.08120518,\n",
       "                -0.11581018, -0.0185739 ],\n",
       "               [-0.07713473, -0.06557476,  0.0810293 , ...,  0.11383499,\n",
       "                -0.00027302,  0.03219157],\n",
       "               [ 0.09601457, -0.07223165,  0.15624519, ..., -0.07556611,\n",
       "                -0.01997917,  0.02415168],\n",
       "               ...,\n",
       "               [-0.11430284, -0.09115985,  0.00830811, ..., -0.11181584,\n",
       "                -0.04514317,  0.0574323 ],\n",
       "               [ 0.08694408, -0.01283276,  0.10999747, ...,  0.03233989,\n",
       "                -0.05395529,  0.10088218],\n",
       "               [-0.00537713, -0.03650163, -0.00079553, ..., -0.01013184,\n",
       "                 0.171235  ,  0.0518052 ]], dtype=float32)},\n",
       " 'conditioner_module/linear_4': {'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'w': DeviceArray([[0., 0., 0., ..., 0., 0., 0.],\n",
       "               [0., 0., 0., ..., 0., 0., 0.],\n",
       "               [0., 0., 0., ..., 0., 0., 0.],\n",
       "               ...,\n",
       "               [0., 0., 0., ..., 0., 0., 0.],\n",
       "               [0., 0., 0., ..., 0., 0., 0.],\n",
       "               [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)},\n",
       " 'conditioner_module/mlp/~/linear_0': {'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'w': DeviceArray([[-0.25140095,  0.08758616, -0.2399307 ,  0.6837126 ,\n",
       "                -0.7447679 ,  0.13870795,  0.30753273, -1.1225071 ,\n",
       "                -0.05254016, -0.6195009 , -0.43485895,  0.20302084,\n",
       "                 0.69758797, -0.37193283, -1.0190424 , -0.10113915,\n",
       "                -0.7348837 ,  0.4027832 , -0.973811  ,  1.1249788 ,\n",
       "                -0.1568221 , -0.79462636, -0.55531687, -0.07611977,\n",
       "                -0.8866833 , -0.78039235, -0.28167385,  0.13703819,\n",
       "                 0.4062587 , -0.28528085,  0.327351  ,  0.19891377,\n",
       "                 0.34304202, -0.30063257,  0.10745104, -0.8023285 ,\n",
       "                -0.02180374,  0.44797626, -0.1907594 ,  0.7958219 ,\n",
       "                -0.43693113, -0.10097846, -0.8813813 , -0.2888429 ,\n",
       "                 0.26488355,  0.21812318,  0.30089226,  0.34657001,\n",
       "                 0.21152148,  0.01733217,  0.5909492 , -0.29970536,\n",
       "                -0.95823115,  0.5767598 ,  1.1390774 ,  0.23311326,\n",
       "                 0.39390418, -0.07125463, -0.6435101 , -0.33503118,\n",
       "                -0.5782391 ,  0.8207451 ,  0.583019  ,  0.18308651,\n",
       "                -0.30454916, -0.64039415, -1.0523095 ,  0.7446242 ,\n",
       "                -0.73392713,  0.95627177,  0.03095472, -0.8952785 ,\n",
       "                -0.48495066,  0.16996485, -0.00800274,  0.41574222,\n",
       "                -0.03323613, -0.612491  , -0.5563217 , -0.2517855 ,\n",
       "                 0.16806336,  0.73028445, -0.22318356, -0.31817362,\n",
       "                 0.11021481,  0.39656523, -0.02037064, -0.29774147,\n",
       "                 0.5658085 , -0.40370604, -0.9075927 , -0.96667856,\n",
       "                 0.7044413 , -0.7385238 ,  0.34593174,  0.6729392 ,\n",
       "                -0.57658464, -0.4664514 ,  0.23136531, -0.5933323 ,\n",
       "                 0.13612154, -0.1508478 , -0.8087038 ,  0.41107336,\n",
       "                -0.9735672 ,  0.12369525,  0.8993962 , -0.0870903 ,\n",
       "                -0.30604732, -0.5338566 , -0.22489029,  0.26980516,\n",
       "                 0.15627867, -0.20837063, -0.37065813,  0.49775162,\n",
       "                 0.24848872, -0.603253  ,  0.25736883,  0.5392951 ,\n",
       "                -0.34108216, -0.5295996 , -0.16628984, -0.35051817,\n",
       "                 0.5224113 , -0.00279331, -0.5499963 ,  0.01611804],\n",
       "               [ 0.10714512, -0.89475787, -0.29895625, -1.0647132 ,\n",
       "                 0.619557  , -0.42461672,  0.1998961 , -0.22251543,\n",
       "                -0.11218053,  0.59916985,  0.34031385,  0.22598149,\n",
       "                -0.15377684, -0.6143489 ,  0.59931606, -0.17335515,\n",
       "                -0.35464263, -0.5901093 , -0.604093  ,  0.31584066,\n",
       "                -0.33615932, -0.21673268,  0.8700099 , -0.24791943,\n",
       "                -0.4609123 ,  0.85363   ,  0.22107467,  0.31356972,\n",
       "                 0.30377746, -0.681558  , -0.26940078,  0.5011768 ,\n",
       "                -0.5803724 , -0.66329026,  0.0253092 ,  0.5706463 ,\n",
       "                 0.4317818 ,  0.6407693 , -0.77043194,  0.5892022 ,\n",
       "                 0.4304526 , -0.08831301,  0.49537653,  0.11914013,\n",
       "                 0.8881251 ,  0.2653791 , -0.6192125 ,  0.03403333,\n",
       "                -0.4119879 ,  1.0970113 ,  1.0358565 ,  0.60477436,\n",
       "                 0.44834414, -0.11142247,  0.02241182,  0.5406374 ,\n",
       "                 0.5430285 , -0.75322324,  0.2137121 ,  0.36589247,\n",
       "                 0.44787478,  0.55793154,  0.05696082,  0.58080924,\n",
       "                -0.0778355 ,  0.02721442, -0.67632645, -0.46943507,\n",
       "                 0.57948846,  0.3041168 , -0.74998355, -0.13640667,\n",
       "                 0.9194312 , -0.6023269 ,  0.2647692 , -0.21113198,\n",
       "                 0.15312928, -0.09374408,  0.6627961 ,  0.748189  ,\n",
       "                -0.36663434, -0.20546298, -0.4945502 ,  0.0189156 ,\n",
       "                 0.02737054, -0.09333947, -0.43588746, -0.7945414 ,\n",
       "                 0.7740163 ,  0.8613935 , -0.644915  ,  0.2941562 ,\n",
       "                -0.12933937,  0.29469028,  0.1410262 , -0.13892458,\n",
       "                 0.20157944,  0.6231497 , -0.12080207,  0.2871928 ,\n",
       "                 0.5674874 , -0.45768797,  0.90790665, -0.15205245,\n",
       "                 0.5743662 , -0.6054371 ,  0.36122417,  0.23636548,\n",
       "                -0.01383264, -0.10838509,  0.1206681 ,  0.2457988 ,\n",
       "                 0.54426545,  0.67869794, -0.103599  ,  0.0739433 ,\n",
       "                 0.5031319 ,  0.49676234, -0.81716007, -0.41320702,\n",
       "                 0.7260052 ,  0.55500543, -0.25255758, -0.01746825,\n",
       "                 0.4817779 ,  0.20558998,  0.89959687,  0.13775374],\n",
       "               [ 0.9313663 ,  0.5593884 ,  0.40864336,  0.6399471 ,\n",
       "                -0.8291885 ,  0.0234695 ,  0.15095158,  0.15758274,\n",
       "                -0.2329659 ,  0.3137045 , -0.32068735, -0.27785686,\n",
       "                 0.01421577, -0.10438426, -0.20627019,  0.09829334,\n",
       "                -0.6046179 ,  0.26083472, -0.49390385,  0.29443255,\n",
       "                 0.18837082,  0.6214964 , -0.3576642 ,  0.00997046,\n",
       "                 0.03091052, -0.02450551, -0.1256714 ,  0.32650027,\n",
       "                -0.09648224, -1.0340157 ,  0.9015785 , -0.6354741 ,\n",
       "                 0.6599253 ,  0.835743  , -0.2482716 ,  0.31242755,\n",
       "                 0.388074  ,  0.7885519 , -0.72832847,  0.62795967,\n",
       "                -0.20718859,  0.48913327,  0.9522754 ,  0.39722696,\n",
       "                -0.04441433,  0.5939961 ,  0.48256654, -0.9506101 ,\n",
       "                -0.42811525, -1.0631942 ,  0.7326725 ,  0.44161466,\n",
       "                 0.4296211 , -0.6314602 ,  0.04350569,  0.258878  ,\n",
       "                 0.27127868,  0.38646552,  0.8622711 , -0.0057876 ,\n",
       "                 0.6054206 ,  0.16228624,  0.39608535,  0.64636195,\n",
       "                -0.551134  ,  0.44201255, -0.5114912 ,  0.2663196 ,\n",
       "                -0.2571966 ,  0.9155366 , -0.17268153, -0.55811036,\n",
       "                -0.83428746,  0.22111472, -0.4927886 , -0.24409486,\n",
       "                 0.7769984 , -0.6012776 ,  0.3988627 ,  0.3825249 ,\n",
       "                -0.32312134,  0.12230857,  0.07799803,  0.16963385,\n",
       "                -0.28913945,  0.594513  , -0.44539693,  0.06782147,\n",
       "                -0.703371  ,  0.14082807, -0.6286239 ,  0.3257858 ,\n",
       "                -0.35361686,  0.1904843 , -0.8498094 ,  0.65660757,\n",
       "                 0.566308  ,  0.48943958, -0.34715378,  0.2391527 ,\n",
       "                 1.1191591 ,  0.5573917 , -0.8048638 , -0.2573705 ,\n",
       "                -0.07575536, -0.01642231,  0.4620741 , -0.03798487,\n",
       "                 0.8260008 , -0.7603419 , -0.10226034,  0.20580146,\n",
       "                 0.49677235,  0.14852594, -0.1240358 ,  0.6225892 ,\n",
       "                -0.8547141 ,  0.51787686,  1.0604491 ,  0.6281216 ,\n",
       "                 0.24230155, -0.04039988, -0.02348054,  0.30992287,\n",
       "                -0.95419115, -0.10012046,  0.13021857, -0.6147217 ]],            dtype=float32)},\n",
       " 'conditioner_module/mlp_1/~/linear_0': {'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'w': DeviceArray([[ 0.02078108,  0.03569958, -0.09074184, ...,  0.10632855,\n",
       "                 0.12405165,  0.0223699 ],\n",
       "               [ 0.00395501, -0.0694342 , -0.01195877, ...,  0.00919545,\n",
       "                -0.02703078,  0.00225287],\n",
       "               [ 0.11819243, -0.11916082,  0.08427953, ...,  0.03087219,\n",
       "                 0.10646163,  0.06691089],\n",
       "               ...,\n",
       "               [ 0.07609697,  0.14326203, -0.02030519, ..., -0.13779368,\n",
       "                 0.07249702,  0.0673167 ],\n",
       "               [-0.03146262,  0.04870329, -0.01543579, ...,  0.05165273,\n",
       "                -0.07816769, -0.03780435],\n",
       "               [-0.0588938 ,  0.0376568 , -0.03261717, ..., -0.04759551,\n",
       "                -0.11503852,  0.1292449 ]], dtype=float32)},\n",
       " 'conditioner_module/mlp_2/~/linear_0': {'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'w': DeviceArray([[ 0.11641986,  0.06062162,  0.08570641, ..., -0.02661132,\n",
       "                 0.00522166, -0.05321172],\n",
       "               [-0.09612878,  0.13764288,  0.12323083, ..., -0.13470094,\n",
       "                -0.13767847,  0.03184697],\n",
       "               [ 0.00642909, -0.06131384, -0.02754444, ...,  0.00922625,\n",
       "                -0.02335455,  0.13359617],\n",
       "               ...,\n",
       "               [-0.03620361,  0.15946916,  0.05472334, ...,  0.06316594,\n",
       "                 0.05481415, -0.04159248],\n",
       "               [ 0.088457  ,  0.07783046,  0.03833425, ...,  0.00539787,\n",
       "                -0.00172452,  0.14950891],\n",
       "               [ 0.09904982, -0.00662384,  0.1275596 , ...,  0.02878463,\n",
       "                -0.02180895,  0.03315656]], dtype=float32)},\n",
       " 'conditioner_module/mlp_3/~/linear_0': {'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "               0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'w': DeviceArray([[ 4.3703333e-02,  1.2906986e-01,  5.8381133e-02, ...,\n",
       "                 6.0426593e-02,  9.2856616e-02, -4.1320752e-02],\n",
       "               [-6.3969404e-02, -9.8209351e-02, -8.2280047e-02, ...,\n",
       "                -4.9808703e-02, -1.1568710e-02, -9.6898332e-02],\n",
       "               [-1.1060080e-01,  1.7079640e-02,  2.4995033e-02, ...,\n",
       "                -3.9381880e-02, -1.1649947e-01,  5.4450698e-02],\n",
       "               ...,\n",
       "               [ 1.4009087e-01,  6.5100096e-02, -6.1040141e-02, ...,\n",
       "                -3.3046002e-03, -5.3083502e-02,  6.2777199e-02],\n",
       "               [-1.1094909e-03,  4.7994174e-02,  1.4363919e-01, ...,\n",
       "                -1.0285010e-01, -2.6131146e-02, -5.5461744e-05],\n",
       "               [-5.2716039e-02,  4.4330087e-02,  1.4255916e-02, ...,\n",
       "                 3.2178074e-02, -3.0911554e-02,  5.7547737e-02]],            dtype=float32)},\n",
       " 'xi': DeviceArray([0.], dtype=float32)}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaxlib.xla_extension.DeviceArray"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(grads_xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conditioner_module/mlp/~/linear_0', 'conditioner_module/linear', 'conditioner_module/mlp_1/~/linear_0', 'conditioner_module/linear_1', 'conditioner_module/mlp_2/~/linear_0', 'conditioner_module/linear_2', 'conditioner_module/mlp_3/~/linear_0', 'conditioner_module/linear_3', 'conditioner_module/linear_4', 'xi'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['conditioner_module/mlp/~/linear_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(3.6019292, dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfi_pce_eig(params, key, N=num_samples, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = jax.grad(lfi_pce_eig)(params, key, N=num_samples, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "new_params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates['xi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_updates_keys = [key for key, value in updates.items() if value != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conditioner_module/linear',\n",
       " 'conditioner_module/linear_1',\n",
       " 'conditioner_module/linear_2',\n",
       " 'conditioner_module/linear_3',\n",
       " 'conditioner_module/linear_4',\n",
       " 'conditioner_module/mlp/~/linear_0',\n",
       " 'conditioner_module/mlp_1/~/linear_0',\n",
       " 'conditioner_module/mlp_2/~/linear_0',\n",
       " 'conditioner_module/mlp_3/~/linear_0']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_updates_keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very small gradients but gradients nonetheless!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_lp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "\n",
    "contrastive_lps = []\n",
    "thetas = []\n",
    "for i in range(M):\n",
    "    theta, _ = sim_linear_prior(num_samples, keys[i + 1])\n",
    "    thetas.append(theta)\n",
    "    contrastive_lp = log_prob.apply(flow_params, x, theta, d, xi)\n",
    "    contrastive_lps.append(contrastive_lp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[  0.64793307,  12.057756  ],\n",
       "             [ -1.8036    , -21.126171  ],\n",
       "             [  1.133066  ,  11.62901   ],\n",
       "             [ -4.5348973 ,  -7.0127087 ],\n",
       "             [ -5.4115043 ,  -3.4683409 ],\n",
       "             [ -7.423147  ,   7.645422  ],\n",
       "             [  1.5872409 ,  11.221082  ],\n",
       "             [  0.8586352 , -10.206353  ],\n",
       "             [  8.100239  ,  -6.1539264 ],\n",
       "             [-13.176195  ,  10.821227  ]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-3.253583 , -3.9614515, -3.3008242, -3.7383153, -3.2508907,\n",
       "             -3.7140958, -3.7958465, -3.1544094, -4.623407 , -3.2264678],            dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob.apply(flow_params, x, thetas[0], d, xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ -6.1988173 ,  -1.2310148 ],\n",
       "             [-12.849492  , -12.620625  ],\n",
       "             [ -7.4009905 ,  -3.9226563 ],\n",
       "             [ 17.474777  ,  14.818273  ],\n",
       "             [  6.0482283 ,   6.752823  ],\n",
       "             [ 13.28307   ,  -1.4132088 ],\n",
       "             [ -2.7293384 ,   3.487775  ],\n",
       "             [  7.874885  ,  -0.23380284],\n",
       "             [-14.064912  ,  11.756083  ],\n",
       "             [  2.0690577 ,  14.524533  ]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-3.253583 , -3.9614515, -3.3008242, -3.7383153, -3.2508907,\n",
       "             -3.7140958, -3.7958465, -3.1544094, -4.623407 , -3.2264678],            dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob.apply(flow_params, x, thetas[1], d, xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "              True,  True], dtype=bool)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_lps[0] == contrastive_lps[4]\n",
    "# wtf? get back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_log_prbs = jnp.concatenate((jax_lexpand(conditional_lp, 1), jnp.array(contrastive_lps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 10)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginal_log_prbs.shape  # This may need to have an extra dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "marginal_lp = jax.nn.logsumexp(marginal_log_prbs, 0) - math.log(M + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# marginal_log_prbs -> marginal_lp needs to be over 0th dimension to remove 11\n",
    "marginal_lp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_lp.shape  # Think this needs to be a 1D array not scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_lp - marginal_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_mean_terms(terms):\n",
    "    mask = torch.isnan(terms) | (terms == float('-inf')) | (terms == float('inf'))\n",
    "    if terms.dtype is torch.float32:\n",
    "        nonnan = (~mask).sum(0).float()\n",
    "    elif terms.dtype is torch.float64:\n",
    "        nonnan = (~mask).sum(0).double()\n",
    "    terms[mask] = 0.\n",
    "    loss = terms.sum(0) / nonnan\n",
    "    agg_loss = loss.sum()\n",
    "    return agg_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conditioner_module/linear', 'conditioner_module/linear_1', 'conditioner_module/linear_2', 'conditioner_module/linear_3', 'conditioner_module/linear_4', 'conditioner_module/mlp/~/linear_0', 'conditioner_module/mlp_1/~/linear_0', 'conditioner_module/mlp_2/~/linear_0', 'conditioner_module/mlp_3/~/linear_0'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['xi'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conditioner_module/linear', 'conditioner_module/linear_1', 'conditioner_module/linear_2', 'conditioner_module/linear_3', 'conditioner_module/linear_4', 'conditioner_module/mlp/~/linear_0', 'conditioner_module/mlp_1/~/linear_0', 'conditioner_module/mlp_2/~/linear_0', 'conditioner_module/mlp_3/~/linear_0', 'xi'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = params.pop('xi', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conditioner_module/linear', 'conditioner_module/linear_1', 'conditioner_module/linear_2', 'conditioner_module/linear_3', 'conditioner_module/linear_4', 'conditioner_module/mlp/~/linear_0', 'conditioner_module/mlp_1/~/linear_0', 'conditioner_module/mlp_2/~/linear_0', 'conditioner_module/mlp_3/~/linear_0'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train. Just need a vector of the samples and thetas that produced them. Don't need a simulator or the prior quite yet.\n",
    "- How do I track the training curves in Jax/Haiku? Gave in and am using WANDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:     0; Validation loss: 9.587\n",
      "STEP:    10; Validation loss: 9.453\n",
      "STEP:    20; Validation loss: 9.295\n",
      "STEP:    30; Validation loss: 9.160\n",
      "STEP:    40; Validation loss: 8.990\n",
      "STEP:    50; Validation loss: 8.950\n",
      "STEP:    60; Validation loss: 8.914\n",
      "STEP:    70; Validation loss: 8.899\n",
      "STEP:    80; Validation loss: 8.833\n",
      "STEP:    90; Validation loss: 8.782\n",
      "STEP:   100; Validation loss: 8.803\n",
      "STEP:   110; Validation loss: 8.844\n",
      "STEP:   120; Validation loss: 8.850\n",
      "STEP:   130; Validation loss: 8.681\n",
      "STEP:   140; Validation loss: 8.731\n",
      "STEP:   150; Validation loss: 8.693\n",
      "STEP:   160; Validation loss: 8.693\n",
      "STEP:   170; Validation loss: 8.739\n",
      "STEP:   180; Validation loss: 8.609\n",
      "STEP:   190; Validation loss: 8.722\n",
      "STEP:   200; Validation loss: 8.581\n",
      "STEP:   210; Validation loss: 8.620\n",
      "STEP:   220; Validation loss: 8.705\n"
     ]
    }
   ],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.init(project=\"lfiax_linRegression_ACE\", entity=\"vz_uci\")\n",
    "\n",
    "# TODO: Put this in hydra config file\n",
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "# d = jnp.array([-10.0, 0.0, 5.0, 10.0])\n",
    "# d = jnp.array([1., 2.])\n",
    "# d = jnp.array([1.])\n",
    "# d_obs = jnp.array([0.])\n",
    "d_obs = jnp.array([])\n",
    "# d_prop = jrandom.uniform(key, shape=(1,), minval=-10.0, maxval=10.0)\n",
    "xi = jnp.array([0.])\n",
    "# d_prop = jnp.array([])\n",
    "d_sim = jnp.concatenate((d_obs, xi), axis=0)\n",
    "len_x = len(d_sim)\n",
    "len_d = len(d_obs)\n",
    "len_xi = len(xi)\n",
    "num_samples = 100\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "d_shape = (len(d_obs),)\n",
    "xi_shape = (len_xi,)\n",
    "EVENT_SHAPE = (len(d_sim),)\n",
    "# EVENT_DIM is important for the normalizing flow's block.\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = (theta_shape[0], len_d, len_xi)\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 5 #3 # 10\n",
    "mlp_num_layers = 4 # 3 # 4\n",
    "hidden_size = 128 # 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 100\n",
    "early_stopping_memory = 10\n",
    "early_stopping_threshold = 5e-2\n",
    "\n",
    "training_steps = 500\n",
    "eval_frequency = 10\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 10000\n",
    "# TODO: put this function in training since d will be changing.\n",
    "X = sim_data(d_sim, num_samples, key)\n",
    "\n",
    "shift = X.mean(axis=0)\n",
    "scale = X.std(axis=0) + 1e-14\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *theta_shape)),\n",
    "    np.zeros((1, *d_shape)),\n",
    "    np.zeros((1, *xi_shape)),\n",
    ")\n",
    "params['xi'] = xi\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Can change the length of the deque for more/less leniency in measuring the loss\n",
    "loss_deque = deque(maxlen=early_stopping_memory)\n",
    "for step in range(training_steps):\n",
    "    params, opt_state, grads_d = update(\n",
    "        params, next(prng_seq), opt_state, next(train_ds)\n",
    "    )\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n",
    "    \n",
    "        loss_deque.append(val_loss)\n",
    "        avg_abs_diff = jnp.mean(abs(jnp.array(loss_deque) - sum(loss_deque)/len(loss_deque)))\n",
    "        if step > warmup_steps and avg_abs_diff < early_stopping_threshold:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([10.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['xi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_prop.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the sampling/log-prob stuff work. Now to actually implment LFI ACE. I've got the flow, now just going to do some of the ACE computations manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_lexpand(A, *dimensions):\n",
    "    \"\"\"Expand tensor, adding new dimensions on left.\"\"\"\n",
    "    if jnp.isscalar(A):\n",
    "        A = A * jnp.ones(dimensions)\n",
    "        return A\n",
    "    shape = tuple(dimensions) + A.shape\n",
    "    A = A[jnp.newaxis, ...]\n",
    "    A = jnp.broadcast_to(A, shape)\n",
    "    return A\n",
    "\n",
    "\n",
    "# Walking through and commenting the code\n",
    "def lfi_ace_eig_loss(model:NormalizingFlow, guide:NormalizingFlow, M, observation_labels, target_labels):\n",
    "    def loss_fn(design, num_particles, evaluation=False, **kwargs):\n",
    "        N = num_particles\n",
    "        # Expand to the number of parallel designs being evaluated\n",
    "        expanded_design = lexpand(design, N)\n",
    "        \n",
    "        # TODO: make a for loop over the proposed models here.\n",
    "        for model in models:\n",
    "            # TODO: would be cool to make sampling from the model's prior a method\n",
    "             # Hmm I need N copies for parallelization. I can figure that out later, though.\n",
    "            model_thetas_0 = model.prior_distribution.sample(num_samples)\n",
    "            y_0, log_prob_y_0 = model.sample_with_log_prob(model_thetas_0, expanded_design)\n",
    "            # Get a dictionary of the expanded y and theta values, just make M copies of y values\n",
    "            # TODO: think of a better way to add these into expanded dictionary values...\n",
    "            y_dict_exp = lexpand(y, M)\n",
    "\n",
    "            # This is essentially the p(y|theta)p(theta)\n",
    "            # Ohh, each model will have two ratios, it's current prob and its prior one\n",
    "            marginal_terms_cross = sum(model.prior_distribution.log_prob(model_thetas_0))\n",
    "            marginal_terms_cross += sum(log_prob_y_0)\n",
    "\n",
    "        # Pray that jax can handle this parallelization - vmap and pmap to the rescue!...?\n",
    "        for _ in range(m):\n",
    "            for model in models:\n",
    "                # Sample from q(theta | y, d) using the guide normalizing flow\n",
    "                theta, log_prob = guide.sample_with_log_prob(y, expanded_design, observation_labels, target_labels)\n",
    "                theta_y_dict = {l: theta[l] for l in target_labels}\n",
    "                theta_y_dict.update(y_dict_exp)\n",
    "\n",
    "                marginal_terms_proposal = -sum(log_prob[l] for l in target_labels)\n",
    "                marginal_terms_proposal += sum(log_prob[l] for l in target_labels)\n",
    "                marginal_terms_proposal += sum(log_prob[l] for l in observation_labels)\n",
    "\n",
    "                marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "                terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "                # At eval time, add p(y | theta, d) terms\n",
    "                if evaluation:\n",
    "                    terms += sum(log_prob[l] for l in observation_labels)\n",
    "                return _safe_mean_terms(terms)\n",
    "\n",
    "        # Sample from q(theta | y, d) using the guide normalizing flow\n",
    "        theta, log_prob = guide.sample_with_log_prob(y, expanded_design, observation_labels, target_labels)\n",
    "        theta_y_dict = {l: theta[l] for l in target_labels}\n",
    "        theta_y_dict.update(y_dict_exp)\n",
    "\n",
    "        marginal_terms_proposal = -sum(log_prob[l] for l in target_labels)\n",
    "        marginal_terms_proposal += sum(log_prob[l] for l in target_labels)\n",
    "        marginal_terms_proposal += sum(log_prob[l] for l in observation_labels)\n",
    "\n",
    "        marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "        terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "        # At eval time, add p(y | theta, d) terms\n",
    "        if evaluation:\n",
    "            terms += sum(log_prob[l] for l in observation_labels)\n",
    "        return _safe_mean_terms(terms)\n",
    "\n",
    "    return loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "p = 20\n",
    "n = 20\n",
    "N = 2 # num_parallel\n",
    "M = 2\n",
    "design = torch.zeros([10, 20, 20])\n",
    "expanded_design = lexpand(design, N)\n",
    "\n",
    "w_prior_loc = torch.zeros(p, device=device)\n",
    "w_prior_scale = scale * torch.ones(p, device=device)\n",
    "sigma_prior_scale = scale * torch.tensor(1., device=device)\n",
    "\n",
    "model_learn_xi = make_regression_model(\n",
    "    w_prior_loc, w_prior_scale, sigma_prior_scale, xi_init)\n",
    "\n",
    "model = model_learn_xi\n",
    "guide = PosteriorGuide(n, p, (N,)).to(device)\n",
    "observation_labels = [\"y\"]\n",
    "target_labels = ['w', 'sigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = poutine.trace(model).get_trace(expanded_design)\n",
    "y_dict_exp = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "y_dict = {l: trace.nodes[l][\"value\"] for l in observation_labels}\n",
    "theta_dict = {l: trace.nodes[l][\"value\"] for l in target_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 20])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dict['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 20])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_dict['w'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_dict['sigma'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 10]), torch.Size([2, 10])]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[trace.nodes[l][\"log_prob\"].shape for l in target_labels]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what's throwing me off. Determining the `log_prob` of the `w` vector compresses it for some reason... Oh, wait, it's the probability of each *vector*. That's why it's compressed to the shape of the `y` and `sigma` shapes. Yea, that makes a lot more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.compute_log_prob()\n",
    "# This is taking the log_prob of a vector from a distribution. I can either use a distribution that \n",
    "# is in a vector format or I think it should be fine to add two separate ones together...\n",
    "marginal_terms_cross = sum(trace.nodes[l][\"log_prob\"] for l in target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_terms_cross += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-48.1695, -70.6690, -58.5410, -57.8346, -82.7463, -79.5401, -34.3802,\n",
       "         -75.5568, -62.8853, -90.3731],\n",
       "        [-68.9690, -46.9697, -79.1669, -81.4540, -75.3442, -28.8084, -77.4504,\n",
       "         -67.5315,   2.8248, -70.4339]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginal_terms_cross"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part would be the denominator of LFI-ACE with the sum of all of the models' log_probs. The numerator might actually be the sum of all the log_probs. The denominator would be the weighted sum... Yea, that makes more sense....\n",
    "\n",
    "Part of the loss could then just be adding on the loss of the prediction of each normalizing flow. That makes one scalar loss. Although, there's an EIG for each of the 'y' outputs, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1\nTrace Shapes:\n Param Sites:\nSample Sites:",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [12], line 108\u001b[0m, in \u001b[0;36mPosteriorGuide.forward\u001b[0;34m(self, y_dict, design_prototype, observation_labels, target_labels)\u001b[0m\n\u001b[1;32m    107\u001b[0m y \u001b[39m=\u001b[39m y_dict[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m.5\u001b[39m\n\u001b[0;32m--> 108\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(y))\n\u001b[1;32m    109\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [12], line 92\u001b[0m, in \u001b[0;36mTensorLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m rmv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39minput\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/contrib/util.py:44\u001b[0m, in \u001b[0;36mrmv\u001b[0;34m(A, b)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m\"\"\"Tensorized matrix vector multiplication of rightmost dimensions.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(A, b\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reguide_trace \u001b[39m=\u001b[39m poutine\u001b[39m.\u001b[39;49mtrace(\n\u001b[1;32m      2\u001b[0m     pyro\u001b[39m.\u001b[39;49mcondition(guide, data\u001b[39m=\u001b[39;49mtheta_dict))\u001b[39m.\u001b[39;49mget_trace(\n\u001b[1;32m      3\u001b[0m     y_dict, expanded_design, observation_labels, target_labels)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Here's a spot where you could update each model's parameters based on log_prob\u001b[39;00m\n\u001b[1;32m      5\u001b[0m reguide_trace\u001b[39m.\u001b[39mcompute_log_prob()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:198\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_trace\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    191\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m    :returns: data structure\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:180\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         exc \u001b[39m=\u001b[39m exc_type(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(exc_value, shapes))\n\u001b[1;32m    179\u001b[0m         exc \u001b[39m=\u001b[39m exc\u001b[39m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39madd_node(\n\u001b[1;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_RETURN\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_RETURN\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, value\u001b[39m=\u001b[39mret\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39madd_node(\n\u001b[1;32m    171\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m_INPUT\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_INPUT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m     exc_type, exc_value, traceback \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_context_wrap\u001b[39m(context, fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [12], line 108\u001b[0m, in \u001b[0;36mPosteriorGuide.forward\u001b[0;34m(self, y_dict, design_prototype, observation_labels, target_labels)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, y_dict, design_prototype, observation_labels, target_labels):\n\u001b[1;32m    107\u001b[0m     y \u001b[39m=\u001b[39m y_dict[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m.5\u001b[39m\n\u001b[0;32m--> 108\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(y))\n\u001b[1;32m    109\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(x))\n\u001b[1;32m    110\u001b[0m     final \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [12], line 92\u001b[0m, in \u001b[0;36mTensorLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m rmv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39minput\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/pyro/contrib/util.py:44\u001b[0m, in \u001b[0;36mrmv\u001b[0;34m(A, b)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrmv\u001b[39m(A, b):\n\u001b[1;32m     43\u001b[0m     \u001b[39m\"\"\"Tensorized matrix vector multiplication of rightmost dimensions.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(A, b\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1\nTrace Shapes:\n Param Sites:\nSample Sites:"
     ]
    }
   ],
   "source": [
    "reguide_trace = poutine.trace(\n",
    "    pyro.condition(guide, data=theta_dict)).get_trace(\n",
    "    y_dict, expanded_design, observation_labels, target_labels)\n",
    "# Here's a spot where you could update each model's parameters based on log_prob\n",
    "reguide_trace.compute_log_prob()\n",
    "marginal_terms_cross -= sum(reguide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = poutine.trace(model).get_trace(expanded_design)\n",
    "y_dict_exp = {l: lexpand(trace.nodes[l][\"value\"], M) for l in observation_labels}\n",
    "y_dict = {l: trace.nodes[l][\"value\"] for l in observation_labels}\n",
    "theta_dict = {l: trace.nodes[l][\"value\"] for l in target_labels}\n",
    "\n",
    "trace.compute_log_prob()\n",
    "marginal_terms_cross = sum(trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_cross += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "reguide_trace = poutine.trace(\n",
    "    pyro.condition(guide, data=theta_dict)).get_trace(\n",
    "    y_dict, expanded_design, observation_labels, target_labels)\n",
    "# Here's a spot where you could update each model's parameters based on log_prob\n",
    "reguide_trace.compute_log_prob()\n",
    "marginal_terms_cross -= sum(reguide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "\n",
    "# Sample M times from q(theta | y, d) for each y\n",
    "reexpanded_design = lexpand(expanded_design, M)\n",
    "guide_trace = poutine.trace(guide).get_trace(\n",
    "    y_dict, reexpanded_design, observation_labels, target_labels\n",
    ")\n",
    "theta_y_dict = {l: guide_trace.nodes[l][\"value\"] for l in target_labels}\n",
    "theta_y_dict.update(y_dict_exp)\n",
    "guide_trace.compute_log_prob()\n",
    "\n",
    "# Re-run that through the model to compute the joint\n",
    "model_trace = poutine.trace(\n",
    "pyro.condition(model, data=theta_y_dict)).get_trace(reexpanded_design)\n",
    "model_trace.compute_log_prob()\n",
    "\n",
    "marginal_terms_proposal = -sum(guide_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_proposal += sum(model_trace.nodes[l][\"log_prob\"] for l in target_labels)\n",
    "marginal_terms_proposal += sum(model_trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "\n",
    "marginal_terms = torch.cat([lexpand(marginal_terms_cross, 1), marginal_terms_proposal])\n",
    "terms = -marginal_terms.logsumexp(0) + math.log(M + 1)\n",
    "\n",
    "# At eval time, add p(y | theta, d) terms\n",
    "# if evaluation:\n",
    "terms += sum(trace.nodes[l][\"log_prob\"] for l in observation_labels)\n",
    "# breakpoint()\n",
    "_safe_mean_terms(terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdm3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bac022e209f3f8f811ac02bf6d6b971751e1ab224096f1893a92a620959b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
