{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "import distrax\n",
    "import haiku as hk\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from lfiax.flows.nsf import make_nsf\n",
    "\n",
    "from typing import (\n",
    "    Any,\n",
    "    Iterator,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "OptState = Any\n",
    "\n",
    "\n",
    "def sim_linear_jax(d: Array, priors: Array, key: PRNGKey):\n",
    "    # Keys for the appropriate functions\n",
    "    keys = jrandom.split(key, 3)\n",
    "\n",
    "    # sample random normal dist\n",
    "    noise_shape = (1,)\n",
    "\n",
    "    mu_noise = jnp.zeros(noise_shape)\n",
    "    sigma_noise = jnp.ones(noise_shape)\n",
    "\n",
    "    n_n = distrax.Independent(\n",
    "        distrax.MultivariateNormalDiag(mu_noise, sigma_noise)\n",
    "    ).sample(seed=keys[0], sample_shape=[len(d), len(priors)])\n",
    "\n",
    "    # sample random gamma noise\n",
    "    n_g = distrax.Gamma(2.0, 1.0 / 2.0).sample(\n",
    "        seed=keys[1], sample_shape=[len(d), len(priors)]\n",
    "    )\n",
    "\n",
    "    # perform forward pass\n",
    "    y = jnp.broadcast_to(priors[:, 0], (len(d), len(priors)))\n",
    "    y = y + jnp.expand_dims(d, 1) @ jnp.expand_dims(priors[:, 1], 0)\n",
    "    y = y + n_g + jnp.squeeze(n_n)\n",
    "    ygrads = priors[:, 1]\n",
    "\n",
    "    return y, ygrads\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helper functions to simulate data\n",
    "# ----------------------------------------\n",
    "def load_dataset(split: tfds.Split, batch_size: int) -> Iterator[Batch]:\n",
    "    ds = split\n",
    "    ds = ds.shuffle(buffer_size=10 * batch_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=1000)\n",
    "    ds = ds.repeat()\n",
    "    return iter(tfds.as_numpy(ds))\n",
    "\n",
    "\n",
    "def sim_data(d: Array, priors: Array, key: PRNGKey):\n",
    "    \"\"\"\n",
    "    Returns data in a format suitable for normalizing flow training.\n",
    "    Data will be in shape [y, thetas]. The `y` variable can vary in size.\n",
    "    \"\"\"\n",
    "    keys = jrandom.split(key, 2)\n",
    "\n",
    "    theta_shape = (2,)\n",
    "\n",
    "    mu = jnp.zeros(theta_shape)\n",
    "    sigma = (3**2) * jnp.ones(theta_shape)\n",
    "\n",
    "    base_distribution = distrax.Independent(  # Should this be independent?\n",
    "        distrax.MultivariateNormalDiag(mu, sigma)\n",
    "    )\n",
    "\n",
    "    priors = base_distribution.sample(seed=keys[0], sample_shape=[num_samples])\n",
    "\n",
    "    # ygrads allows to be compared to other implementations (Kleinegesse et)\n",
    "    y, ygrads = sim_linear_jax(d, priors, keys[1])\n",
    "\n",
    "    return jnp.column_stack((y.T, jnp.squeeze(priors), jnp.broadcast_to(d, (num_samples, len(d)))))\n",
    "\n",
    "\n",
    "def prepare_data(batch: Batch, len_x: int, prng_key: Optional[PRNGKey] = None) -> Array:\n",
    "    # Batch is [y, thetas, d]\n",
    "    data = batch.astype(np.float32)\n",
    "    # Handling the scalar case\n",
    "    if data.shape[1] <= 3:\n",
    "        x = jnp.expand_dims(data[:, :-2], -1)\n",
    "    # Use known length of x to split up the cond_data\n",
    "    data_shape = tuple(data.shape)\n",
    "    start = jnp.array([0, 0])\n",
    "    stop = jnp.array([data_shape[0], len_x])\n",
    "    x = lax.dynamic_slice(data, start, stop)\n",
    "    cond_data = data[:, len_x:]\n",
    "    return x, cond_data\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Haiku transform functions for training and evaluation\n",
    "# ----------------------------\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob(data: Array, cond_data: Array) -> Array:\n",
    "    # Get batch\n",
    "    shift = data.mean(axis=0)\n",
    "    scale = data.std(axis=0) + 1e-14\n",
    "\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "        standardize_x=True,\n",
    "        standardize_z=True,\n",
    "        use_resnet=True,\n",
    "        event_dim=EVENT_DIM,\n",
    "        shift=shift,\n",
    "        scale=scale,\n",
    "    )\n",
    "    return model.log_prob(data, cond_data)\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def model_sample(key: PRNGKey, num_samples: int, cond_data: Array) -> Array:\n",
    "    model = make_nsf(\n",
    "        event_shape=EVENT_SHAPE,\n",
    "        cond_info_shape=cond_info_shape,\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins,\n",
    "    )\n",
    "    z = jnp.repeat(cond_data, num_samples, axis=0)\n",
    "    z = jnp.expand_dims(z, -1)\n",
    "    return model._sample_n(key=key, n=[num_samples], z=z)\n",
    "\n",
    "\n",
    "# @jax.jit(static_argnums=(4,))\n",
    "def loss_fn(params: hk.Params, prng_key: PRNGKey, batch: Batch, len_x: int) -> Array:\n",
    "    # TODO: pass the length of x here\n",
    "    x, cond_data = prepare_data(batch, len_x, prng_key)\n",
    "    # Loss is average negative log likelihood.\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, cond_data))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_fn(params: hk.Params, batch: Batch) -> Array:\n",
    "    x, cond_data = prepare_data(batch)\n",
    "    loss = -jnp.mean(log_prob.apply(params, x, cond_data))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jax.jit#(static_argnums = (4,))\n",
    "def update(\n",
    "    params: hk.Params, prng_key: PRNGKey, opt_state: OptState, batch: Batch, len_x: int\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    grads = jax.grad(loss_fn)(params, prng_key, batch, len_x)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Shapes must be 1D sequences of concrete values of integer type, got Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=0/1)>.\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m opt_state \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39minit(params)\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(training_steps):\n\u001b[0;32m---> 54\u001b[0m     params, opt_state \u001b[39m=\u001b[39m update(params, \u001b[39mnext\u001b[39;49m(prng_seq), opt_state, \u001b[39mnext\u001b[39;49m(train_ds), len_x)\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m eval_frequency \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     57\u001b[0m         val_loss \u001b[39m=\u001b[39m eval_fn(params, \u001b[39mnext\u001b[39m(valid_ds))\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [45], line 170\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(params, prng_key, opt_state, batch, len_x)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39m@jax\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m#(static_argnums = (4,))\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\n\u001b[1;32m    167\u001b[0m     params: hk\u001b[39m.\u001b[39mParams, prng_key: PRNGKey, opt_state: OptState, batch: Batch, len_x: \u001b[39mint\u001b[39m\n\u001b[1;32m    168\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[hk\u001b[39m.\u001b[39mParams, OptState]:\n\u001b[1;32m    169\u001b[0m     \u001b[39m\"\"\"Single SGD update step.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     grads \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mgrad(loss_fn)(params, prng_key, batch, len_x)\n\u001b[1;32m    171\u001b[0m     updates, new_opt_state \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mupdate(grads, opt_state)\n\u001b[1;32m    172\u001b[0m     new_params \u001b[39m=\u001b[39m optax\u001b[39m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [45], line 152\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, prng_key, batch, len_x)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_fn\u001b[39m(params: hk\u001b[39m.\u001b[39mParams, prng_key: PRNGKey, batch: Batch, len_x: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m    151\u001b[0m     \u001b[39m# TODO: pass the length of x here\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     x, cond_data \u001b[39m=\u001b[39m prepare_data(batch, len_x, prng_key)\n\u001b[1;32m    153\u001b[0m     \u001b[39m# Loss is average negative log likelihood.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mjnp\u001b[39m.\u001b[39mmean(log_prob\u001b[39m.\u001b[39mapply(params, x, cond_data))\n",
      "Cell \u001b[0;32mIn [45], line 103\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(batch, len_x, prng_key)\u001b[0m\n\u001b[1;32m    101\u001b[0m start \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m])\n\u001b[1;32m    102\u001b[0m stop \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray([data_shape[\u001b[39m0\u001b[39m], len_x])\n\u001b[0;32m--> 103\u001b[0m x \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39;49mdynamic_slice(data, start, stop)\n\u001b[1;32m    104\u001b[0m cond_data \u001b[39m=\u001b[39m data[:, len_x:]\n\u001b[1;32m    105\u001b[0m \u001b[39mreturn\u001b[39;00m x, cond_data\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/sdm3/lib/python3.9/site-packages/jax/core.py:1827\u001b[0m, in \u001b[0;36mcanonicalize_shape\u001b[0;34m(shape, context)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1826\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[0;32m-> 1827\u001b[0m \u001b[39mraise\u001b[39;00m _invalid_shape_error(shape, context)\n",
      "\u001b[0;31mTypeError\u001b[0m: Shapes must be 1D sequences of concrete values of integer type, got Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=0/1)>.\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions."
     ]
    }
   ],
   "source": [
    "# TODO: Put this in hydra config file\n",
    "seed = 1231\n",
    "key = jrandom.PRNGKey(seed)\n",
    "\n",
    "d = jnp.array([-10.0, 0.0, 5.0, 10.0])\n",
    "# d = jnp.array([1., 2.])\n",
    "# d = jnp.array([1.])\n",
    "len_x = len(d)\n",
    "num_samples = 100\n",
    "\n",
    "# Params and hyperparams\n",
    "theta_shape = (2,)\n",
    "EVENT_SHAPE = (len(d),)\n",
    "EVENT_DIM = 1\n",
    "cond_info_shape = theta_shape\n",
    "\n",
    "batch_size = 128\n",
    "flow_num_layers = 10\n",
    "mlp_num_layers = 4\n",
    "hidden_size = 500\n",
    "num_bins = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "training_steps = 10  # 00\n",
    "eval_frequency = 100\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Simulating the data to be used to train the flow.\n",
    "num_samples = 10000\n",
    "X = sim_data(d, num_samples, key)\n",
    "\n",
    "# Create tf dataset from sklearn dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Splitting into train/validate ds\n",
    "train = dataset.skip(2000)\n",
    "val = dataset.take(2000)\n",
    "\n",
    "# load_dataset(split: tfds.Split, batch_size: int)\n",
    "train_ds = load_dataset(train, 512)\n",
    "valid_ds = load_dataset(val, 512)\n",
    "\n",
    "# Training\n",
    "prng_seq = hk.PRNGSequence(42)\n",
    "params = log_prob.init(\n",
    "    next(prng_seq),\n",
    "    np.zeros((1, *EVENT_SHAPE)),\n",
    "    np.zeros((1, *cond_info_shape)),\n",
    ")\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "for step in range(training_steps):\n",
    "    params, opt_state = update(params, next(prng_seq), opt_state, next(train_ds), len_x)\n",
    "\n",
    "    if step % eval_frequency == 0:\n",
    "        val_loss = eval_fn(params, next(valid_ds))\n",
    "        print(f\"STEP: {step:5d}; Validation loss: {val_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(len_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = jnp.array([0, 0])\n",
    "stop = jnp.array([X.shape[0], len_x])\n",
    "x = lax.dynamic_slice(X, start, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdm3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bac022e209f3f8f811ac02bf6d6b971751e1ab224096f1893a92a620959b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
